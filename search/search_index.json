{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A Rust client for Kubernetes in the style of a more generic client-go, a runtime abstraction inspired by controller-runtime, and a derive macro for CRDs inspired by kubebuilder. Hosted by CNCF as a Sandbox Project</p> <p>These crates build upon Kubernetes apimachinery + api concepts to enable generic abstractions. These abstractions allow Rust reinterpretations of reflectors, controllers, and custom resource interfaces, so that you can write applications easily.</p> <p> Getting Started  Q&amp;A  Crates  Github  Community</p>"},{"location":"adopters/","title":"Adopters","text":""},{"location":"adopters/#open-source","title":"Open Source","text":"<ul> <li>linkerd-policy-controller - the policy controllers for the Linkerd service mesh</li> <li>krustlet - a complete <code>WASM</code> running <code>kubelet</code></li> <li>stackable operators - (kafka, zookeeper, and more)</li> <li>bottlerocket-update-operator</li> <li>vector</li> <li>kdash tui - terminal dashboard for kubernetes</li> <li>logdna agent</li> <li>kubeapps pinniped</li> <li>kubectl-view-allocations - kubectl plugin to list resource allocations</li> <li>krator - kubernetes operators using state machines</li> <li>hahaha - an operator that cleans up sidecars after Jobs</li> <li>kubectl-watch - a kubectl plugin to provide a pretty delta change view of being watched kubernetes resources</li> <li>gateway-api - API bindings for Kubernetes Gateway API</li> <li>blixt - A Kubernetes Gateway API-based Layer 4 Load-Balancer for ingress</li> <li>databricks-kube-operator - GitOps/Helm style management of Databricks jobs</li> <li>mirrord - Run your local service in the context of your remote cluster.</li> <li>tembo-operator - Goodbye Database Sprawl, Hello Postgres</li> <li>Shulker - a Kubernetes operator for mananing complex and dynamic Minecraft infrastructures</li> <li>r\u016bn\u014d - A Secret Generator for Kubernetes written in Rust</li> <li>Akri - A Kubernetes resource interface for the edge</li> <li>Kubewarden (CNCF) - A Kubernetes policy engine powered by WebAssembly</li> <li>SimKube - A record-and-replay simulation engine for Kubernetes</li> <li>kty - The terminal for Kubernetes</li> <li>CAAPF - Cluster API Fleet addon provider (Fleet GitOps integration with cluster management solution)</li> <li>crust-gather - Collect your cluster state and serve it via a kubectl compatible API server</li> </ul>"},{"location":"adopters/#companies","title":"Companies","text":"<ul> <li>AWS</li> <li>Buoyant</li> <li>Deis Labs</li> <li>Stackable</li> <li>Datadog</li> <li>logdna</li> <li>Bitnami</li> <li>Materialize</li> <li>Qualified</li> <li>TrueLayer</li> <li>ViacomCBS</li> <li>nais</li> <li>Kong</li> <li>MetalBear</li> <li>Tembo</li> <li>Aptakube</li> </ul> <p>If you're using <code>kube-rs</code> in production and are not on this list, please submit a pull request!</p>"},{"location":"adopters/#reverse-dependencies","title":"Reverse Dependencies","text":"<p>Open source users of <code>kube</code> are additionally viewable through reverse dependency listings on both github and crates.io (for published resources). These will contain a more comprehensive/up-to-date list of adopters, with the caveat that some of these can be more experimental.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This document describes the high-level architecture of kube-rs.</p> <p>This is intended for contributors or people interested in architecture.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>The kube-rs repository contains 5 main crates, examples and tests.</p> <p>The main crate that users generally import is <code>kube</code>, and it's a straight facade crate that re-exports from the four other crates:</p> <ul> <li><code>kube_core</code> -&gt; re-exported as <code>core</code></li> <li><code>kube_client</code> -&gt; re-exported as <code>api</code> + <code>client</code> + <code>config</code> + <code>discovery</code></li> <li><code>kube_derive</code> -&gt; re-exported as <code>CustomResource</code></li> <li><code>kube_runtime</code> -&gt; re-exported as <code>runtime</code></li> </ul> <p>In terms of dependencies between these 4:</p> <ul> <li><code>kube_core</code> is used by <code>kube_runtime</code>, <code>kube_derive</code> and <code>kube_client</code></li> <li><code>kube_client</code> is used by <code>kube_runtime</code></li> <li><code>kube_runtime</code> is the highest level abstraction</li> </ul> <p>The extra indirection crate <code>kube</code> is there to avoid cyclic dependencies between the client and the runtime (if the client re-exported the runtime then the two crates would be cyclically dependent).</p> <p>NB: We refer to these crates by their <code>crates.io</code> name using underscores for separators, but the folders have dashes as separators.</p> <p>When working on features/issues with <code>kube-rs</code> you will generally work inside one of these crates at a time, so we will focus on these in isolation, but talk about possible overlaps at the end.</p>"},{"location":"architecture/#kubernetes-ecosystem-considerations","title":"Kubernetes Ecosystem Considerations","text":"<p>The Rust ecosystem does not exist in a vaccum as we take heavy inspirations from the popular Go ecosystem. In particular:</p> <ul> <li><code>core</code> module contains invariants from apimachinery that is preseved across individual apis</li> <li><code>client::Client</code> is a re-envisioning of a generic client-go</li> <li><code>runtime::Controller</code> abstraction follows conventions in controller-runtime</li> <li><code>derive::CustomResource</code> derive macro for CRDs is loosely inspired by kubebuilder's annotations</li> </ul> <p>We do occasionally diverge on matters where following the go side is worse for the rust language, but when it comes to choosing names and finding out where some modules / functionality should reside; a precedent in <code>client-go</code>, <code>apimachinery</code>, <code>controller-runtime</code> and <code>kubebuilder</code> goes a long way.</p>"},{"location":"architecture/#generated-structs","title":"Generated Structs","text":"<p>We do not maintain the Kubernetes types generated from the <code>swagger.json</code> or the protos at present moment, and we do not handle client-side validation of fields relating to these types (that's left to the api-server).</p> <p>We generally use k8s-openapi's Rust bindings for Kubernetes' builtin types types, see:</p> <ul> <li>github.com:k8s-openapi</li> <li>docs.rs:k8s-openapi</li> </ul> <p>We also maintain an experimental set of Protobuf bindings, see k8s-pb.</p>"},{"location":"architecture/#crate-overviews","title":"Crate Overviews","text":""},{"location":"architecture/#kube-core","title":"kube-core","text":"<p>This crate only contains types relevant to the Kubernetes API, abstractions analogous to what you'll find inside apimachinery, and extra Rust traits that help us with generics further down in <code>kube-client</code>.</p> <p>Starting out with the basic type modules first:</p> <ul> <li><code>metadata</code>: the various metadata types; <code>ObjectMeta</code>, <code>ListMeta</code>, <code>TypeMeta</code></li> <li><code>request</code> + <code>response</code> + <code>subresource</code>: a sans-IO style http interface for the API</li> <li><code>watch</code>: a generic enum and behaviour for the watch api</li> <li><code>params</code>: generic parameters passed to sans-IO request interface (<code>ListParams</code> etc, called <code>ListOptions</code> in apimachinery)</li> </ul> <p>Then there are traits</p> <ul> <li><code>crd</code>: a versioned <code>CustomResourceExt</code> trait for <code>kube-derive</code></li> <li><code>object</code> generic conveniences for iterating over typed lists of objects, and objects following spec/status conventions</li> <li><code>resource</code>: a <code>Resource</code> trait for <code>kube-client</code>'s <code>Api</code> + a convenience <code>ResourceExt</code> trait for users</li> </ul> <p>The most important export here is the <code>Resource</code> trait and its impls. It is a pretty complex trait, with an associated type called <code>DynamicType</code> (that is default empty). Every <code>ObjectMeta</code>-using type that comes from <code>k8s-openapi</code> gets a blanket impl of <code>Resource</code> so we can use them generically (in <code>kube_client::Api</code>).</p> <p>Finally, there are two modules used by the higher level <code>discovery</code> module (in <code>kube-client</code>) and they have similar counterparts in apimachinery/restmapper + apimachinery/group_version:</p> <ul> <li><code>discovery</code>: types returned by the discovery api; capabilities, verbs, scopes, key info</li> <li><code>gvk</code>: partial type information to infer api types</li> </ul> <p>The main type here from these two modules is <code>ApiResource</code> because it can also be used to construct a <code>kube_client::Api</code> instance without compile-time type information (both <code>DynamicObject</code> and <code>Object</code> has <code>Resource</code> impls where <code>DynamicType = ApiResource</code>).</p>"},{"location":"architecture/#kube-client","title":"kube-client","text":""},{"location":"architecture/#config","title":"config","text":"<p>Contains logic for determining the runtime environment (local kubeconfigs or in-cluster) so that we can construct our <code>Config</code> from either source.</p> <ul> <li><code>Config</code> is the source-agnostic type (with all the information needed by our <code>Client</code>)</li> <li><code>Kubeconfig</code> is for loading from <code>~/.kube/config</code> or from any number of kubeconfig like files set by <code>KUBECONFIG</code> evar.</li> <li><code>Config::from_cluster_env</code> reads environment variables that are injected when running inside a pod</li> </ul> <p>In general this module has similar functionality to the upstream client-go/clientcmd module.</p>"},{"location":"architecture/#client","title":"client","text":"<p>The <code>Client</code> is one of the most complicated parts of <code>kube-rs</code>, because it has the most generic interface. People can mock the <code>Client</code>, people can replace individual components and force inject headers, people can choose their own tls stack, and - in theory - use whatever http clients they want.</p> <p>Generally, the <code>Client</code> is created from the properties of a <code>Config</code> to create a particular <code>hyper::Client</code> with a pre-configured amount of tower::Layers (see <code>TryFrom&lt;Config&gt; for Client</code>), but users can also pass in an arbitrary <code>tower::Service</code> (to fully customise or to mock). The signature restrictions on <code>Client::new</code> is commensurately large.</p> <p>The <code>tls</code> module contains the <code>openssl</code> or <code>rustls</code> interfaces to let users pick their tls stacks. The connectors created in that module is passed to <code>hyper::Client</code> based on feature selection.</p> <p>The <code>Client</code> can be created from a particular type of using the properties in the <code>Config</code> to configure its layers. Some of our layers come straight from tower-http:</p> <ul> <li><code>tower_http::DecompressionLayer</code> to deal with gzip compression</li> <li><code>tower_http::TraceLayer</code> to propagate http request information onto tracing spans.</li> <li><code>tower_http::AddAuthorizationLayer</code> to set bearer tokens / basic auth (when needed)</li> </ul> <p>but we also have our own layers in the <code>middleware</code> module:</p> <ul> <li><code>BaseUriLayer</code> prefixes <code>Config::base_url</code> to requests</li> <li><code>AuthLayer</code> configures either <code>AddAuthorizationLayer</code> or <code>AsyncFilterLayer&lt;RefreshableToken&gt;</code> depending on authentication method in the kubeconfig. <code>AsyncFilterLayer&lt;RefreshableToken&gt;</code> is like <code>AddAuthorizationLayer</code>, but with a token that's refreshed when necessary.</li> </ul> <p>(The <code>middleware</code> module is kept small to avoid mixing the business logic (<code>client::auth</code> openid connect oauth provider logic) with the tower layering glue.)</p> <p>The exported layers and tls connectors are mainly exposed through the <code>config_ext</code> module's <code>ConfigExt</code> trait which is only implemented by <code>Config</code> (because the config has all the properties needed for this in general, and it helps minimise our api surface).</p> <p>Finally, the <code>Client</code> manages other key aspects of IO the protocol such as:</p> <ul> <li><code>Client::connect</code> performs an HTTP Upgrade for specialised verbs</li> <li><code>Client::request</code> handles 90% of all requests</li> <li><code>Client::request_events</code> handles streaming <code>watch</code> events using <code>tokio_utils</code>'s <code>FramedRead</code> codec</li> <li><code>Client::request_status</code> handles <code>Either&lt;T, Status&gt;</code> responses from Kubernetes</li> </ul>"},{"location":"architecture/#api","title":"api","text":"<p>The generic <code>Api</code> type and its methods.</p> <p>Builds on top of the <code>Request</code> / <code>Response</code> interface in <code>kube_core</code> by parametrising over a generic type <code>K</code> that implement <code>Resource</code> (plus whatever else is needed).</p> <p>The <code>Api</code> absorbs a <code>Client</code> on construction and is then configured with its <code>Scope</code> (through its <code>::namespaced</code> / <code>::default_namespaced</code> or <code>::all</code> constructors).</p> <p>For dynamic types (<code>Object</code> and <code>DynamicObject</code>) it has slightly more complicated constructors which have the <code>_with</code> suffix.</p> <p>The <code>core_methods</code> and most <code>subresource</code> methods generally follow this recipe:</p> <ul> <li>create <code>Request</code></li> <li>store the Kubernetes verb in the [<code>http::Extensions</code>] object</li> <li>call the request with the <code>Client</code> and tell it what type(s) to deserialize into</li> </ul> <p>Some subresource methods (behind the <code>ws</code> feature) use the <code>remote_command</code> module's <code>AttachedProcess</code> interface expecting a duplex stream to deal with specialised websocket verbs (<code>exec</code> and <code>attach</code>) and is calling <code>Client::connect</code> first to get that stream.</p>"},{"location":"architecture/#discovery","title":"discovery","text":"<p>Deals with dynamic discovery of what apis are available on the api-server. Normally this can be used to discover custom resources, but also certain standard resources that vary between providers.</p> <p>The <code>Discovery</code> client can be used to do a full recursive sweep of api-groups into all api resources (through <code>filter</code>/<code>exclude</code> -&gt; <code>run</code>) and then the users can periodically re-<code>run</code> to keep the cache up to date (as Kubernetes is being upgraded behind the scenes).</p> <p>The <code>discovery</code> module also contains a way to run smaller queries through the <code>oneshot</code> module; e.g. resolving resource name when having group version kind, resolving every resource within one specific group, or even one group at a pinned version.</p> <p>The equivalent Go logic is found in client-go/discovery</p>"},{"location":"architecture/#kube-derive","title":"kube-derive","text":"<p>The smallest crate. A simple derive proc_macro to generate Kubernetes wrapper structs and trait impls around a data struct.</p> <p>Uses <code>darling</code> to parse <code>#[kube(attrs...)]</code> then uses <code>syn</code> and <code>quote</code> to produce a suitable syntax tree based on the attributes requested.</p> <p>It ultimately contains a lot of ugly json coercing from attributes into serialization code, but this is code that everyone working with custom resources need.</p> <p>It has hooks into <code>schemars</code> when using <code>JsonSchema</code> to ensure the correct type of CRD schema is attached to the right part of the generated custom resource definition.</p>"},{"location":"architecture/#kube-runtime","title":"kube-runtime","text":"<p>The highest level crate that deals with the highest level abstractions (such as controllers/watchers/reflectors) and specific Kubernetes apis that need common care (finalisers, waiting for conditions, event publishing).</p>"},{"location":"architecture/#watcher","title":"watcher","text":"<p>The <code>watcher</code> module contains state machine wrappers around <code>Api::watch</code> that will watch and auto-recover on allowable failures. The <code>watcher</code> fn is the general purpose one that is similar to informers in Go land, and will watch a collection of objects. The <code>watch_object</code> is a specialised version of this that watches a single object.</p>"},{"location":"architecture/#reflector","title":"reflector","text":"<p>The <code>reflector</code> module contains wrappers around <code>watcher</code> that will cache objects in memory. The <code>reflector</code> fn wraps a <code>watcher</code> and a state <code>Store</code> that is updated on every event emitted by the <code>watcher</code>.</p> <p>The reason for the difference between <code>watcher::Event</code> (created by <code>watcher</code>) and <code>kube::api::WatchEvent</code> (created by <code>Api::watch</code>) is that <code>watcher</code> will deals with desync errors and do a full relist whose result is then propagated as a single event, ensuring the <code>reflector</code> can do a single, atomic update to its state <code>Store</code>.</p>"},{"location":"architecture/#controller","title":"controller","text":"<p>The <code>controller</code> module contains the <code>Controller</code> type and its associated definitions.</p> <p>The <code>Controller</code> is configured to watch one root object (configured via <code>::new</code>), and several owned objects (via <code>::owns</code>), and - once <code>::run</code> - it will hit a users <code>reconcile</code> function for every change to the root object or any of its child objects (and internally it will traverse up the object tree - usually through owner references - to find the affected root object).</p> <p>The user is then meant to provide an idempotent <code>reconcile</code> fn, that does not know what underlying object was changed, to ensure the state configured in its crd, is what can be seen in the world.</p> <p>To manage this, a vector of watchers is converted into a set of streams of the same type by mapping the watchers so they have the same output type. This is why <code>watches</code> and <code>owns</code> differ: <code>owns</code> looks up <code>OwnerReferences</code>, but <code>watches</code> need you to define the relation yourself with a <code>mapper</code>. The mappers we support are <code>trigger_owners</code>, <code>trigger_self</code>, and the custom <code>trigger_with</code>.</p> <p>Once we have combined the stream of streams we essentially have a flattened super stream with events from multiple watchers that will act as our input events. With this, the <code>applier</code> can start running its fairly complex machinery:</p> <ol> <li>new input events get sent to the <code>scheduler</code></li> <li>scheduled events are then passed them through a <code>Runner</code> preventing duplicate parallel requests for the same object</li> <li>when running, we send the affected object to the users <code>reconciler</code> fn and await that future</li> <li>a) on success, prepare the users <code>Action</code> (generally a slow requeue several minutes from now)</li> <li>b) on failure, prepare a <code>Action</code> based on the users error policy (generally a backoff'd requeue with shorter initial delay)</li> <li>Map resulting <code>Action</code>s through an ad-hoc <code>scheduler</code> channel</li> <li>Resulting requeue requests through the channel are picked up at the top of <code>applier</code> and merged with input events in step 1.</li> </ol> <p>Ideally, the process runs forever, and it minimises unnecessary reconcile calls (like users changing more than one related object while one reconcile is already happening).</p> <p>See controller internals for some more information on this.</p>"},{"location":"architecture/#finalizer","title":"finalizer","text":"<p>Contains a helper wrapper <code>finalizer</code> for a <code>reconcile</code> fn used by a <code>Controller</code> when a user is using finalizers to handle garbage collection.</p> <p>This lets the user focus on simply selecting the type of behaviour they would like to exhibit based on whether the object is being deleted or it's just being regularly reconciled (through enum matching on <code>finalizer::Event</code>). This lets the user elide checking for potential deletion timestamps and manage the state machinery of <code>metadata.finalizers</code> through jsonpatching.</p>"},{"location":"architecture/#wait","title":"wait","text":"<p>Contains helpers for waiting for <code>conditions</code>, or objects to be fully removed (i.e. waiting for finalizers post delete).</p> <p>These build upon <code>watch_object</code> with specific mappers.</p>"},{"location":"architecture/#events","title":"events","text":"<p>Contains an event <code>Recorder</code> ala client-go/events that controllers can hook into, to publish events related to their reconciliations.</p>"},{"location":"architecture/#crate-delineation-and-overlaps","title":"Crate Delineation and Overlaps","text":"<p>When working on the the client machinery, it's important to realise that there are effectively 5 layers involved:</p> <ol> <li>Sans-IO request builder (in <code>kube_core::Request</code>)</li> <li>IO (in <code>kube_client::Client</code>)</li> <li>Typing (in <code>kube_client::Api</code>)</li> <li>Helpers for using the API correctly (e.g.<code>kube_runtime::watcher</code>)</li> <li>High-level abstractions for specific tasks (e.g. <code>kube_runtime::controller</code>)</li> </ol> <p>At level 3, we essentially have what the K8s team calls a basic client. As a consequence, new methods/subresources typically cross 2 crate boundaries (<code>kube_core</code>, <code>kube_client</code>), and needs to touch 3 main modules.</p> <p>Similarly, there are also the traits and types that define what an api means in <code>kube_core</code> like <code>Resource</code> and <code>ApiResource</code>. If modifying these, then changes to <code>kube-derive</code> are likely necessary, as it needs to directly implement this for users.</p> <p>These types of cross-crate dependencies are why we expose <code>kube</code> as a single versioned facade crate that users can upgrade atomically (without being caught in the middle of a publish cycle). This also gives us better compatibility with <code>dependabot</code>.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":"<ul> <li>see https://github.com/kube-rs/kube/compare/2.0.1...main</li> </ul>"},{"location":"changelog/#201--2025-09-12","title":"2.0.1 / 2025-09-12","text":""},{"location":"changelog/#whats-changed","title":"What's Changed","text":"<p>Fixes an accidental inclusion of a constraint added to <code>Api::log_stream</code> introduced in the 2.0.0 Rust 2024 upgrade.</p>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Remove unused use&lt;'_&gt; from log_stream() by @pgerber in https://github.com/kube-rs/kube/pull/1824</li> </ul>"},{"location":"changelog/#201--2025-09-12_1","title":"2.0.1 / 2025-09-12","text":""},{"location":"changelog/#200--2025-09-08","title":"2.0.0 / 2025-09-08","text":""},{"location":"changelog/#kubernetes-v1_34-support-via-k8s-openapi-026","title":"Kubernetes <code>v1_34</code> support via <code>k8s-openapi</code> 0.26","text":"<p>Please upgrade k8s-openapi along with kube to avoid conflicts.</p>"},{"location":"changelog/#schemars-10","title":"Schemars 1.0","text":"<p>A fairly significant upgrade in https://github.com/kube-rs/kube/pull/1780. Our external facing API should be unchanged, although some schemars public import paths have changed. Note that if you are implementing <code>schemars</code> traits directly, then see the upstream schemars/migrating (and maybe consider using <code>KubeSchema</code> for relevant schema overrides).</p> <p>Please upgrade schemars along with kube for this version to avoid conflicts.</p>"},{"location":"changelog/#new-minimums","title":"New Minimums","text":"<p>Minimum versions: MSRV 1.85.0 (for edition 2024), MK8SV: 1.30 (unchanged).</p>"},{"location":"changelog/#highlights","title":"Highlights","text":"<p>This version is contains fixes, dependency clearups, and dependency updates. Noteworthy additions are <code>TryFrom</code> impls for <code>Kubeconfig</code> users in #1801, and a namespace accessor in <code>Api</code> in #1788</p>"},{"location":"changelog/#new-major","title":"New Major","text":"<p>A new semver major for unstable, public facing dependency updates. As per the new release cycle, it is aligned with the Kubernetes release.</p>"},{"location":"changelog/#whats-changed_1","title":"What's Changed","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Add <code>TryFrom</code> conversions for <code>Kubeconfig</code> -&gt; <code>Config</code> -&gt; <code>Client</code> by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1801</li> <li>Add pub fn namespace(&amp;self) -&gt; Option&lt;&amp;str&gt; to Api by @tgrushka in https://github.com/kube-rs/kube/pull/1788</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Update to schemars 1.0 by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1780</li> <li>Bump Rust Edition to 2024 and MSRV to 1.85 by @clux in https://github.com/kube-rs/kube/pull/1785</li> <li>Replace <code>hyper-socks2</code> with <code>hyper-util</code> client-proxy feature by @tottoto in https://github.com/kube-rs/kube/pull/1795</li> <li>Bump k8s-openapi to 0.26.0 by @clux in https://github.com/kube-rs/kube/pull/1817</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Clamp scheduling delay to 6 months by @dervoeti in https://github.com/kube-rs/kube/pull/1779</li> <li>Update admission example and pin to a local crd by @clux in https://github.com/kube-rs/kube/pull/1782</li> <li>Fix interactive auth mode to allow prompt messages by @gememma in https://github.com/kube-rs/kube/pull/1800</li> <li>Make kube::runtime::controller::Action ctors const by @imp in https://github.com/kube-rs/kube/pull/1804</li> <li>Fix oidc with openssl by @saif-88 in https://github.com/kube-rs/kube/pull/1807</li> </ul>"},{"location":"changelog/#110--2025-05-26","title":"1.1.0 / 2025-05-26","text":""},{"location":"changelog/#whats-changed_2","title":"What's Changed","text":"<p>Missing attribute bugfix + extra standard derives on core::conversion structs.</p>"},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Add missing derives on conversion types by @sbernauer in https://github.com/kube-rs/kube/pull/1759</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Emit <code>#[schemars(crate)]</code> attribute by @Techassi in https://github.com/kube-rs/kube/pull/1764</li> </ul> <p>Full Changelog: https://github.com/kube-rs/kube/compare/1.0.0...1.1.0</p>"},{"location":"changelog/#100--2025-05-13","title":"1.0.0 / 2025-05-13","text":""},{"location":"changelog/#a-major-version","title":"A Major Version","text":"<p>It's been a long time coming, but time has come to draw the line in the sand. No alphas, no betas. Hope it finds you all well. Thanks to everyone who has contributed over the years.</p> <p>This is a somewhat symbolic gesture, because semver-breaking changes are still hard to avoid with a large set of sub-1.0 dependencies we need to bump, as well as managing the large api surface of Kubernetes.</p> <p>Therefore, the plan is to align our breaking changes and major bumps with Kubernetes versions / k8s-openapi versions for now, and this should allow our other releases to stream in. See https://github.com/kube-rs/kube/issues/1688 for more information.</p>"},{"location":"changelog/#kubernetes-v1_33-support-via-k8s-openapi-025","title":"Kubernetes <code>v1_33</code> support via <code>k8s-openapi</code> 0.25","text":"<p>Please upgrade k8s-openapi along with kube to avoid conflicts.</p> <p>New minimum versions: MSRV 1.82.0, MK8SV: 1.30*</p>"},{"location":"changelog/#kubeschema","title":"KubeSchema","text":"<p>The <code>CELSchema</code> alternate derive for <code>JsonSchema</code> has been renamed to <code>KubeSchema</code> to indicate the increased functionality.</p> <p>In addition to being able to inject CEL rules for validations, it can now also inject <code>x-kubernetes</code> properties such as merge-strategy via https://github.com/kube-rs/kube/pull/1750, handle <code>#[validate]</code> attributes https://github.com/kube-rs/kube/pull/1749, and pass validation rules as string literals https://github.com/kube-rs/kube/pull/1754 :</p> <pre><code>#[derive(CustomResource, Serialize, Deserialize, Debug, PartialEq, Clone, KubeSchema)]\n#[kube(...properties)\nstruct DocumentSpec {\n    /// New merge strategy support\n    #[x_kube(merge_strategy = ListMerge::Set)]\n    x_kubernetes_set: Vec&lt;String&gt;,\n\n    /// CEL Validation now lives on x_kube and supports literal Rules:\n    #[x_kube(validation = \"!has(self.variantOne) || self.variantOne.int &gt; 22\")]\n    complex_enum: ComplexEnum,\n}\n</code></pre> <p>See kube.rs docs on validation for more info. Huge thanks to @Danil-Grigorev.</p>"},{"location":"changelog/#whats-changed_3","title":"What's Changed","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>feat(deps): enable <code>hyper-util/tracing</code> feature flag by @cratelyn in https://github.com/kube-rs/kube/pull/1734</li> <li>Permit literal string validation for CEL expressions by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1754</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Support additional <code>x-kubernetes-*</code> schema extensions by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1750</li> <li>Bump <code>k8s-openapi</code> to <code>0.25.0</code> by @clux in https://github.com/kube-rs/kube/pull/1756</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Remove deprecated <code>watcher::Event</code> <code>into_iter_*</code> methods by @clux in https://github.com/kube-rs/kube/pull/1738</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>docs: Adjust #[kube(scale(...)] doc example by @Techassi in https://github.com/kube-rs/kube/pull/1733</li> <li>Add suffix to generated struct by <code>CELSchema</code> by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1747</li> <li>Allow schemars validate attribute in <code>CELSchema</code> by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1749</li> <li>fix: resolve conflict with schemars preserve_order feature by @HoKim98 in https://github.com/kube-rs/kube/pull/1758</li> </ul>"},{"location":"changelog/#0990--2025-03-12","title":"0.99.0 / 2025-03-12","text":""},{"location":"changelog/#highlights_1","title":"Highlights","text":""},{"location":"changelog/#dependency-cleanups","title":"Dependency Cleanups","text":"<ul> <li><code>backoff</code> (unmaintained) replaced with <code>backon</code> in https://github.com/kube-rs/kube/pull/1653</li> <li>No change if you are using <code>default_backoff</code> natively, or through <code>Controller</code>.</li> <li>Parameters configurable via <code>ExponentialBackoff</code> from <code>backon::ExponentialBuilder</code> into <code>WatchStreamExt::backoff</code></li> <li><code>json-patch</code> bumped and uses re-exported <code>jsonptr</code> for less version clashes https://github.com/kube-rs/kube/pull/1718</li> <li><code>rand</code> dependency no longer explicit as only rng is under <code>ws</code> feature via <code>tungstenite</code>'s <code>client::generate_key</code> https://github.com/kube-rs/kube/pull/1691</li> <li><code>ring</code> (still maintained) now optional for <code>rustls-tls</code> feature (for alternate <code>aws-lc-rs</code>) https://github.com/kube-rs/kube/pull/1717</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Support for the <code>v5.channel.k8s.io</code> streaming <code>ws</code> protocol to allow closing streams properly (kubernetes.io blog) https://github.com/kube-rs/kube/pull/1693</li> <li><code>CustomResource</code> derive; typed attributes for <code>#[kube(scale)]</code> and <code>#[kube(deprecated)]</code> in https://github.com/kube-rs/kube/pull/1656 + https://github.com/kube-rs/kube/pull/1697</li> <li><code>Client::with_valid_until</code> to handle short lived local client certs https://github.com/kube-rs/kube/pull/1707</li> <li>New common <code>conditions</code> that can be awaited https://github.com/kube-rs/kube/pull/1710</li> </ul>"},{"location":"changelog/#whats-changed_4","title":"What's Changed","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Add typed scale argument to derive macro by @Techassi in https://github.com/kube-rs/kube/pull/1656</li> <li>Add deprecated argument to derive macro by @Techassi in https://github.com/kube-rs/kube/pull/1697</li> <li>Add <code>Api::get_metadata_opt_with</code> by @sebsoto in https://github.com/kube-rs/kube/pull/1708</li> <li>Add common wait conditions for Deployments, LoadBalancer Services, and Ingress by @detjensrobert in https://github.com/kube-rs/kube/pull/1710</li> <li>Add <code>Client::with_valid_until</code> for client cert expiry by @goenning in https://github.com/kube-rs/kube/pull/1707</li> <li>kube-runtime: make <code>ExponentialBackoff</code> public by @gdeleon2 in https://github.com/kube-rs/kube/pull/1716</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Replace <code>backoff</code> with <code>backon</code> by @flavio in https://github.com/kube-rs/kube/pull/1653</li> <li>Bump <code>rand</code> to 0.9 by @clux in https://github.com/kube-rs/kube/pull/1686</li> <li>Remove <code>rand</code> dependency in favor of <code>tungstenite</code> fn by @clux in https://github.com/kube-rs/kube/pull/1691</li> <li>Exec can return stdout data even after stdin is closed. by @esw-amzn in https://github.com/kube-rs/kube/pull/1693</li> <li>Bump <code>json-patch</code> to 4 use bundled <code>jsonptr</code> to 0.7 by @clux in https://github.com/kube-rs/kube/pull/1718</li> <li>Allow removing hyper-rustls/ring feature by @eliad-wiz in https://github.com/kube-rs/kube/pull/1717</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>kube-runtime: fix exponential backoff max times by @eliad-wiz in https://github.com/kube-rs/kube/pull/1713</li> <li><code>CustomResource</code> derive; allow <code>status</code> attribute to take a path by @clux in https://github.com/kube-rs/kube/pull/1704</li> </ul>"},{"location":"changelog/#0980--2024-12-23","title":"0.98.0 / 2024-12-23","text":""},{"location":"changelog/#highlights_2","title":"Highlights","text":"<ul> <li>Kubernetes <code>v1_32</code> support via <code>k8s-openapi</code> 0.24</li> <li>Please upgrade k8s-openapi along with kube to avoid conflicts.</li> <li>New minimum versions: MSRV 1.81.0, MK8SV: 1.28</li> <li><code>kube-derive</code> additions:</li> <li>A <code>CELSchema</code> derive macro wrapper around <code>JsonSchema</code> for injecting cel validations into the schema #1649</li> <li>Allow overriding <code>served</code> and <code>storage</code> booleans for multiple versions of <code>CustomResource</code> derives: #1644</li> <li><code>kube-runtime</code> event <code>Recorder</code> now aggregates repeat events #1655 (some breaking changes, see controller-rs#116)</li> <li><code>kube-client</code> UTF-16 edge case handling for windows #1654</li> </ul>"},{"location":"changelog/#whats-changed_5","title":"What's Changed","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Add <code>storage</code> and <code>served</code> argument to derive macro by @Techassi in https://github.com/kube-rs/kube/pull/1644</li> <li>Implement  <code>derive(CELSchema)</code> macro for generating cel validation on CRDs by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1649</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Add series implementation for <code>runtime</code> event recorder by @pando85 in https://github.com/kube-rs/kube/pull/1655</li> <li>Bump <code>k8s-openapi</code> for Kubernetes <code>v1_32</code> support and MSRV by @clux in https://github.com/kube-rs/kube/pull/1671</li> <li>Update tokio-tungstenite requirement from 0.24.0 to 0.25.0 by @dependabot in https://github.com/kube-rs/kube/pull/1666</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Add support for UTF-16 encoded kubeconfig files by @goenning in https://github.com/kube-rs/kube/pull/1654</li> </ul>"},{"location":"changelog/#0970--2024-11-20","title":"0.97.0 / 2024-11-20","text":""},{"location":"changelog/#highlights_3","title":"Highlights","text":"<ul> <li><code>CustomResource</code> derive added features for crd yaml output:</li> <li>selectable fields #1605 + #1610</li> <li>annotations and labels #1631</li> <li>Configuration edge cases:</li> <li>Avoid double installations of <code>aws-lc-rs</code> (rustls crypto) provider #1617</li> <li>Kubeconfig fix for <code>null</code> user; #1608</li> <li>Default runtime watcher backoff alignment with <code>client-go</code> #1603</li> <li>Feature use:</li> <li>Client proxy feature-set misuse prevention #1626</li> <li>Allow disabling <code>gzip</code> via <code>Config</code> #1627</li> <li>Depedency minors: <code>thiserror</code>, <code>hashbrown</code>, <code>jsonptr</code>, <code>json-patch</code>. Killed <code>lazy_static</code> / <code>once_cell</code></li> </ul>"},{"location":"changelog/#whats-changed_6","title":"What's Changed","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Feature: Allow to pass selectableFields for CRD definition by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1605</li> <li>add support for CRD annotations and labels in kube-derive by @verokarhu in https://github.com/kube-rs/kube/pull/1631</li> <li>Feature: Add config setting to disable gzip compression #1627 by @markdingram in https://github.com/kube-rs/kube/pull/1628</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>upgrade to hashbrown 0.15.0 by @rorosen in https://github.com/kube-rs/kube/pull/1599</li> <li>update jsonptr + json-patch by @aviramha in https://github.com/kube-rs/kube/pull/1600</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>fix(kube-runtime): setup backoff with builder pattern by @tiagolobocastro in https://github.com/kube-rs/kube/pull/1603</li> <li>allow null user in kubeconfig's context by @aviramha in https://github.com/kube-rs/kube/pull/1608</li> <li>Gauge SelectableField by k8s 1.30 version by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1610</li> <li>Add a compile_error if setting selectable fields on K8s &lt; 1.30 by @clux in https://github.com/kube-rs/kube/pull/1612</li> <li>conditionally install <code>aws-lc-rs</code> by @goenning in https://github.com/kube-rs/kube/pull/1617</li> <li>Warn when trying to use an unsupported proxy protocol by @nightkr in https://github.com/kube-rs/kube/pull/1626</li> </ul>"},{"location":"changelog/#0960--2024-10-09","title":"0.96.0 / 2024-10-09","text":""},{"location":"changelog/#highlights_4","title":"Highlights","text":"<ul> <li>Features: <code>webpki-roots</code> added #1323, and predicates no longer require <code>unstable-runtime</code> #1578</li> <li>Local auth: improve leniency/kubectl-alignment #1595, remove http proxy vars #1520</li> <li>Dependencies: upgrades to <code>tower</code> and <code>secrecy</code>, and <code>derivative</code> swapped for <code>educe</code></li> </ul>"},{"location":"changelog/#whats-changed_7","title":"What's Changed","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>rustls: optionally use WebPKI roots to avoid panicking on Android &amp; iOS by @ewilken in https://github.com/kube-rs/kube/pull/1323</li> <li>Stabilise runtime predicates by @clux in https://github.com/kube-rs/kube/pull/1578</li> <li>Add <code>ObjectRef::from</code> as alias for <code>::from_obj</code> by @nightkr in https://github.com/kube-rs/kube/pull/1598</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Bump <code>secrecy</code> to 0.10 by @clux in https://github.com/kube-rs/kube/pull/1588</li> <li>Upgrades <code>tower</code> to 0.5.1 by @markdingram in https://github.com/kube-rs/kube/pull/1589</li> <li>runtime: rename references from Flatten to Decode by @clux in https://github.com/kube-rs/kube/pull/1520</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>remove using HTTP PROXY from environment variable by @aviramha in https://github.com/kube-rs/kube/pull/1587</li> </ul>"},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>replace derivative dependency with educe by @rorosen in https://github.com/kube-rs/kube/pull/1585</li> <li>change auth behavior to match upstream on unknown/empty user - use null auth by @aviramha in https://github.com/kube-rs/kube/pull/1595</li> </ul>"},{"location":"changelog/#0950--2024-09-16","title":"0.95.0 / 2024-09-16","text":""},{"location":"changelog/#kubernetes-v1_31-support-via-k8s-openapi-023","title":"Kubernetes <code>v1_31</code> support via <code>k8s-openapi</code> 0.23","text":"<p>Please upgrade k8s-openapi along with kube to avoid conflicts.</p> <p>New minimum versions: MSRV 1.77.2, MK8SV: 1.26</p>"},{"location":"changelog/#whats-changed_8","title":"What's Changed","text":""},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Update tokio-tungstenite requirement from 0.23.0 to 0.24.0 by @dependabot in https://github.com/kube-rs/kube/pull/1579</li> <li>Bump <code>k8s-openapi</code> to 0.23 for Kubernetes 1.31 support by @clux in https://github.com/kube-rs/kube/pull/1581</li> </ul> <p>Full Changelog: https://github.com/kube-rs/kube/compare/0.94.2...0.95.0</p>"},{"location":"changelog/#0942--2024-09-13","title":"0.94.2 / 2024-09-13","text":""},{"location":"changelog/#whats-changed_9","title":"What's Changed","text":"<p>Fixes a runtime regression in <code>watch_object</code>.</p>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Ensure <code>watch_object</code> handles objects removed before init by @markdingram in https://github.com/kube-rs/kube/pull/1577</li> </ul>"},{"location":"changelog/#0941--2024-09-09","title":"0.94.1 / 2024-09-09","text":""},{"location":"changelog/#whats-changed_10","title":"What's Changed","text":"<p>Convenience release. Adjusted a version bound to avoid possibility of running into version compatibility errors with <code>hyper-rustls</code>.</p>"},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Update hyper-rustls minimum version by @divergentdave in https://github.com/kube-rs/kube/pull/1575</li> </ul>"},{"location":"changelog/#0940--2024-09-09","title":"0.94.0 / 2024-09-09","text":""},{"location":"changelog/#highlights_5","title":"Highlights","text":"<p>Support for <code>rustls</code>'s aws-lc-rs is available under a new <code>kube/aws-lc-rs</code> feature. Via https://github.com/kube-rs/kube/pull/1568 for https://github.com/kube-rs/kube/issues/1562</p> <p>Furthermore, there are improvements to partial typing:</p> <ol> <li>Added a <code>DeserializeGuard</code> safety wrapper to lift deserialisation errors (to e.g. not break watchers). See the errorbound example and core module module.  Wrapped type be used with e.g. <code>Api::&lt;DeserializeGuard&lt;CaConfigMap&gt;&gt;</code>. Via https://github.com/kube-rs/kube/pull/1556</li> <li>A derive macro for <code>Resource</code>; <code>#[derive(Resource)]</code> allows inheriting existing <code>k8s-openapi</code> resource implementations to avoid stepping down to the dynamic api. See the cert check example for usage. Via https://github.com/kube-rs/kube/pull/1565</li> </ol>"},{"location":"changelog/#whats-changed_11","title":"What's Changed","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Add error boundary wrapper type by @nightkr in https://github.com/kube-rs/kube/pull/1556</li> <li>Implement Error for error_boundary::InvalidObject by @nightkr in https://github.com/kube-rs/kube/pull/1558</li> <li>Add finalizers predicate filter by @ivan-kiselev in https://github.com/kube-rs/kube/pull/1560</li> <li>optional feature to use <code>aws-lc-rs</code> rustls feature by @mcluseau in https://github.com/kube-rs/kube/pull/1568</li> <li>Add <code>Resource</code> derive macro by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1565</li> </ul>"},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Make implicitly dependent feature explicitly depend on each other by @clux in https://github.com/kube-rs/kube/pull/1551</li> </ul>"},{"location":"changelog/#0931--2024-07-23","title":"0.93.1 / 2024-07-23","text":""},{"location":"changelog/#whats-changed_12","title":"What's Changed","text":""},{"location":"changelog/#fixed_10","title":"Fixed","text":"<ul> <li>add missing feature gate on ConfigExt for no-features build by @HoKim98 in https://github.com/kube-rs/kube/pull/1549</li> </ul>"},{"location":"changelog/#0930--2024-07-22","title":"0.93.0 / 2024-07-22","text":""},{"location":"changelog/#highlights_6","title":"Highlights","text":"<p>Better query validation, better client header customisation, and two new modules:</p> <ol> <li><code>core::labels</code> module for creating typed label selectors for  <code>ListParams</code> or <code>WatchParams</code>. Can be constructed from a native <code>LabelSelector</code>, or directly from a <code>Selector</code> of <code>Expression</code>s. PR.</li> <li><code>prelude</code> to simplify imports of extension traits. PR.</li> </ol> <p>A big thank you to everyone who contributed to this release!</p>"},{"location":"changelog/#whats-changed_13","title":"What's Changed","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>add option to provide headers to send as client by @aviramha in https://github.com/kube-rs/kube/pull/1523</li> <li>Add prelude for blanket and extension traits across sub-crates by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1527</li> <li>Label selector support by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1539</li> </ul>"},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Update garde requirement from 0.19.0 to 0.20.0 by @dependabot in https://github.com/kube-rs/kube/pull/1535</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li>runtime: remove deprecated default_backoff by @clux in https://github.com/kube-rs/kube/pull/1518</li> </ul>"},{"location":"changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Fix watcher not fully paginating on Init by @clux in https://github.com/kube-rs/kube/pull/1525 (ported to 0.92.1)</li> <li>Prevent empty string object name requests from being sent to the apiserver by @xMAC94x in https://github.com/kube-rs/kube/pull/1541</li> </ul>"},{"location":"changelog/#0921--2024-06-19","title":"0.92.1 / 2024-06-19","text":""},{"location":"changelog/#bugfix-release","title":"Bugfix Release","text":"<p>This release fixes #1524; a regression from 0.92.0 causing <code>watcher</code> to skip pages on initial list. See #1525.</p> <p>It is recommended to upgrade from 0.92.0.</p>"},{"location":"changelog/#whats-changed_14","title":"What's Changed","text":""},{"location":"changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Fix watcher not fully paginating on Init by @clux in https://github.com/kube-rs/kube/pull/1525</li> </ul>"},{"location":"changelog/#0920--2024-06-12","title":"0.92.0 / 2024-06-12","text":""},{"location":"changelog/#runtime-decreased-memory-usage-from-watcher","title":"Runtime: Decreased Memory Usage from <code>watcher</code>","text":"<p>Buffering of initial pages / init streams is no longer a mandatory process with <code>watcher::Event</code> gaining new <code>Init</code>, <code>InitApply</code>, and <code>InitDone</code> events. These events are read on the store side maintaining the atomicity/completeness guarantees for <code>reflector</code> and <code>Store</code> users.</p> <p>This constitutes a significant memory decrease for all <code>watcher</code> users, and it has more details in a new kube.rs/blog post.</p> <p>The downside is a breaking change to <code>watcher::Event</code>. Plain usage of <code>watcher</code> / <code>reflector</code> / <code>Controller</code> should generally not need to change anything, but custom stores / matches on <code>watcher::Event</code> will need an update. If you are writing custom stores, the new signals should be helpful for improved caching.</p> <p>Thanks to @fabriziosestito via Kubewarden for https://github.com/kube-rs/kube/pull/1494 . Follow-ups for this feature: https://github.com/kube-rs/kube/pull/1499 and https://github.com/kube-rs/kube/pull/1504.</p>"},{"location":"changelog/#client-http-proxy-support","title":"Client: HTTP Proxy Support","text":"<p>Support is now introduced under the <code>http-proxy</code> feature pulling in hyper-http-proxy complementing the already existing <code>socks5</code> proxy feature.</p> <p>Thanks to @aviramha via MetalBear for the support in https://github.com/kube-rs/kube/pull/1496, with follow-ups https://github.com/kube-rs/kube/pull/1501 + https://github.com/kube-rs/kube/pull/1502</p>"},{"location":"changelog/#whats-changed_15","title":"What's Changed","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>Added support for HTTP proxy with hyper-proxy2 by @aviramha in https://github.com/kube-rs/kube/pull/1496</li> <li>Implement client native object reference fetching by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1511</li> </ul>"},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>Reduce buffering between watcher and Store by @fabriziosestito in https://github.com/kube-rs/kube/pull/1494</li> <li>Rename new watcher Event names and remove one that cannot happen by @clux in https://github.com/kube-rs/kube/pull/1499</li> <li>Update <code>tokio-tungstenite</code> to 0.23 by @Toasterson in https://github.com/kube-rs/kube/pull/1509</li> <li>Align <code>watcher::Event</code> init/page variants by @clux in https://github.com/kube-rs/kube/pull/1504</li> <li>Update json-patch to 2.0.0 by @bobsongplus in https://github.com/kube-rs/kube/pull/1507</li> </ul>"},{"location":"changelog/#fixed_13","title":"Fixed","text":"<ul> <li>Fix potentially panicing unchecked duration adds in runtime by @clux in https://github.com/kube-rs/kube/pull/1489</li> <li>ObjectList now accepts null metadata like upstream k8s does by @aviramha in https://github.com/kube-rs/kube/pull/1492</li> <li>rename http_proxy feature to http-proxy and add it to the umbrella crate by @aviramha in https://github.com/kube-rs/kube/pull/1501</li> <li>move from <code>hyper-proxy2</code> to <code>hyper-http-proxy</code> by @aviramha in https://github.com/kube-rs/kube/pull/1502</li> </ul>"},{"location":"changelog/#0910--2024-05-06","title":"0.91.0 / 2024-05-06","text":""},{"location":"changelog/#kubernetes-v1_30-support-via-k8s-openapi-022","title":"Kubernetes <code>v1_30</code> support via <code>k8s-openapi</code> 0.22","text":"<p>Please upgrade k8s-openapi along with kube to avoid conflicts.</p>"},{"location":"changelog/#unstable-stream-sharing","title":"Unstable Stream Sharing","text":"<p>A more complete implementation that allows sharing <code>watcher</code> streams between multiple <code>Controller</code>s (for https://github.com/kube-rs/kube/issues/1080) has been added under the <code>unstable-runtime</code> feature-flag in #1449 and #1483 by @mateiidavid. This represents the first usable implementation of shared streams (and replaces the older prototype part in #1470). While some changes are expected, you can check the shared_stream_controller example for a high-level overview.</p>"},{"location":"changelog/#whats-changed_16","title":"What's Changed","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Add shared stream interfaces by @mateiidavid in https://github.com/kube-rs/kube/pull/1449</li> <li>Allow to create non-controller owner reference for resource by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1475</li> <li>feat(runtime): support for owned shared streams by @mateiidavid in https://github.com/kube-rs/kube/pull/1483</li> </ul>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>Upgrade <code>k8s-openapi</code> to 0.22 and bump MK8SV to 1.25 by @clux in https://github.com/kube-rs/kube/pull/1485</li> </ul>"},{"location":"changelog/#removed_3","title":"Removed","text":"<ul> <li>Remove abandoned <code>StreamSubscribe</code> implementation by @clux in https://github.com/kube-rs/kube/pull/1470</li> </ul>"},{"location":"changelog/#fixed_14","title":"Fixed","text":"<ul> <li>Include inner error message in Display for SerdeError by @XAMPPRocky in https://github.com/kube-rs/kube/pull/1481</li> <li>Remove invalid <code>uniqueItems</code> property from CRDs when Sets are used  by @sbernauer in https://github.com/kube-rs/kube/pull/1484</li> </ul>"},{"location":"changelog/#0900--2024-04-03","title":"0.90.0 / 2024-04-03","text":""},{"location":"changelog/#highlights_7","title":"Highlights","text":""},{"location":"changelog/#kubeclientbody-improvements","title":"<code>kube::client::Body</code> Improvements","text":"<ul> <li>Unit testing helpers #1444 + #1445,</li> <li>Accuracy; <code>size_hint</code> and <code>is_end_stream</code> implemented in #1452 + internal cleanups #1453 and #1455</li> </ul>"},{"location":"changelog/#dependency-cleanups_1","title":"Dependency Cleanups","text":"<ul> <li><code>rustls</code> to 0.23 in #1457</li> <li><code>once_cell</code> removed in #1447 (no longer needed)</li> <li><code>futures</code> feature prune in #1442</li> <li><code>chrono</code> features prune in #1448, and bump its min version pin in #1458</li> </ul>"},{"location":"changelog/#whats-changed_17","title":"What's Changed","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>Add proxy <code>Body::collect_bytes</code> for easier unit tests by @clux in https://github.com/kube-rs/kube/pull/1445</li> </ul>"},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>update to <code>rustls</code> 0.23 by @tottoto in https://github.com/kube-rs/kube/pull/1457</li> </ul>"},{"location":"changelog/#fixed_15","title":"Fixed","text":"<ul> <li>disable unused <code>futures</code> feature by @tottoto in https://github.com/kube-rs/kube/pull/1442</li> <li>Expose <code>Body::empty</code> for easier tests by @clux in https://github.com/kube-rs/kube/pull/1444</li> <li>replace <code>once_cell</code> Lazy with ordinary static by @tottoto in https://github.com/kube-rs/kube/pull/1447</li> <li>replace <code>chrono</code> feature <code>clock</code> with <code>now</code> by @tottoto in https://github.com/kube-rs/kube/pull/1448</li> <li>implement <code>http_body</code> trait method by @tottoto in https://github.com/kube-rs/kube/pull/1452</li> <li>Fix examples for custom clients not authenticating by @clux in https://github.com/kube-rs/kube/pull/1450</li> <li>Set a compatible minimum <code>chrono</code> version by @clux in https://github.com/kube-rs/kube/pull/1458</li> </ul> <p>Full Changelog: https://github.com/kube-rs/kube/compare/0.89.0...0.90.0</p>"},{"location":"changelog/#0890--2024-03-26","title":"0.89.0 / 2024-03-26","text":""},{"location":"changelog/#upgrading-hyper-and-http-to-10-and-msrv-to-1750","title":"Upgrading <code>hyper</code> and <code>http</code> to 1.0 and MSRV to <code>1.75.0</code>","text":"<p>This release completes the hyper &amp; http ecosystem upgrade #1351 via #1438. In particular, this change includes upgrades to <code>http</code>, <code>http-body</code>, <code>tower-http</code>, <code>hyper</code>, <code>hyper-openssl</code>, <code>hyper-rustls</code>, <code>hyper-socks2</code>, <code>hyper-timeout</code>, <code>tame-oauth</code>, <code>tokio-tungstenite</code>, <code>tower-http</code>, <code>rustls</code>, <code>rustls-pemfile</code>, as well as adopting the new <code>hyper_util</code> and <code>http_body_util</code> to make the change.</p> <p>While this change constitutes significant internal churn (and a new <code>kube::client::Body</code>), our external api remains largely unchanged. Some minor changes are necessary for custom clients, and for integration testing using <code>tower_mock</code>. See the controller-rs upgrade pr or the examples folder in this commit for details.</p>"},{"location":"changelog/#whats-changed_18","title":"What's Changed","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li>client_ext for <code>Client::get</code> and <code>Client::list</code> by @clux in https://github.com/kube-rs/kube/pull/1375</li> <li>direct node access logs/portforward/exec/attach via kubelet debug interface by @XciD in https://github.com/kube-rs/kube/pull/1428</li> </ul>"},{"location":"changelog/#changed_12","title":"Changed","text":"<ul> <li>Bump MSRV to <code>1.75.0</code> by @clux in https://github.com/kube-rs/kube/pull/1408</li> <li>Ease the bound for <code>reflector</code> to only request identifying metadata by @SOF3 in https://github.com/kube-rs/kube/pull/1393</li> <li>Update base64 requirement from 0.21.4 to 0.22.0 by @dependabot in https://github.com/kube-rs/kube/pull/1422</li> <li>upgrade <code>jsonpath-rust</code> to 0.5.0 by @clux in https://github.com/kube-rs/kube/pull/1429</li> <li>update to hyper 1 by @tottoto in https://github.com/kube-rs/kube/pull/1438</li> </ul>"},{"location":"changelog/#fixed_16","title":"Fixed","text":"<ul> <li>Serialize TerminalSize fields as PascalCase by @nightkr in https://github.com/kube-rs/kube/pull/1407</li> <li><code>Kubeconfig</code> allow <code>certificate_authority_data</code> not present in <code>ExecAuthCluster</code> by @ljun20160606 in https://github.com/kube-rs/kube/pull/1432</li> <li>fix: check err on <code>Client::request_stream</code> by @XciD in https://github.com/kube-rs/kube/pull/1433</li> </ul>"},{"location":"changelog/#0881--2024-01-26","title":"0.88.1 / 2024-01-26","text":""},{"location":"changelog/#whats-changed_19","title":"What's Changed","text":"<p>This is a bug fix release for a deserialization issue introduced in 0.88.0.</p>"},{"location":"changelog/#fixed_17","title":"Fixed","text":"<ul> <li>Minor fixes to <code>ObjectList</code> by @flavio in https://github.com/kube-rs/kube/pull/1398</li> </ul>"},{"location":"changelog/#0880--2024-01-21","title":"0.88.0 / 2024-01-21","text":""},{"location":"changelog/#kubernetes-v1_29-support-via-k8s-openapi-021","title":"Kubernetes <code>v1_29</code> support via <code>k8s-openapi</code> 0.21","text":"<p>Please upgrade k8s-openapi along with kube to avoid conflicts.</p>"},{"location":"changelog/#whats-changed_20","title":"What's Changed","text":""},{"location":"changelog/#added_13","title":"Added","text":"<ul> <li>Add type meta data for list types by @Danil-Grigorev in https://github.com/kube-rs/kube/pull/1380</li> </ul>"},{"location":"changelog/#changed_13","title":"Changed","text":"<ul> <li>Bump MSRV to 1.70 by @clux in https://github.com/kube-rs/kube/pull/1384</li> <li>Upgrade <code>k8s-openapi</code> for Kubernetes <code>v1_29</code> support by @clux in https://github.com/kube-rs/kube/pull/1394</li> </ul>"},{"location":"changelog/#0872--2023-12-22","title":"0.87.2 / 2023-12-22","text":""},{"location":"changelog/#whats-changed_21","title":"What's Changed","text":""},{"location":"changelog/#added_14","title":"Added","text":"<ul> <li>Add support for <code>LogParams::since_time</code> by @clux in https://github.com/kube-rs/kube/pull/1342</li> <li>Provide cluster info to exec plugins by @aviramha in https://github.com/kube-rs/kube/pull/1331</li> <li>Allow setting a description on a derived CRD by @sbernauer in https://github.com/kube-rs/kube/pull/1359</li> </ul>"},{"location":"changelog/#changed_14","title":"Changed","text":"<ul> <li>Bump MSRV from 1.64 to 1.65 by @clux in https://github.com/kube-rs/kube/pull/1353</li> <li>Switch from <code>jsonpath_lib</code> to <code>jsonpath-rust</code> by @ilya-bobyr in https://github.com/kube-rs/kube/pull/1345</li> </ul>"},{"location":"changelog/#0871--2023-11-01","title":"0.87.1 / 2023-11-01","text":""},{"location":"changelog/#headlines","title":"Headlines","text":"<ul> <li>fixed a <code>Controller</code> issue with reconciliation requests disappearing when using <code>concurrency</code> #1324</li> <li>improved <code>Client</code> with better exec auth behaviour #1320, timeout control #1314, and socks5 proxy handling #1311</li> <li>small changes to an unstable streams feature #1304, and a a derive property that is now illegal with <code>syn</code> 2 #1307</li> </ul> <p>Big thanks to everyone involved \ud83c\udf83</p>"},{"location":"changelog/#whats-changed_22","title":"What's Changed","text":""},{"location":"changelog/#added_15","title":"Added","text":"<ul> <li>Feature-flagged support for <code>socks5</code> proxy in <code>Client</code> by @Razz4780 in https://github.com/kube-rs/kube/pull/1311</li> <li>Add example for raw API Server requests by @mateiidavid in https://github.com/kube-rs/kube/pull/1330</li> </ul>"},{"location":"changelog/#changed_15","title":"Changed","text":"<ul> <li>Document <code>Controller::reconcile_on</code>  and remove <code>Err</code> input requirement by @clux in https://github.com/kube-rs/kube/pull/1304</li> <li>Bump <code>base64</code> to <code>0.21</code> by @clux in https://github.com/kube-rs/kube/pull/1308</li> <li>Upgrade <code>darling</code> and <code>syn</code> and rename <code>#[kube(struct)]</code> by @clux in https://github.com/kube-rs/kube/pull/1307</li> </ul>"},{"location":"changelog/#fixed_18","title":"Fixed","text":"<ul> <li>Fixed <code>KUBERNETES_EXEC_INFO</code> environment variable passed to auth plugins by @Razz4780 in https://github.com/kube-rs/kube/pull/1320</li> <li>Fix <code>Controller</code>: pending messages are stuck in the <code>scheduled</code> map by @co42 in https://github.com/kube-rs/kube/pull/1324</li> <li>Set a default write timeout by @alex-hunt-materialize in https://github.com/kube-rs/kube/pull/1314</li> </ul> <p>Full Changelog: https://github.com/kube-rs/kube/compare/0.86.0...0.87.1</p>"},{"location":"changelog/#0860--2023-09-08","title":"0.86.0 / 2023-09-08","text":""},{"location":"changelog/#headlines_1","title":"Headlines","text":""},{"location":"changelog/#k8s-openapi-020-for-kubernetes-v1_28","title":"k8s-openapi 0.20 for Kubernetes <code>v1_28</code>","text":"<p>Please note upstream api removals. As usual, upgrade k8s-openapi along with kube to avoid issues.</p>"},{"location":"changelog/#default-tls-stack-changed-to-rustls","title":"Default TLS stack changed to <code>rustls</code>","text":"<p>With last year's upstream changes from rustls (closing all our existing rustls issues - see https://github.com/kube-rs/kube/issues/1192), this is now the better choice for security, features, and ease of building. The previous default openssl stack can still be used with <code>default-features = false</code> plus the <code>openssl-tls</code> feature.</p>"},{"location":"changelog/#controller-configuration","title":"Controller Configuration","text":"<p>A controller <code>Config</code> has been added to allow tweaking two behaviour parameters (debouncing  in #1265 and concurrency limits in #1277) of the <code>Controller</code>. Huge thanks to @aryan9600 for his work.</p>"},{"location":"changelog/#streaming-lists","title":"Streaming Lists","text":"<p>The <code>sendInitialEvents</code> alpha feature is now supported, and is quickly testable in the pod_watcher example when using the feature gate. This will help optimise the memory profile of controllers when the feature becomes generally available. Amazing work by first time contributor @casualjim.</p>"},{"location":"changelog/#whats-changed_23","title":"What's Changed","text":""},{"location":"changelog/#added_16","title":"Added","text":"<ul> <li>add <code>controller::Config</code> and debounce period to scheduler by @aryan9600 in https://github.com/kube-rs/kube/pull/1265</li> <li>adds watch-list implementation without breaking changes by @casualjim in https://github.com/kube-rs/kube/pull/1255</li> <li>allow configuring controller's concurrency by @aryan9600 in https://github.com/kube-rs/kube/pull/1277</li> </ul>"},{"location":"changelog/#changed_16","title":"Changed","text":"<ul> <li>Change default TLS stack to <code>rustls-tls</code> by @clux in https://github.com/kube-rs/kube/pull/1261</li> <li>Bump k8s-openapi to 0.20.0 by @clux in https://github.com/kube-rs/kube/pull/1291</li> </ul>"},{"location":"changelog/#fixed_19","title":"Fixed","text":"<ul> <li><code>core</code>: omit invalid resource version parameters when doing paged requests by @goenning in https://github.com/kube-rs/kube/pull/1281</li> </ul>"},{"location":"changelog/#0850--2023-08-06","title":"0.85.0 / 2023-08-06","text":""},{"location":"changelog/#whats-changed_24","title":"What's Changed","text":""},{"location":"changelog/#added_17","title":"Added","text":"<ul> <li>Add <code>WatchStreamExt::reflect</code> to allow chaining on a reflector  by @clux in https://github.com/kube-rs/kube/pull/1252</li> <li>Implement ephemeral containers subresource by @jmintb in https://github.com/kube-rs/kube/pull/1153</li> </ul>"},{"location":"changelog/#changed_17","title":"Changed","text":"<ul> <li>Swap <code>dirs-next</code> dependency to cargo-team maintained <code>home</code> crate by @utkarshgupta137 in https://github.com/kube-rs/kube/pull/1207</li> <li>Upgrade <code>k8s-openapi</code> to <code>0.19.0</code> for <code>v1_27</code> support by @clux in https://github.com/kube-rs/kube/pull/1271</li> </ul>"},{"location":"changelog/#fixed_20","title":"Fixed","text":"<ul> <li><code>watcher</code>: return <code>NoResourceVersion</code> error if resource version is empty by @aryan9600 in https://github.com/kube-rs/kube/pull/1259</li> <li>Update the <code>scheduler</code> message when preponing by @nightkr in https://github.com/kube-rs/kube/pull/1260</li> </ul>"},{"location":"changelog/#0840--2023-07-14","title":"0.84.0 / 2023-07-14","text":""},{"location":"changelog/#highlights_8","title":"Highlights","text":""},{"location":"changelog/#stream-improvements","title":"Stream Improvements","text":"<p>On the <code>runtime</code> side, the <code>Controller</code> now delays reconciles until the main <code>Store</code> is ready (via a new <code>Store</code> helper from #1243). The stream selection for owned resources is more efficient (#1240), and the underlying <code>watcher</code> streams now all paginate (#1249). There are also many new  <code>WatchStreamExt</code> helpers ( #1246 + #1228 + #1232) as a continued work towards the more customisable streams-api (#1080).</p> <p>On the client-side; streaming logs are now easier to deal with as an <code>AsyncBufRead</code> #1235.</p>"},{"location":"changelog/#oidc-refresh","title":"OIDC Refresh","text":"<p>Optional OIDC refreshable token support was introduced in #1229 under <code>kube/oidc</code> for out-of-cluster <code>Client</code> configuration. Previously, refresh support was limited to non-OIDC tokens from the <code>GcpOuth</code> provider (<code>kube/oauth</code>) or through arbitrary <code>exec</code> calls / <code>TokenFile</code> loading.</p>"},{"location":"changelog/#whats-changed_25","title":"What's Changed","text":""},{"location":"changelog/#added_18","title":"Added","text":"<ul> <li>Add <code>Predicate</code> trait to allow combination + fallbacks by @clux in https://github.com/kube-rs/kube/pull/1228</li> <li>Added refreshing OIDC ID token as an optional feature by @Razz4780 in https://github.com/kube-rs/kube/pull/1229</li> <li>Add <code>WatchStreamExt::default_backoff</code> shorthand by @clux in https://github.com/kube-rs/kube/pull/1232</li> <li>Derive <code>PartialEq</code> on <code>core</code> params structs by @danrspencer in https://github.com/kube-rs/kube/pull/1237</li> <li>Track store readiness by @nightkr in https://github.com/kube-rs/kube/pull/1243</li> <li>Add <code>WatchStreamExt::modify()</code> to modify events by @aryan9600 in https://github.com/kube-rs/kube/pull/1246</li> <li>Add default pagination to <code>watcher</code> by @clux in https://github.com/kube-rs/kube/pull/1249</li> </ul>"},{"location":"changelog/#changed_18","title":"Changed","text":"<ul> <li>Bump MSRV from 1.63 to 1.64 by @clux in https://github.com/kube-rs/kube/pull/1233</li> <li>Change <code>Api::log_stream</code> to return <code>AsyncBufRead</code> by @aryan9600 in https://github.com/kube-rs/kube/pull/1235</li> </ul>"},{"location":"changelog/#fixed_21","title":"Fixed","text":"<ul> <li>Make <code>Controller::owns</code> use <code>metadata_watcher</code> internally by @clux in https://github.com/kube-rs/kube/pull/1240</li> </ul>"},{"location":"changelog/#0830--2023-06-05","title":"0.83.0 / 2023-06-05","text":""},{"location":"changelog/#whats-changed_26","title":"What's Changed","text":""},{"location":"changelog/#added_19","title":"Added","text":"<ul> <li>Add <code>Controller::reconcile_on</code> by @co42 in https://github.com/kube-rs/kube/pull/1163</li> <li>Add <code>predicates::resource_version</code> by @clux in https://github.com/kube-rs/kube/pull/1221</li> <li>add <code>Duration</code> to <code>kube-core</code> by @hawkw in https://github.com/kube-rs/kube/pull/1224</li> </ul>"},{"location":"changelog/#changed_19","title":"Changed","text":"<ul> <li>Introduce <code>GetParams</code> support by @mateiidavid in https://github.com/kube-rs/kube/pull/1214</li> </ul>"},{"location":"changelog/#fixed_22","title":"Fixed","text":"<ul> <li>Swap <code>validator</code> for <code>garde</code> by @mateiidavid in https://github.com/kube-rs/kube/pull/1212</li> <li>fix: <code>#[kube(crates(serde = \"some_crate::serde\"))]</code> was not working by @chubei in https://github.com/kube-rs/kube/pull/1215</li> </ul>"},{"location":"changelog/#0822--2023-04-19","title":"0.82.2 / 2023-04-19","text":""},{"location":"changelog/#watcher-fixes","title":"Watcher Fixes","text":"<p>Two fixes to allow <code>watcher::Config</code> to function as intended.</p>"},{"location":"changelog/#whats-changed_27","title":"What's Changed","text":""},{"location":"changelog/#fixed_23","title":"Fixed","text":"<ul> <li><code>runtime::watcher</code>: only set rv if semantic is any by @goenning in https://github.com/kube-rs/kube/pull/1204</li> <li><code>watcher::Config</code>: Derive <code>Clone</code> + <code>Debug</code> + <code>PartialEq</code> by @clux in https://github.com/kube-rs/kube/pull/1206</li> </ul> <p>Full Changelog: https://github.com/kube-rs/kube/compare/0.82.1...0.82.2</p>"},{"location":"changelog/#0821--2023-04-14","title":"0.82.1 / 2023-04-14","text":""},{"location":"changelog/#bugfix-release_1","title":"Bugfix Release","text":"<p><code>nullable</code> is re-instated on <code>Option</code> types from <code>CustomResource</code> generated schemas, due to unintended errors removing it caused on <code>Api::patch</code> calls on <code>None</code> members that were not setting <code>#[serde(skip_serializing_if = \"Option::is_none\")]</code>. This only affected 0.81 and 0.82 from last week, and does not require user action regardless of where you are upgrading from.</p> <p>This release also fixes a <code>metadata_watcher</code> triggering deserialization error from doing <code>Api::list_metadata</code> on an empty set.</p>"},{"location":"changelog/#whats-changed_28","title":"What's Changed","text":""},{"location":"changelog/#fixed_24","title":"Fixed","text":"<ul> <li>Fix <code>WatchParams</code> bookmarks for <code>watch_metadata</code> by @clux in https://github.com/kube-rs/kube/pull/1193</li> <li>Fix <code>ObjectList</code> not deserializing <code>items: null</code> by @suryapandian in https://github.com/kube-rs/kube/pull/1199</li> <li>Revert \"kube-derive: Disable <code>option_nullable</code> for CRD generation\" by @Dav1dde in https://github.com/kube-rs/kube/pull/1201</li> </ul>"},{"location":"changelog/#0820--2023-04-08","title":"0.82.0 / 2023-04-08","text":""},{"location":"changelog/#dependency-updates","title":"Dependency Updates","text":"<p>This release brings in the new <code>k8s-openapi</code> release. Be sure to upgrade <code>k8s-openapi</code> and <code>kube</code> simultaneously to avoid multiple version errors:</p> <pre><code>cargo upgrade -p k8s-openapi -p kube -i\n</code></pre>"},{"location":"changelog/#whats-changed_29","title":"What's Changed","text":""},{"location":"changelog/#changed_20","title":"Changed","text":"<ul> <li>Bump <code>serde_yaml</code> to 0.9 by @clux in https://github.com/kube-rs/kube/pull/1188</li> <li>Bump <code>k8s-openapi</code> to 0.18.0 by @clux in https://github.com/kube-rs/kube/pull/1190</li> </ul> <p>Full Changelog: https://github.com/kube-rs/kube/compare/0.81.0...0.82.0</p>"},{"location":"changelog/#0810--2023-04-07","title":"0.81.0 / 2023-04-07","text":""},{"location":"changelog/#highlights_9","title":"Highlights","text":""},{"location":"changelog/#listwatch-changes","title":"List/Watch Changes","text":"<p>One big change is the splitting of <code>ListParams</code> into <code>ListParams</code> and <code>WatchParams</code> in #1162 and #1171. If you were using <code>api.list</code> directly, this should not affect you, but <code>api.watch</code> calls will need a replace of <code>ListParams</code> to <code>WatchParams</code>. Apart from the resulting field splitting, the two structs still have a mostly compatible api.</p> <p>If you were passing <code>ListParams</code> to <code>watcher</code>, you can change this for a new <code>watcher::Config</code> with a mostly compatible interface:</p> <pre><code>-    let stream = watcher(api, ListParams::default());\n+    let stream = watcher(api, watcher::Config::default());\n</code></pre> <p>The reason for this change has been to add support for specific version match strategies and has new builders on both <code>ListParams</code> and <code>watcher::Config</code> to control the strategy. Using the new <code>VersionMatch::NotOlderThan</code> can reduce strain on the apiserver for individual <code>api.list</code> calls. Watchers will benefit the most from this, and should consider using the semantic <code>Any</code> strategy (= <code>NotOlderThan</code> with version \"0\") on all relists by setting <code>watcher::Config::any_semantic()</code>.</p>"},{"location":"changelog/#rustls","title":"rustls","text":"<p>This release closes all our rustls issues as a consequence of the long standing IP address incompatibility (#153) having been resolved upstream. All <code>rustls</code> specific overrides (such as using the deprecated <code>incluster_dns</code> strategy for configuration #1184) have been removed as a result.</p>"},{"location":"changelog/#controller-streams","title":"Controller streams","text":"<p>Multiple new <code>runtime</code> features have been added to be able to more precisely control the input streams used by <code>Controller</code> a starting step towards stream sharing (#1080) and as a way to reduce excess input events. Because these interfaces are likely to remain in flux for some time, these are only available under unstable feature flags.</p>"},{"location":"changelog/#whats-changed_30","title":"What's Changed","text":""},{"location":"changelog/#added_20","title":"Added","text":"<ul> <li>Add <code>predicates</code> to allow filtering <code>watcher</code> streams by @clux in https://github.com/kube-rs/kube/pull/911</li> <li>Add <code>Controller::owns_stream</code> by @Dav1dde in https://github.com/kube-rs/kube/pull/1173</li> <li>Add <code>Controller::for_stream</code> + <code>Controller::watches_stream</code> by @clux in https://github.com/kube-rs/kube/pull/1187</li> </ul>"},{"location":"changelog/#changed_21","title":"Changed","text":"<ul> <li>Split <code>ListParams</code> and <code>WatchParams</code> by @nabokihms in https://github.com/kube-rs/kube/pull/1162</li> <li>Make <code>VersionMatch</code> follow upstream + configure list semantics in <code>watcher::Config</code> by @clux in https://github.com/kube-rs/kube/pull/1171</li> <li>kube-derive: Disable <code>option_nullable</code> for CRD generation by @Dav1dde in https://github.com/kube-rs/kube/pull/1079</li> </ul>"},{"location":"changelog/#fixed_25","title":"Fixed","text":"<ul> <li>Run <code>rustls</code> CI against IP cluster address by @clux in https://github.com/kube-rs/kube/pull/1183</li> <li>Fix:  tower buffer's worker closed unexpectedly by @divinerapier in https://github.com/kube-rs/kube/pull/1185</li> <li>Avoid special <code>Config::incluster</code> behavior for <code>rustls</code> by @clux in https://github.com/kube-rs/kube/pull/1184</li> </ul>"},{"location":"changelog/#0800--2023-03-02","title":"0.80.0 / 2023-03-02","text":""},{"location":"changelog/#notes","title":"Notes","text":"<p>The <code>PartialObjectMeta</code> struct has been changed to allow static dispatch through a new generic parameter. It comes with a new <code>PartialObjectMetaExt</code> trait to help construct it.</p> <p>Early release for the above change to the new metadata api, plus a trigger for our currently broken docs.rs.</p>"},{"location":"changelog/#whats-changed_31","title":"What's Changed","text":""},{"location":"changelog/#changed_22","title":"Changed","text":"<ul> <li>Genericize <code>PartialObjectMeta</code> over the underlying <code>Resource</code> by @clux in https://github.com/kube-rs/kube/pull/1152</li> </ul>"},{"location":"changelog/#fixed_26","title":"Fixed","text":"<ul> <li>Bypass nightly ICE in docs build by @clux in https://github.com/kube-rs/kube/pull/1155</li> </ul> <p>Full Changelog: https://github.com/kube-rs/kube/compare/0.79.0...0.80.0</p>"},{"location":"changelog/#0790--2023-02-23","title":"0.79.0 / 2023-02-23","text":""},{"location":"changelog/#watch-improvements","title":"Watch Improvements","text":"<p>A big feature this time around is the added support for the metadata api via #1137. This is a variant api that only returns the <code>ObjectMeta</code> and <code>TypeMeta</code> to reduce network load, and has a low-level watch analogue available at <code>Api::watch_metadata</code>. Most users will generally want an infinite watch stream rather than the low-level method, so <code>kube::runtime::metadata_watcher</code> has been added as a direct analogue of <code>watcher</code> via #1145. The dynamic_watcher example shows how to switch between the two to get up and running.</p> <p>The <code>watcher</code> also emits warnings now when HTTP <code>403</code>s are encountered from Kubernetes, as this usually indicates a non-transient misconfiguration that must be fixed on the administrator side with RBAC.</p> <p>Finally, there is work in progress on shared streams via <code>WatchStreamExt</code> from #1131 under an unstable feature.</p>"},{"location":"changelog/#whats-changed_32","title":"What's Changed","text":""},{"location":"changelog/#added_21","title":"Added","text":"<ul> <li>Client: expose <code>default_namespace()</code> by @jpmcb in https://github.com/kube-rs/kube/pull/1123</li> <li>Add support for metadata API by @mateiidavid in https://github.com/kube-rs/kube/pull/1137</li> <li>Runtime: Add <code>WatchStreamExt::subscribe</code> by @danrspencer in https://github.com/kube-rs/kube/pull/1131</li> <li>Introduce support for persistent metadata watches by @mateiidavid in https://github.com/kube-rs/kube/pull/1145</li> </ul>"},{"location":"changelog/#changed_23","title":"Changed","text":"<ul> <li>Bump Rust MSRV to 1.63.0 by @mateiidavid in https://github.com/kube-rs/kube/pull/1146</li> </ul>"},{"location":"changelog/#fixed_27","title":"Fixed","text":"<ul> <li><code>Config</code>: make cluster/users/clusters optional by @goenning in https://github.com/kube-rs/kube/pull/1120</li> <li>Add better logging for watcher errors by @clux in https://github.com/kube-rs/kube/pull/1134</li> <li>kubeconfig: deserialize null vectors as default by @goenning in https://github.com/kube-rs/kube/pull/1142</li> </ul>"},{"location":"changelog/#0780--2023-01-06","title":"0.78.0 / 2023-01-06","text":""},{"location":"changelog/#kubernetes-bump","title":"Kubernetes Bump","text":"<p>This release brings in the new <code>k8s-openapi</code> release for <code>1.26</code> structs, and sets our MK8SV to <code>1.21</code>. Be sure to upgrade <code>k8s-openapi</code> and <code>kube</code> simultaneously to avoid multiple version errors:</p> <pre><code>cargo upgrade -p k8s-openapi -p kube -i\n</code></pre>"},{"location":"changelog/#whats-changed_33","title":"What's Changed","text":""},{"location":"changelog/#added_22","title":"Added","text":"<ul> <li>reflector: add helper function to the <code>Store</code> by @eliad-wiz in https://github.com/kube-rs/kube/pull/1111</li> </ul>"},{"location":"changelog/#changed_24","title":"Changed","text":"<ul> <li>Bump <code>k8s-openapi@0.17.0</code> and MK8SV by @clux in https://github.com/kube-rs/kube/pull/1116</li> </ul>"},{"location":"changelog/#removed_4","title":"Removed","text":"<ul> <li>Remove deprecated <code>Config::timeout</code> by @clux in https://github.com/kube-rs/kube/pull/1113</li> </ul>"},{"location":"changelog/#fixed_28","title":"Fixed","text":"<ul> <li>fix shell exec exiting message loop when terminalSizeReceiver is dropped by @erebe in https://github.com/kube-rs/kube/pull/1112</li> </ul>"},{"location":"changelog/#0770--2022-12-15","title":"0.77.0 / 2022-12-15","text":""},{"location":"changelog/#highlights_10","title":"Highlights","text":"<p>This release saw numerous improvements across various parts of the codebase with lots of help from external contributors. Look for improvements in error handling, client exec behaviour, dynamic object conversion, certificate handling, and last, but not least; lots of enhancements in the <code>config</code> module. Huge thanks to everyone who contributed!</p>"},{"location":"changelog/#config-enhancements","title":"<code>Config</code> Enhancements","text":"<p>Kubeconfigs relying on <code>ExecConfig</code> for auth should now work with a lot more cases (with improvements to script interactivity, cert passing, env-drop, and windows behaviour). We further aligned our <code>Kubeconfig</code> parsing with client-go's behaviour, and also exposed <code>Kubeconfig::merge</code>. Finally, we now pass <code>Config::tls_server_name</code> through to the <code>Client</code>, which has let us include a better rustls workaround for the long-standing ip issue (enabled by default).</p>"},{"location":"changelog/#whats-changed_34","title":"What's Changed","text":""},{"location":"changelog/#added_23","title":"Added","text":"<ul> <li>Add <code>DynamicObjects::try_parse</code> for typed object conversion by @jmintb in https://github.com/kube-rs/kube/pull/1061</li> <li>Add <code>ExecConfig::drop_env</code> to filter host evars for auth providers by @aviramha in https://github.com/kube-rs/kube/pull/1062</li> <li>Add support for terminal size when executing command inside a container by @armandpicard in https://github.com/kube-rs/kube/pull/983</li> <li>add cmd-drop-env to AuthProviderConfig by @aviramha in https://github.com/kube-rs/kube/pull/1074</li> <li>Check for client cert with exec by @rcanderson23 in https://github.com/kube-rs/kube/pull/1089</li> <li>Change <code>Kubeconfig::merge</code> fn to public. by @goenning in https://github.com/kube-rs/kube/pull/1100</li> <li>Fix interactivity in auth exec by @armandpicard in https://github.com/kube-rs/kube/pull/1083</li> </ul>"},{"location":"changelog/#changed_25","title":"Changed","text":"<ul> <li>[windows] skip window creation on auth exec by @goenning in https://github.com/kube-rs/kube/pull/1095</li> <li>Add <code>Config::tls_server_name</code> and validate when using rustls by @clux in https://github.com/kube-rs/kube/pull/1104</li> </ul>"},{"location":"changelog/#removed_5","title":"Removed","text":"<ul> <li>Remove deprecated <code>ResourceExt::name</code> by @clux in https://github.com/kube-rs/kube/pull/1105</li> </ul>"},{"location":"changelog/#fixed_29","title":"Fixed","text":"<ul> <li>Bump tracing dependency to 0.1.36 by @teozkr in https://github.com/kube-rs/kube/pull/1070</li> <li>Improve error message on azure auth not being supported by @goenning in https://github.com/kube-rs/kube/pull/1082</li> <li>exec: ensure certs always end with a new line by @goenning in https://github.com/kube-rs/kube/pull/1096</li> <li>fix: align kube-rs with client-go config parsing by @goenning in https://github.com/kube-rs/kube/pull/1077</li> <li>Return error from <code>watcher</code> when kinds do not support watch by @clux in https://github.com/kube-rs/kube/pull/1101</li> </ul>"},{"location":"changelog/#0760--2022-10-28","title":"0.76.0 / 2022-10-28","text":""},{"location":"changelog/#highlights_11","title":"Highlights","text":""},{"location":"changelog/#derivecustomresource-now-supports-schemas-with-untagged-enums","title":"<code>#[derive(CustomResource)]</code> now supports schemas with untagged enums","text":"<p>Expanding on our existing support for storing Rust's struct enums in CRDs, Kube will now try to convert <code>#[serde(untagged)]</code> enums as well. Note that if the same field is present in multiple untagged variants then they must all have the same shape.</p>"},{"location":"changelog/#removed-deprecated-try_flatten_-functions","title":"Removed deprecated <code>try_flatten_*</code> functions","text":"<p>These have been deprecated since 0.72, and are replaced by the equivalent <code>WatchStreamExt</code> methods.</p>"},{"location":"changelog/#whats-changed_35","title":"What's Changed","text":""},{"location":"changelog/#added_24","title":"Added","text":"<ul> <li>Adds example to <code>Controller::watches</code> by @Dav1dde in https://github.com/kube-rs/kube/pull/1026</li> <li>Discovery: Add <code>ApiGroup::resources_by_stability</code> by @imuxin in https://github.com/kube-rs/kube/pull/1022</li> <li>Add support for untagged enums in CRDs by @sbernauer in https://github.com/kube-rs/kube/pull/1028</li> <li>Derive PartialEq for DynamicObject by @pbzweihander in https://github.com/kube-rs/kube/pull/1048</li> </ul>"},{"location":"changelog/#removed_6","title":"Removed","text":"<ul> <li>Runtime: Remove deprecated util <code>try_flatten_</code> helpers by @clux in https://github.com/kube-rs/kube/pull/1019</li> <li>Remove <code>native-tls</code> feature by @kazk in https://github.com/kube-rs/kube/pull/1044</li> </ul>"},{"location":"changelog/#fixed_30","title":"Fixed","text":"<ul> <li>add fieldManager querystring to all operations by @goenning in https://github.com/kube-rs/kube/pull/1031</li> <li>Add verify_tls1x_signature for NoCertVerification by @rvql in https://github.com/kube-rs/kube/pull/1034</li> <li>Fix compatibility with schemars' preserve_order feature by @teozkr in https://github.com/kube-rs/kube/pull/1050</li> <li>Hoist enum values from subschemas by @teozkr in https://github.com/kube-rs/kube/pull/1051</li> </ul>"},{"location":"changelog/#0750--2022-09-21","title":"0.75.0 / 2022-09-21","text":""},{"location":"changelog/#highlights_12","title":"Highlights","text":""},{"location":"changelog/#upgrade-k8s-openapi-to-016-for-kubernetes-125","title":"Upgrade <code>k8s-openapi</code> to 0.16 for Kubernetes 1.25","text":"<p>The update to k8s-openapi@0.16.0 makes this the first release with tentative Kubernetes 1.25 support. While the new structs and apis now exist, we recommend holding off on using 1.25 until a deserialization bug in the apiserver is resolved upstream. See #997 / #1008 for details.</p> <p>To upgrade, ensure you bump both <code>kube</code> and <code>k8s-openapi</code>:</p> <pre><code>cargo upgrade kube k8s-openapi\n</code></pre>"},{"location":"changelog/#newold-configincluster-default-to-connect-in-cluster","title":"New/Old <code>Config::incluster</code> default to connect in cluster","text":"<p>Our previous default of connecting to the Kubernetes apiserver via <code>kubernetes.default.svc</code> has been reverted back to use the old environment variables after Kubernetes updated their position that the environment variables are not legacy. This does unfortunately regress on <code>rustls</code> support, so for those users we have included a <code>Config::incluster_dns</code> to work around the old rustls issue while it is open.</p>"},{"location":"changelog/#controller-error_policy-extension","title":"Controller <code>error_policy</code> extension","text":"<p>The <code>error_policy</code> fn now has access to the <code>object</code> that failed the reconciliation to ease metric creation / failure attribution. The following change is needed on the user side:</p> <pre><code>-fn error_policy(error: &amp;Error, ctx: Arc&lt;Data&gt;) -&gt; Action {\n+fn error_policy(_obj: Arc&lt;YourObject&gt;, error: &amp;Error, ctx: Arc&lt;Data&gt;) -&gt; Action {\n</code></pre>"},{"location":"changelog/#polish--subresources--conversion","title":"Polish / Subresources / Conversion","text":"<p>There are also a slew of ergonomics improvements, closing of gaps in subresources, adding initial support for <code>ConversionReview</code>, making <code>Api::namespaced</code> impossible to use for non-namepaced resources (a common pitfall), as well as many great fixes to the edge cases in portforwarding and finalizers. Many of these changes came from first time contributors. A huge thank you to everyone involved.</p>"},{"location":"changelog/#whats-changed_36","title":"What's Changed","text":""},{"location":"changelog/#added_25","title":"Added","text":"<ul> <li>Make <code>Config::auth_info</code> public by @danrspencer in https://github.com/kube-rs/kube/pull/959</li> <li>Make raw <code>Client::send</code> method public by @tiagolobocastro in https://github.com/kube-rs/kube/pull/972</li> <li>Make <code>types</code> on <code>AdmissionRequest</code> and <code>AdmissionResponse</code> public by @clux in https://github.com/kube-rs/kube/pull/977</li> <li>Add <code>#[serde(default)]</code> to metadata field of <code>DynamicObject</code> by @pbzweihander in https://github.com/kube-rs/kube/pull/987</li> <li>Add <code>create_subresource</code> method to <code>Api</code> and <code>create_token_request</code> method to <code>Api&lt;ServiceAccount&gt;</code> by @pbzweihander in https://github.com/kube-rs/kube/pull/989</li> <li>Controller: impl Eq and PartialEq for <code>Action</code> by @Sherlock-Holo in https://github.com/kube-rs/kube/pull/993</li> <li>Add support for CRD <code>ConversionReview</code> types by @MikailBag in https://github.com/kube-rs/kube/pull/999</li> </ul>"},{"location":"changelog/#changed_26","title":"Changed","text":"<ul> <li>Constrain Resource trait and Api::namespaced by Scope by @clux in https://github.com/kube-rs/kube/pull/956</li> <li>Add connect/read/write timeouts to <code>Config</code> by @goenning in https://github.com/kube-rs/kube/pull/971</li> <li>Controller: Include the object being reconciled in the <code>error_policy</code> by @felipesere in https://github.com/kube-rs/kube/pull/995</li> <li><code>Config</code>: New <code>incluster</code> and <code>incluster_dns</code> constructors by @olix0r in https://github.com/kube-rs/kube/pull/1001</li> <li>Upgrade <code>k8s-openapi</code> to 0.16 by @clux in https://github.com/kube-rs/kube/pull/1008</li> </ul>"},{"location":"changelog/#fixed_31","title":"Fixed","text":"<ul> <li>Remove <code>tracing::instrument</code> from <code>apply_debug_overrides</code> by @kazk in https://github.com/kube-rs/kube/pull/958</li> <li>fix duplicate finalizers race condition by @alex-hunt-materialize in https://github.com/kube-rs/kube/pull/965</li> <li>fix: portforward connection cleanup by @tiagolobocastro in https://github.com/kube-rs/kube/pull/973</li> </ul>"},{"location":"changelog/#0740--2022-07-09","title":"0.74.0 / 2022-07-09","text":""},{"location":"changelog/#highlights_13","title":"Highlights","text":""},{"location":"changelog/#polish-bug-fixes-guidelines-ci-improvements-and-new-contributors","title":"Polish, bug fixes, guidelines, ci improvements, and new contributors","text":"<p>This release features smaller improvements/additions/cleanups/fixes, many of which are from new first-time contributors! Thank you everyone! The listed deadlock fix was backported to 0.73.1.</p> <p>We have also been trying to clarify and prove a lot more of our external-facing guarantees, and as a result:</p> <ul> <li>We have codified our Kubernetes versioning policy </li> <li>The Rust version policy has extended its support range</li> <li>Our CI has been extended</li> </ul>"},{"location":"changelog/#resourceextname-deprecation","title":"<code>ResourceExt::name</code> deprecation","text":"<p>A consequence of all the policy writing and the improved clarity we have decided to deprecate the common <code>ResourceExt::name</code> helper.</p> <p>This method could panic and it is unexpected for the users and bad for our consistency. To get the old functionality, you can replace any <code>.name()</code> call on a Kubernetes resources with <code>.name_unchecked()</code>; but as the name implies, it can panic (in a local setting, or during admission). We recommend you replace it with the new <code>ResourceExt::name_any</code> for a general identifier:</p> <pre><code>-pod.name()\n+pod.name_any()\n</code></pre>"},{"location":"changelog/#whats-changed_37","title":"What's Changed","text":""},{"location":"changelog/#added_26","title":"Added","text":"<ul> <li>Add support for passing the <code>fieldValidation</code> query parameter on patch by @phroggyy in https://github.com/kube-rs/kube/pull/929</li> <li>Add <code>conditions::is_job_completed</code> by @clux in https://github.com/kube-rs/kube/pull/935</li> </ul>"},{"location":"changelog/#changed_27","title":"Changed","text":"<ul> <li>Deprecate <code>ResourceExt::name</code> in favour of safe name_* alternatives by @clux in https://github.com/kube-rs/kube/pull/945</li> </ul>"},{"location":"changelog/#removed_7","title":"Removed","text":"<ul> <li>Remove <code>#[kube(apiextensions)]</code> flag from <code>kube-derive</code> by @clux in https://github.com/kube-rs/kube/pull/920</li> </ul>"},{"location":"changelog/#fixed_32","title":"Fixed","text":"<ul> <li>Document every public derived fn from kube-derive by @clux in https://github.com/kube-rs/kube/pull/919</li> <li>fix applier hangs which can happen with many watched objects by @moustafab in https://github.com/kube-rs/kube/pull/925</li> <li>Applier: Improve reconciler reschedule context to avoid deadlocking on full channel by @teozkr in https://github.com/kube-rs/kube/pull/932</li> <li>Fix deserialization issue in AdmissionResponse by @clux in https://github.com/kube-rs/kube/pull/939</li> <li>Admission controller example fixes by @Alibirb in https://github.com/kube-rs/kube/pull/950</li> </ul>"},{"location":"changelog/#0731--2022-06-03","title":"0.73.1 / 2022-06-03","text":""},{"location":"changelog/#highlights_14","title":"Highlights","text":"<p>This patch release fixes a bug causing <code>applier</code> and <code>Controller</code> to deadlock when too many Kubernetes object change events were ingested at once. All users of <code>applier</code> and <code>Controller</code> are encouraged to upgrade as quickly as possible. Older versions are also affected, this bug is believed to have existed since the original release of <code>kube_runtime</code>.</p>"},{"location":"changelog/#whats-changed_38","title":"What's Changed","text":""},{"location":"changelog/#fixed_33","title":"Fixed","text":"<ul> <li>[0.73 backport] fix applier hangs which can happen with many watched objects (#925) by @moustafab (backported by @teozkr) in https://github.com/kube-rs/kube/pull/927</li> </ul> <p>Full Changelog: https://github.com/kube-rs/kube/compare/0.73.0...0.73.1</p>"},{"location":"changelog/#0730--2022-05-23","title":"0.73.0 / 2022-05-23","text":""},{"location":"changelog/#highlights_15","title":"Highlights","text":""},{"location":"changelog/#new-k8s-openapi-version-and-msrv","title":"New <code>k8s-openapi</code> version and MSRV","text":"<p>Support added for Kubernetes <code>v1_24</code> support via the new <code>k8s-openapi</code> version. Please also run <code>cargo upgrade --workspace k8s-openapi</code> when upgrading <code>kube</code>.</p> <p>This also bumps our MSRV to <code>1.60.0</code>.</p>"},{"location":"changelog/#reconciler-change","title":"Reconciler change","text":"<p>A small ergonomic change in the <code>reconcile</code> signature has removed the need for the <code>Context</code> object. This has been replaced by an <code>Arc</code>. The following change is needed in your controller:</p> <pre><code>-async fn reconcile(doc: Arc&lt;MyObject&gt;, context: Context&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt;\n+async fn reconcile(doc: Arc&lt;MyObject&gt;, context: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt;\n</code></pre> <p>This will simplify the usage of the <code>context</code> argument. You should no longer need to pass <code>.get_ref()</code> on its every use. See the controller-rs upgrade change for details.</p>"},{"location":"changelog/#whats-changed_39","title":"What's Changed","text":""},{"location":"changelog/#added_27","title":"Added","text":"<ul> <li>Add Discovery::groups_alphabetical following kubectl sort order by @clux in https://github.com/kube-rs/kube/pull/887</li> </ul>"},{"location":"changelog/#changed_28","title":"Changed","text":"<ul> <li>Replace runtime::controller::Context with Arc by @teozkr in https://github.com/kube-rs/kube/pull/910</li> <li>runtime: Return the object from <code>await_condition</code> by @olix0r in https://github.com/kube-rs/kube/pull/877</li> <li>Bump k8s-openapi to 0.15 for kubernetes v1_24 and bump MSRV to 1.60 by @clux in https://github.com/kube-rs/kube/pull/916</li> </ul>"},{"location":"changelog/#0720--2022-05-13","title":"0.72.0 / 2022-05-13","text":""},{"location":"changelog/#highlights_16","title":"Highlights","text":""},{"location":"changelog/#ergonomics-improvements","title":"Ergonomics improvements","text":"<p>A new <code>runtime::WatchSteamExt</code> (#899 + #906) allows for simpler setups for streams from <code>watcher</code> or <code>reflector</code>.</p> <pre><code>- let stream = utils::try_flatten_applied(StreamBackoff::new(watcher(api, lp), b));\n+ let stream = watcher(api, lp).backoff(b).applied_objects();\n</code></pre> <p>The <code>util::try_flatten_*</code> helpers have been marked as deprecated since they are not used by the stream impls.</p> <p>A new <code>reflector:store()</code> fn allows simpler reflector setups #907:</p> <pre><code>- let store = reflector::store::Writer::&lt;Node&gt;::default();\n- let reader = store.as_reader();\n+ let (reader, writer) = reflector::store();\n</code></pre> <p>Additional conveniences getters/settes to <code>ResourceExt</code> for manged_fields and creation_timestamp #888 + #898, plus a <code>GroupVersion::with_kind</code> path to a GVK, and a <code>TryFrom&lt;TypeMeta&gt; for GroupVersionKind</code> in #896.</p>"},{"location":"changelog/#crd-version-selection","title":"CRD Version Selection","text":"<p>Managing multiple version in CustomResourceDefinitions can be pretty complicated, but we now have helpers and docs on how to tackle it.</p> <p>A new function <code>kube::core::crd::merge_crds</code> have been added (in #889) to help push crd schemas generated by kube-derived crds with different <code>#[kube(version)]</code> properties. See the kube-derive#version documentation for details.</p> <p>A new example showcases how one can manage two or more versions of a crd and what the expected truncation outcomes are when moving between versions.</p>"},{"location":"changelog/#examples","title":"Examples","text":"<p>Examples now have moved to <code>tracing</code> for its logging, respects <code>RUST_LOG</code>, and namespace selection via the kubeconfig context. There is also a larger kubectl example showcasing <code>kubectl apply -f yaml</code> as well as <code>kubectl {edit,delete,get,watch}</code> via #885 + #897.</p>"},{"location":"changelog/#whats-changed_40","title":"What's Changed","text":""},{"location":"changelog/#added_28","title":"Added","text":"<ul> <li>Allow merging multi-version CRDs into a single schema by @clux in https://github.com/kube-rs/kube/pull/889</li> <li>Add GroupVersion::with_kind and TypeMeta -&gt; GroupVersionKind converters by @clux in https://github.com/kube-rs/kube/pull/896</li> <li>Add managed_fields accessors to ResourceExt by @clux in https://github.com/kube-rs/kube/pull/898</li> <li>Add ResourceExt::creation_timestamp by @clux in https://github.com/kube-rs/kube/pull/888</li> <li>Support lowercase http_proxy &amp; https_proxy evars by @DevineLiu in https://github.com/kube-rs/kube/pull/892</li> <li>Add a WatchStreamExt trait for stream chaining by @clux in https://github.com/kube-rs/kube/pull/899</li> <li>Add Event::modify + reflector::store helpers by @clux in https://github.com/kube-rs/kube/pull/907</li> </ul>"},{"location":"changelog/#changed_29","title":"Changed","text":"<ul> <li>Switch to kubernetes cluster dns for incluster url everywhere by @clux in https://github.com/kube-rs/kube/pull/876</li> <li>Update tower-http requirement from 0.2.0 to 0.3.2 by @dependabot in https://github.com/kube-rs/kube/pull/893</li> </ul>"},{"location":"changelog/#removed_8","title":"Removed","text":"<ul> <li>Remove deprecated legacy crd v1beta1 by @clux in https://github.com/kube-rs/kube/pull/890</li> </ul>"},{"location":"changelog/#0710--2022-04-12","title":"0.71.0 / 2022-04-12","text":""},{"location":"changelog/#highlights_17","title":"Highlights","text":"<p>Several quality of life changes and improvement this release for port-forwarding, a new <code>ClientBuilder</code>, better handling of <code>kube-derive</code> edge-cases.</p> <p>We highlight some changes here that you should be especially aware of.</p>"},{"location":"changelog/#eventsrecorder-publishing-to-kube-system-for-cluster-scoped-resources","title":"events::Recorder publishing to <code>kube-system</code> for cluster scoped resources","text":"<p>Publishing events via Recorder for cluster scoped resources (supported since <code>0.70.0</code>) now publish to <code>kube-system</code> rather than <code>default</code>, as all but the newest clusters struggle with publishing events in the <code>default</code> namespace.</p>"},{"location":"changelog/#default-tls-stack-set-to-openssl","title":"Default TLS stack set to OpenSSL","text":"<p>The previous <code>native-tls</code> default  was there because we used to depend on <code>reqwest</code>, but because we depended on openssl anyway the feature does not make much sense. Changing to <code>openssl-tls</code> also improves the situation on macOS where the Security Framework struggles with PKCS#12 certs from OpenSSL v3.  The <code>native-tls</code> feature will still be available in this release in case of issues, but the plan is to decommission it shortly. Of course, we all ideally want to move to rustls, but we are still blocked by #153.</p>"},{"location":"changelog/#whats-changed_41","title":"What's Changed","text":""},{"location":"changelog/#added_29","title":"Added","text":"<ul> <li>Add <code>ClientBuilder</code> that lets users add custom middleware without full stack replacement by @teozkr in https://github.com/kube-rs/kube/pull/855</li> <li>Support top-level enums in CRDs by @sbernauer in https://github.com/kube-rs/kube/pull/856</li> </ul>"},{"location":"changelog/#changed_30","title":"Changed","text":"<ul> <li>portforward: Improve API and support background task cancelation by @olix0r in https://github.com/kube-rs/kube/pull/854</li> <li>Make remote commands cancellable and remove panics by @kazk in https://github.com/kube-rs/kube/pull/861</li> <li>Change the default TLS to OpenSSL by @kazk in https://github.com/kube-rs/kube/pull/863</li> <li>change event recorder cluster namespace to kube-system by @clux in https://github.com/kube-rs/kube/pull/871</li> </ul>"},{"location":"changelog/#fixed_34","title":"Fixed","text":"<ul> <li>Fix schemas containing both properties and additionalProperties by @jcaesar in https://github.com/kube-rs/kube/pull/845</li> <li>Make dependency pins between sibling crates stricter by @clux in https://github.com/kube-rs/kube/pull/864</li> <li>Fix in-cluster kube_host_port generation for IPv6 by @somnusfish in https://github.com/kube-rs/kube/pull/875</li> </ul>"},{"location":"changelog/#0700--2022-03-20","title":"0.70.0 / 2022-03-20","text":""},{"location":"changelog/#highlights_18","title":"Highlights","text":""},{"location":"changelog/#support-for-ec-keys-with-rustls","title":"Support for EC keys with rustls","text":"<p>This was one of the big blockers for using <code>rustls</code> against clusters like <code>k3d</code> or <code>k3s</code> While not sufficient to fix using those clusters out of the box, it is now possible to use them with a workarodund</p>"},{"location":"changelog/#more-ergonomic-reconciler","title":"More ergonomic reconciler","text":"<p>The signature and end the <code>Ok</code> action in <code>reconcile</code> fns has been simplified slightly, and requires the following user updates:</p> <pre><code>-async fn reconcile(obj: Arc&lt;MyObject&gt;, ctx: Context&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction, Error&gt; {\n-    ...\n-    Ok(ReconcilerAction {\n-        requeue_after: Some(Duration::from_secs(300)),\n-    })\n+async fn reconcile(obj: Arc&lt;MyObject&gt;, ctx: Context&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt; {\n+    ...\n+    Ok(Action::requeue(Duration::from_secs(300)))\n</code></pre> <p>The <code>Action</code> import lives in the same place as the old <code>ReconcilerAction</code>.</p>"},{"location":"changelog/#whats-changed_42","title":"What's Changed","text":""},{"location":"changelog/#added_30","title":"Added","text":"<ul> <li>Add support for EC private keys by @farcaller in https://github.com/kube-rs/kube/pull/804</li> <li>Add helper for creating a controller owner_ref on Resource by @clux in https://github.com/kube-rs/kube/pull/850</li> </ul>"},{"location":"changelog/#changed_31","title":"Changed","text":"<ul> <li>Remove <code>scheduler::Error</code> by @teozkr in https://github.com/kube-rs/kube/pull/827</li> <li>Bump parking_lot to 0.12, but allow dep duplicates by @clux in https://github.com/kube-rs/kube/pull/836</li> <li>Update tokio-tungstenite requirement from 0.16.1 to 0.17.1 by @dependabot in https://github.com/kube-rs/kube/pull/841</li> <li>Let OccupiedEntry::commit take PostParams by @teozkr in https://github.com/kube-rs/kube/pull/842</li> <li>Change ReconcileAction to Action and add associated ctors by @clux in https://github.com/kube-rs/kube/pull/851</li> </ul>"},{"location":"changelog/#fixed_35","title":"Fixed","text":"<ul> <li>Token reloading with RwLock by @kazk in https://github.com/kube-rs/kube/pull/835</li> <li>Fix event publishing for cluster scoped crds by @zhrebicek in https://github.com/kube-rs/kube/pull/847</li> <li>Fix invalid CRD when Enum variants have descriptions by @sbernauer in https://github.com/kube-rs/kube/pull/852</li> </ul>"},{"location":"changelog/#0691--2022-02-16","title":"0.69.1 / 2022-02-16","text":""},{"location":"changelog/#highlights_19","title":"Highlights","text":"<p>This is an emergency patch release fixing a bug in 0.69.0 where a <code>kube::Client</code> would deadlock after running inside a cluster for about a minute (#829).</p> <p>All users of 0.69.0 are encouraged to upgrade immediately. 0.68.x and below are not affected.</p>"},{"location":"changelog/#whats-changed_43","title":"What's Changed","text":""},{"location":"changelog/#fixed_36","title":"Fixed","text":"<ul> <li>[0.69.x] Fix deadlock in token reloading by @clux (backported by @teozkr) in https://github.com/kube-rs/kube/pull/831</li> </ul>"},{"location":"changelog/#0690--2022-02-14","title":"0.69.0 / 2022-02-14","text":""},{"location":"changelog/#highlights_20","title":"Highlights","text":""},{"location":"changelog/#ergonomic-additions-to-api","title":"Ergonomic Additions to Api","text":"<p>Two new methods have been added to the client <code>Api</code> this release to reduce the amount of boiler-plate needed for common patterns.</p> <ul> <li><code>Api::entry</code> via 811 - to aid idempotent crud operation flows (following the style of <code>Map::Entry</code>)</li> <li><code>Api::get_opt</code> via 809 - to aid dealing with the <code>NotFound</code> type error via a returned <code>Option</code></li> </ul>"},{"location":"changelog/#in-cluster-token-reloading","title":"In-cluster Token reloading","text":"<p>Following a requirement for Kubernetes clients against versions <code>&gt;= 1.22.0</code>, our bundled <code>AuthLayer</code> will reload tokens every minute when deployed in-cluster.</p>"},{"location":"changelog/#whats-changed_44","title":"What's Changed","text":""},{"location":"changelog/#added_31","title":"Added","text":"<ul> <li>Add conversion for <code>ObjectRef&lt;K&gt;</code> to <code>ObjectReference</code> by @teozkr in https://github.com/kube-rs/kube/pull/815</li> <li>Add <code>Api::get_opt</code> for better existence handling by @teozkr in https://github.com/kube-rs/kube/pull/809</li> <li>Entry API by @teozkr in https://github.com/kube-rs/kube/pull/811</li> </ul>"},{"location":"changelog/#changed_32","title":"Changed","text":"<ul> <li>Reload token file at least once a minute by @kazk in https://github.com/kube-rs/kube/pull/768</li> <li>Prefer kubeconfig over in-cluster config by @teozkr in https://github.com/kube-rs/kube/pull/823</li> </ul>"},{"location":"changelog/#fixed_37","title":"Fixed","text":"<ul> <li>Disable CSR utilities on K8s &lt;1.19 by @teozkr in https://github.com/kube-rs/kube/pull/817</li> </ul>"},{"location":"changelog/#0680--2022-02-01","title":"0.68.0 / 2022-02-01","text":""},{"location":"changelog/#interface-changes","title":"Interface Changes","text":"<p>To reduce the amount of allocation done inside the <code>runtime</code> by reflectors and controllers, the following change via #786 is needed on the signature of your <code>reconcile</code> functions:</p> <pre><code>-async fn reconcile(myobj: MyK, ctx: Context&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction&gt;\n+async fn reconcile(myobj: Arc&lt;MyK&gt;, ctx: Context&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction&gt;\n</code></pre> <p>This also affects the finalizer helper.</p>"},{"location":"changelog/#port-forwarding","title":"Port-forwarding","text":"<p>As one of the last steps toward gold level client requirements, port-forwarding landed in #446. There are 3 new examples (<code>port_forward*.rs</code>) that showcases how to use this websocket based functionality.</p>"},{"location":"changelog/#whats-changed_45","title":"What's Changed","text":""},{"location":"changelog/#added_32","title":"Added","text":"<ul> <li>Add a VS Code devcontainer configuration by @olix0r in https://github.com/kube-rs/kube/pull/788</li> <li>Add support for user impersonation by @teozkr in https://github.com/kube-rs/kube/pull/797</li> <li>Add port forward by @kazk in https://github.com/kube-rs/kube/pull/446</li> </ul>"},{"location":"changelog/#changed_33","title":"Changed","text":"<ul> <li>runtime: Store resources in an <code>Arc</code> by @olix0r in https://github.com/kube-rs/kube/pull/786</li> <li>Propagate Arc through the finalizer reconciler helper by @teozkr in https://github.com/kube-rs/kube/pull/792</li> <li>Disable unused default features of chrono crate by @dreamer in https://github.com/kube-rs/kube/pull/801</li> </ul>"},{"location":"changelog/#fixed_38","title":"Fixed","text":"<ul> <li>Use absolute path to Result in derives by @teozkr in https://github.com/kube-rs/kube/pull/795</li> <li>core: add missing reason to Display on Error::Validation in Request by @clux in https://github.com/kube-rs/kube/pull/798</li> </ul>"},{"location":"changelog/#0670--2022-01-25","title":"0.67.0 / 2022-01-25","text":""},{"location":"changelog/#changed_34","title":"Changed","text":"<ul> <li>runtime: Replace <code>DashMap</code> with a locked <code>AHashMap</code> by @olix0r in https://github.com/kube-rs/kube/pull/785</li> <li>update k8s-openapi for kubernetes 1.23 support by @clux in https://github.com/kube-rs/kube/pull/789</li> </ul>"},{"location":"changelog/#0660--2022-01-15","title":"0.66.0 / 2022-01-15","text":"<p>Tons of ergonomics improvements, and 3 new contributors. Highlighted first is the 3 most discussed changes:</p>"},{"location":"changelog/#support-for-auto-generating-schemas-for-enums-in-kube-derive","title":"Support for auto-generating schemas for enums in <code>kube-derive</code>","text":"<p>It is now possible to embed complex enums inside structs that use <code>#[derive(CustomResource)]</code>.</p> <p>This has been a highly requested feature since the inception of auto-generated schemas. It does not work for all cases, and has certain ergonomics caveats, but represents a huge step forwards.</p> <p>Note that if you depend on <code>kube-derive</code> directly rather than via <code>kube</code> then you must now add the <code>schema</code> feature to <code>kube-core</code></p>"},{"location":"changelog/#new-streambackoff-mechanism-in-kube-runtime","title":"New <code>StreamBackoff</code> mechanism in <code>kube-runtime</code>","text":"<p>To avoid spamming the apiserver when on certain watch errors cases, it's now possible to stream wrap the <code>watcher</code> to set backoffs. The new <code>default_backoff</code> follows existing <code>client-go</code> conventions of being kind to the apiserver.</p> <p>Initially, this is default-enabled in <code>Controller</code> watches (configurable via <code>Controller::trigger_backoff</code>) and avoids spam errors when crds are not installed.</p>"},{"location":"changelog/#new-version-priority-parser-in-kube-core","title":"New version priority parser in <code>kube-core</code>","text":"<p>To aid users picking the most appropriate version of a <code>kind</code> from api discovery or through a CRD, two new sort orders have been exposed on the new <code>kube_core::Version</code></p> <ul> <li><code>Version::priority</code> implementing kubernetes version priority</li> <li><code>Version::generation</code> implementing a more traditional; generational sort (highest version)</li> </ul>"},{"location":"changelog/#changes","title":"Changes","text":"<p>Merged PRs from github release.</p>"},{"location":"changelog/#added_33","title":"Added","text":"<ul> <li>Add <code>DeleteParams</code> constructors for easily setting <code>PropagationPolicy</code> by @kate-goldenring in https://github.com/kube-rs/kube/pull/757</li> <li>Add Serialize to ObjecList and add field-selector and jsonpath example by @ChinYing-Li in https://github.com/kube-rs/kube/pull/760</li> <li>Implement cordon/uncordon for Node by @ChinYing-Li in https://github.com/kube-rs/kube/pull/762</li> <li>Export Version priority parser with Ord impls in kube_core by @clux in https://github.com/kube-rs/kube/pull/764</li> <li>Add Api fns for arbitrary subresources and approval subresource for CertificateSigningRequest by @ChinYing-Li in https://github.com/kube-rs/kube/pull/773</li> </ul>"},{"location":"changelog/#changed_35","title":"Changed","text":"<ul> <li>Add backoff handling for watcher and Controller by @clux in https://github.com/kube-rs/kube/pull/703</li> <li>Remove crate private <code>identity_pem</code> field from <code>Config</code> by @kazk in https://github.com/kube-rs/kube/pull/771</li> <li>Use SecretString in AuthInfo to avoid credential leaking by @ChinYing-Li in https://github.com/kube-rs/kube/pull/766</li> </ul>"},{"location":"changelog/#0650--2021-12-10","title":"0.65.0 / 2021-12-10","text":"<ul> <li>BREAKING: Removed <code>kube::Error::OpenSslError</code> - #716</li> <li>BREAKING: Removed <code>kube::Error::SslError</code> - #704 and #716</li> <li>BREAKING: Added <code>kube::Error::NativeTls(kube::client::NativeTlsError)</code> for errors from Native TLS - #716</li> <li>BREAKING: Added <code>kube::Error::RustlsTls(kube::client::RustlsTlsError)</code> for errors from Rustls TLS - #704</li> <li>Modified <code>Kubeconfig</code> parsing - allow empty kubeconfigs as per kubectl - #721</li> <li>Added <code>Kubeconfig::from_yaml</code> - #718 via #719</li> <li>Updated <code>rustls</code> to 0.20.1 - #704</li> <li>BREAKING: Added <code>ObjectRef</code> to the object that failed to be reconciled to <code>kube::runtime::controller::Error::ReconcileFailed</code> - #733</li> <li>BREAKING: Removed <code>api_version</code> and <code>kind</code> fields from <code>kind</code> structs generated by <code>kube::CustomResource</code> - #739</li> <li>Updated <code>tokio-tungstenite</code> to 0.16 - #750</li> <li>Updated <code>tower-http</code> to 0.2.0 - #748</li> <li>BREAKING: <code>kube-client</code>: replace <code>RefreshTokenLayer</code> with <code>AsyncFilterLayer</code> in <code>AuthLayer</code> - #752</li> </ul>"},{"location":"changelog/#0640--2021-11-16","title":"0.64.0 / 2021-11-16","text":"<ul> <li>BREAKING: Replaced feature <code>kube-derive/schema</code> with attribute <code>#[kube(schema)]</code> - #690</li> <li>If you currently disable default <code>kube-derive</code> default features to avoid automatic schema generation, add <code>#[kube(schema = \"disabled\")]</code> to your spec struct instead</li> <li>BREAKING: Moved <code>CustomResource</code> derive crate overrides into subattribute <code>#[kube(crates(...))]</code> - #690</li> <li>Replace <code>#[kube(kube_core = .., k8s_openapi = .., schema = .., serde = .., serde_json = ..)]</code> with <code>#[kube(crates(kube_core = .., k8s_openapi = .., schema = .., serde = .., serde_json = ..))]</code></li> <li>Added <code>openssl-tls</code> feature to use <code>openssl</code> for TLS on all platforms. Note that, even though <code>native-tls</code> uses a platform specific TLS, <code>kube</code> requires <code>openssl</code> on all platforms because <code>native-tls</code> only allows PKCS12 input to load certificates and private key at the moment, and creating PKCS12 requires <code>openssl</code>. - #700</li> <li>BREAKING: Changed to fail loading configurations with PEM-encoded certificates containing invalid sections instead of ignoring them. Updated <code>pem</code> to 1.0.1. - #702</li> <li><code>oauth</code>: Updated <code>tame-oauth</code> to 0.6.0 which supports the same default credentials flow as the Go <code>oauth2</code> for Google OAuth. In addition to reading the service account information from JSON file specified with <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable, Application Default Credentials from <code>gcloud</code>, and obtaining OAuth tokens from local metadata server when running inside GCP are now supported. - #701</li> </ul>"},{"location":"changelog/#refining-errors","title":"Refining Errors","text":"<p>We started working on improving error ergonomics. See the tracking issue #688 for more details.</p> <p>The following is the summary of changes to <code>kube::Error</code> included in this release:</p> <ul> <li>Added <code>Error::Auth(kube::client::AuthError)</code> (errors related to client auth, some of them were previously in <code>Error::Kubeconfig</code>)</li> <li>Added <code>Error::BuildRequest(kube::core::request::Error)</code> (errors building request from <code>kube::core</code>)</li> <li>Added <code>Error::InferConfig(kube::config::InferConfigError)</code> (for <code>Client::try_default</code>)</li> <li>Added <code>Error::OpensslTls(kube::client::OpensslTlsError)</code> (new <code>openssl-tls</code> feature) - #700</li> <li>Added <code>Error::UpgradeConnection(kube::client::UpgradeConnectinError)</code> (<code>ws</code> feature, errors from upgrading a connection)</li> <li>Removed <code>Error::Connection</code> (was unused)</li> <li>Removed <code>Error::RequestBuild</code> (was unused)</li> <li>Removed <code>Error::RequestSend</code> (was unused)</li> <li>Removed <code>Error::RequestParse</code> (was unused)</li> <li>Removed <code>Error::InvalidUri</code> (replaced by variants of errors in <code>kube::config</code> errors)</li> <li>Removed <code>Error::RequestValidation</code> (replaced by a variant of <code>Error::BuildRequest</code>)</li> <li>Removed <code>Error::Kubeconfig</code> (replaced by <code>Error::InferConfig</code>, and <code>Error::Auth</code>)</li> <li>Removed <code>Error::ProtocolSwitch</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>Error::MissingUpgradeWebSocketHeader</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>Error::MissingConnectionUpgradeHeader</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>Error::SecWebSocketAcceptKeyMismatch</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>Error::SecWebSocketProtocolMismatch</code> (<code>ws</code> only, replaced by <code>Error::UpgradeConnection</code>)</li> <li>Removed <code>impl From&lt;T&gt; for Error</code></li> </ul> Expand for more details  The following breaking changes were made as a part of an effort to refine errors (the list is large, but most of them are lower level, and shouldn't require much change in most cases):  * Removed `impl From for kube::Error` - [#686](https://github.com/kube-rs/kube/issues/686) * Removed unused error variants in `kube::Error`: `Connection`, `RequestBuild`, `RequestSend`, `RequestParse` - [#689](https://github.com/kube-rs/kube/issues/689) * Removed unused error variant `kube::error::ConfigError::LoadConfigFile` - [#689](https://github.com/kube-rs/kube/issues/689) * Changed `kube::Error::RequestValidation(String)` to `kube::Error::BuildRequest(kube::core::request::Error)`. Includes possible errors from building an HTTP request, and contains some errors from `kube::core` that was previously grouped under `kube::Error::SerdeError` and `kube::Error::HttpError`. `kube::core::request::Error` is described below. - [#686](https://github.com/kube-rs/kube/issues/686) * Removed `kube::core::Error` and `kube::core::Result`. `kube::core::Error` was replaced by more specific errors. - [#686](https://github.com/kube-rs/kube/issues/686)   - Replaced `kube::core::Error::InvalidGroupVersion` with `kube::core::gvk::ParseGroupVersionError`   - Changed the error returned from `kube::core::admission::AdmissionRequest::with_patch` to `kube::core::admission::SerializePatchError` (was `kube::core::Error::SerdeError`)   - Changed the error associated with `TryInto&gt;` to `kube::core::admission::ConvertAdmissionReviewError` (was `kube::core::Error::RequestValidation`)   - Changed the error returned from methods of `kube::core::Request` to `kube::core::request::Error` (was `kube::core::Error`). `kube::core::request::Error` represents possible errors when building an HTTP request. The removed `kube::core::Error` had `RequestValidation(String)`, `SerdeError(serde_json::Error)`, and `HttpError(http::Error)` variants. They are now `Validation(String)`, `SerializeBody(serde_json::Error)`, and  `BuildRequest(http::Error)` respectively in `kube::core::request::Error`. * Changed variants of error enums in `kube::runtime` to tuples. Replaced `snafu` with `thiserror`. - [#686](https://github.com/kube-rs/kube/issues/686) * Removed `kube::error::ConfigError` and `kube::Error::Kubeconfig(ConfigError)` - [#696](https://github.com/kube-rs/kube/issues/696)   - Error variants related to client auth were moved to a new error `kube::client::AuthError` as described below   - Remaining variants were split into `kube::config::{InferConfigError, InClusterError, KubeconfigError}` as described below * Added `kube::client::AuthError` by extracting error variants related to client auth from `kube::ConfigError` and adding more variants to preserve context - [#696](https://github.com/kube-rs/kube/issues/696) * Moved `kube::error::OAuthError` to `kube::client::OAuthError` - [#696](https://github.com/kube-rs/kube/issues/696) * Changed all errors in `kube::client::auth` to `kube::client::AuthError` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::Error::Auth(kube::client::AuthError)` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::InferConfigError` which is an error from `Config::infer()` and `kube::Error::InferConfig(kube::config::InferConfigError)` - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::InClusterError` for errors related to loading in-cluster configuration by splitting `kube::ConfigError` and adding more variants to preserve context. - [#696](https://github.com/kube-rs/kube/issues/696) * Added `kube::config::KubeconfigError` for errors related to loading kubeconfig by splitting `kube::ConfigError` and adding more variants to preserve context. - [#696](https://github.com/kube-rs/kube/issues/696) * Changed methods of `kube::Config` to return these erorrs instead of `kube::Error` - [#696](https://github.com/kube-rs/kube/issues/696) * Removed `kube::Error::InvalidUri` which was replaced by error variants preserving context, such as `KubeconfigError::ParseProxyUrl` - [#696](https://github.com/kube-rs/kube/issues/696) * Moved all errors from upgrading to a WebSocket connection into `kube::Error::UpgradeConnection(kube::client::UpgradeConnectionError)` - [#696](https://github.com/kube-rs/kube/issues/696)"},{"location":"changelog/#0632--2021-10-28","title":"0.63.2 / 2021-10-28","text":"<ul> <li><code>kube::runtime::events</code>: fix build and hide module on kubernetes &lt; 1.19 (events/v1 missing there) - #685</li> </ul>"},{"location":"changelog/#0631--2021-10-26","title":"0.63.1 / 2021-10-26","text":"<ul> <li><code>kube::runtime::wait::Condition</code> added boolean combinators (<code>not</code>/<code>and</code>/<code>or</code>) - #678</li> <li><code>kube</code>: fix docs.rs build - #681 via #682</li> </ul>"},{"location":"changelog/#0630--2021-10-26","title":"0.63.0 / 2021-10-26","text":"<ul> <li>rust <code>edition</code> bumped to <code>2021</code> - #664, #666, #667</li> <li><code>kube::CustomResource</code> derive can now take arbitrary <code>#[kube(k8s_openapi)]</code> style-paths for <code>k8s_openapi</code>, <code>schemars</code>, <code>serde</code>, and <code>serde_json</code> - #675</li> <li><code>kube</code>: fix <code>native-tls</code> included when only <code>rustls-tls</code> feature is selected - #673 via #674</li> </ul>"},{"location":"changelog/#0620--2021-10-22","title":"0.62.0 / 2021-10-22","text":"<ul> <li><code>kube</code> now re-exports <code>kube-runtime</code> under <code>runtime</code> feature - #651 via #652</li> <li>no need to keep both <code>kube</code> and <code>kube_runtime</code> in <code>Cargo.toml</code> anymore</li> <li>fixes issues with dependabot / lock-step upgrading</li> <li>change <code>kube_runtime::X</code> import paths to <code>kube::runtime::X</code> when moving to the feature</li> <li><code>kube::runtime</code> added <code>events</code> module with an event <code>Recorder</code> - #249 via #653 + #662 + #663</li> <li><code>kube::runtime::wait::conditions</code> added <code>is_crd_established</code> helper - #659</li> <li><code>kube::CustomResource</code> derive can now take an arbitrary <code>#[kube(kube_core)]</code> path for <code>kube::core</code> - #658</li> <li><code>kube::core</code> consistently re-exported across crates</li> <li>docs: major overhaul + architecture.md - #416 via #652</li> </ul>"},{"location":"changelog/#0610--2021-10-09","title":"0.61.0 / 2021-10-09","text":"<ul> <li><code>kube-core</code>: BREAKING: extend <code>CustomResourceExt</code> trait with <code>::shortnames</code> method (impl in <code>kube-derive</code>) - #641</li> <li><code>kube-runtime</code>: add <code>wait</code> module to <code>await_condition</code>, and added <code>watch_object</code> to watcher - #632 via #633</li> <li><code>kube</code>: add <code>Restart</code> marker trait to allow <code>Api::restart</code> on core workloads - #630 via #635</li> <li>bump dependencies: <code>tokio-tungstenite</code>, <code>k8s-openapi</code>, <code>schemars</code>, <code>tokio</code> in particular - #643 + #645</li> </ul>"},{"location":"changelog/#0600--2021-09-02","title":"0.60.0 / 2021-09-02","text":"<ul> <li><code>kube</code>: support <code>k8s-openapi</code> with <code>v1_22</code> features - #621 via #622</li> <li><code>kube</code>: <code>BREAKING</code>: support for <code>CustomResourceDefinition</code> at <code>v1beta1</code> now requires an opt-in <code>deprecated-crd-v1beta1</code> feature - #622</li> <li><code>kube-core</code>: add content-type header to requests with body - #626 via #627</li> </ul>"},{"location":"changelog/#0590--2021-08-09","title":"0.59.0 / 2021-08-09","text":"<ul> <li><code>BREAKING</code>: bumped <code>k8s-openapi</code> to 0.13.0 - #581 via #616</li> <li><code>kube</code> connects to kubernetes via cluster dns when using <code>rustls</code> - #587 via #597</li> <li>client now works with <code>rustls</code> feature in-cluster - #153 via #597</li> <li><code>kube</code> nicer serialization of <code>Kubeconfig</code> - #613</li> <li><code>kube-core</code> added serde traits for <code>ApiResource</code> - #590</li> <li><code>kube-core</code> added <code>CrdExtensions::crd_name</code> method (implemented by <code>kube-derive</code>) - #583</li> <li><code>kube-core</code> added the <code>HasSpec</code> and <code>HasStatus</code> traits - #605</li> <li><code>kube-derive</code> added support to automatically implement the <code>HasSpec</code> and <code>HasStatus</code> traits - #605</li> <li><code>kube-runtime</code> fix tracing span hierarchy from applier - #600</li> </ul>"},{"location":"changelog/#0581--2021-07-06","title":"0.58.1 / 2021-07-06","text":"<ul> <li><code>kube-runtime</code>: fix non-unix builds - #582</li> </ul>"},{"location":"changelog/#0580--2021-07-05","title":"0.58.0 / 2021-07-05","text":"<ul> <li><code>kube</code>: <code>BREAKING</code>: subresource marker traits renamed conjugation: <code>Log</code>, <code>Execute</code>, <code>Attach</code>, <code>Evict</code> (previously <code>Logging</code>, <code>Executable</code>, <code>Attachable</code>, <code>Evictable</code>) - #536 via #560</li> <li><code>kube-derive</code> added <code>#[kube(category)]</code> attr to set CRD categories - #559</li> <li><code>kube-runtime</code> added <code>finalizer</code> helper #291 via #475</li> <li><code>kube-runtime</code> added tracing for why reconciliations happened #457 via #571</li> <li><code>kube-runtime</code> added <code>Controller::reconcile_all_on</code> to allow scheduling all objects for reconciliation #551 via #555</li> <li><code>kube-runtime</code> added <code>Controller::graceful_shutdown_on</code> for shutting down the <code>Controller</code> while waiting for running reconciliations to finish - #552 via #573</li> <li>BREAKING: <code>controller::applier</code> now starts a graceful shutdown when the <code>queue</code> terminates</li> <li>BREAKING: <code>scheduler</code> now shuts down immediately when <code>requests</code> terminates, rather than waiting for the pending reconciliations to drain</li> <li><code>kube-runtime</code> added tracking for reconciliation reason</li> <li>Added: <code>Controller::owns_with</code> and <code>Controller::watches_with</code> to pass a <code>dyntype</code> argument for dynamic <code>Api</code>s - #575</li> <li>BREAKING: <code>Controller::owns</code> signature changed to not allow <code>DynamicType</code>s</li> <li>BREAKING: <code>controller::trigger_*</code> now returns a <code>ReconcileRequest</code> rather than <code>ObjectRef</code>. The <code>ObjectRef</code> can be accessed via the <code>obj_ref</code> field</li> </ul>"},{"location":"changelog/#known-issues","title":"Known Issues","text":"<ul> <li>Api::replace can fail to unset list values with k8s-openapi 0.12 #581</li> </ul>"},{"location":"changelog/#0570--2021-06-16","title":"0.57.0 / 2021-06-16","text":"<ul> <li><code>kube</code>: custom clients now respect default namespaces - fixes #534 via #544</li> <li>BREAKING: custom clients via <code>Client::new</code> must pass <code>config.default_namespace</code> as 2<sup>nd</sup> arg</li> <li><code>kube</code>: Added <code>CustomResourceExt</code> trait for <code>kube-derive</code> - #497 via #545</li> <li>BREAKING: <code>kube-derive</code> users must import <code>kube::CustomResourceExt</code> (or <code>kube::core::crd::v1beta1::CustomResourceExt</code> if using legacy <code>#[kube(apiextensions = \"v1beta1\")]</code>) to use generated methods <code>Foo::crd</code> or <code>Foo::api_resource</code></li> <li>BREAKING: <code>k8s_openapi</code> bumped to 0.12.0 - #531<ul> <li>Generated structs simplified + <code>Resource</code> trait expanded</li> <li>Adds support for kubernetes <code>v1_21</code></li> <li>Contains bugfix for kubernetes#102159</li> </ul> </li> <li><code>kube</code> resource plurals is no longer inferred from <code>k8s-openapi</code> structs - #284 via #556</li> <li>BREAKING: <code>kube::Resource</code> trait now requires a <code>plural</code> implementation</li> </ul>"},{"location":"changelog/#known-issues_1","title":"Known Issues","text":"<ul> <li>Api::replace can fail to unset list values with k8s-openapi 0.12 #581</li> </ul>"},{"location":"changelog/#0560--2021-06-05","title":"0.56.0 / 2021-06-05","text":"<ul> <li><code>kube</code>: added <code>Api::default_namespaced</code> - #209 via #534</li> <li><code>kube</code>: added <code>config</code> feature - #533 via #535</li> <li><code>kube</code>: BREAKING: moved <code>client::discovery</code> module to <code>kube::discovery</code> and rewritten module #538</li> <li><code>discovery</code>: added <code>oneshot</code> helpers for quick selection of recommended resources / kinds #538</li> <li><code>discovery</code>: moved <code>ApiResource</code> and <code>ApiCapabilities</code> (result of discovery) to <code>kube_core::discovery</code></li> <li> <p>BREAKING: removed internal <code>ApiResource::from_apiresource</code></p> </li> <li> <p><code>kube::Client</code> is now configurable with layers using <code>tower-http</code> #539 via #540</p> </li> <li>three new examples added: <code>custom_client</code>, <code>custom_client_tls</code> and <code>custom_client_trace</code></li> <li>Big feature streamlining, big service and layer restructuring, dependency restructurings</li> <li>Changes can hit advanced users, but unlikely to hit base use cases with <code>Api</code> and <code>Client</code>.</li> <li>In depth changes broken down below:</li> </ul>"},{"location":"changelog/#tls-enhancements","title":"TLS Enhancements","text":"<ul> <li>Add <code>kube::client::ConfigExt</code> extending <code>Config</code> for custom <code>Client</code>. This includes methods to configure TLS connection when building a custom client #539</li> <li><code>native-tls</code>: <code>Config::native_tls_https_connector</code> and <code>Config::native_tls_connector</code></li> <li><code>rustls-tls</code>: <code>Config::rustls_https_connector</code> and <code>Config::rustls_client_config</code></li> <li>Remove the requirement of having <code>native-tls</code> or <code>rustls-tls</code> enabled when <code>client</code> is enabled. Allow one, both or none.</li> <li>When both, the default Service will use <code>native-tls</code> because of #153. <code>rustls</code> can be still used with a custom client. Users will have an option to configure TLS at runtime.</li> <li>When none, HTTP connector is used.</li> <li>Remove TLS features from <code>kube-runtime</code></li> <li>BREAKING: Features must be removed if specified</li> <li>Remove <code>client</code> feature from <code>native-tls</code> and <code>rust-tls</code> features</li> <li><code>config</code> + <code>native-tls</code>/<code>rustls-tls</code> can be used independently, e.g., to create a simple HTTP client</li> <li>BREAKING: <code>client</code> feature must be added if <code>default-features = false</code></li> </ul>"},{"location":"changelog/#layers","title":"Layers","text":"<ul> <li><code>ConfigExt::base_uri_layer</code> (<code>BaseUriLayer</code>) to set cluster URL (#539)</li> <li><code>ConfigExt::auth_layer</code> that returns optional layer to manage <code>Authorization</code> header (#539)</li> <li><code>gzip</code>: Replaced custom decompression module with <code>DecompressionLayer</code> from <code>tower-http</code> (#539)</li> <li>Replaced custom <code>LogRequest</code> with <code>TraceLayer</code> from <code>tower-http</code> (#539)</li> <li>Request body is no longer shown</li> <li>Basic and Bearer authentication using <code>AddAuthorizationLayer</code> (borrowing from https://github.com/tower-rs/tower-http/pull/95 until released)</li> <li>BREAKING: Remove <code>headers</code> from <code>Config</code>. Injecting arbitrary headers is now done with a layer on a custom client.</li> </ul>"},{"location":"changelog/#dependency-changes","title":"Dependency Changes","text":"<ul> <li>Remove <code>static_assertions</code> since it's no longer used</li> <li>Replace <code>tokio_rustls</code> with <code>rustls</code> and <code>webpki</code> since we're not using <code>tokio_rustls</code> directly</li> <li>Replace uses of <code>rustls::internal::pemfile</code> with <code>rustls-pemfile</code></li> <li>Remove <code>url</code> and always use <code>http::Uri</code></li> <li>BREAKING: <code>Config::cluster_url</code> is now <code>http::Uri</code></li> <li>BREAKING: <code>Error::InternalUrlError(url::ParseError)</code> and <code>Error::MalformedUrl(url::ParseError)</code> replaced by <code>Error::InvalidUri(http::uri::InvalidUri)</code></li> </ul>"},{"location":"changelog/#0550--2021-05-21","title":"0.55.0 / 2021-05-21","text":"<ul> <li><code>kube</code>: <code>client</code> feature added (default-enabled) - #528</li> <li><code>kube</code>: <code>PatchParams</code> force now only works with <code>Patch::Apply</code> #528</li> <li><code>kube</code>: <code>api</code> <code>discovery</code> module now uses a new <code>ApiResource</code> struct #495 + #482</li> <li><code>kube</code>: <code>api</code> BREAKING: <code>DynamicObject</code> + <code>Object</code> now takes an <code>ApiResource</code> rather than a <code>GroupVersionKind</code></li> <li><code>kube</code>: <code>api</code> BREAKING: <code>discovery</code> module's <code>Group</code> renamed to <code>ApiGroup</code></li> <li><code>kube</code>: <code>client</code> BREAKING: <code>kube::client::Status</code> moved to <code>kube::core::Status</code> (accidental, re-adding in 0.56)</li> <li><code>kube-core</code> crate factored out of <code>kube</code> to reduce dependencies - #516 via #517 + #519 + #522 + #528 + #530</li> <li><code>kube</code>: <code>kube::Service</code> removed to allow <code>kube::Client</code> to take an abritrary <code>Service&lt;http::Request&lt;hyper::Body&gt;&gt;</code> - #532</li> </ul>"},{"location":"changelog/#0540--2021-05-19","title":"0.54.0 / 2021-05-19","text":"<ul> <li>yanked 30 minutes after release due to #525</li> <li>changes lifted to 0.55.0</li> </ul>"},{"location":"changelog/#0530--2021-05-15","title":"0.53.0 / 2021-05-15","text":"<ul> <li><code>kube</code>: <code>admission</code> controller module added under feature - #477 via #484 + fixes in #488 #498 #499 + #507 + #509</li> <li><code>kube</code>: <code>config</code> parsing of pem blobs now resilient against missing newlines - #504 via #505</li> <li><code>kube</code>: <code>discovery</code> module added to simplify dynamic api usage - #491</li> <li><code>kube</code>: <code>api</code> BREAKING: <code>DynamicObject::namespace</code> renamed to <code>::within</code> - #502</li> <li><code>kube</code>: <code>api</code> BREAKING: added <code>ResourceExt</code> trait moving the getters from <code>Resource</code> trait - #486</li> <li><code>kube</code>: <code>api</code> added a generic interface for subresources via <code>Request</code> - #487</li> <li><code>kube</code>: <code>api</code> fix bug in <code>PatchParams::dry_run</code> not being serialized correctly - #511</li> </ul>"},{"location":"changelog/#0530-migration-guide","title":"0.53.0 Migration Guide","text":"<p>The most likely issue you'll run into is from <code>kube</code> when using <code>Resource</code> trait which has been split:</p> <pre><code>+use kube::api::ResouceExt;\n-    let name = Resource::name(&amp;foo);\n-    let ns = Resource::namespace(&amp;foo).expect(\"foo is namespaced\");\n+    let name = ResourceExt::name(&amp;foo);\n+    let ns = ResourceExt::namespace(&amp;foo).expect(\"foo is namespaced\");\n</code></pre>"},{"location":"changelog/#0520--2021-03-31","title":"0.52.0 / 2021-03-31","text":"<ul> <li><code>kube-derive</code>: allow overriding <code>#[kube(plural)]</code> and <code>#[kube(singular)]</code> - #458 via #463</li> <li><code>kube</code>: added tracing instrumentation for io operations in <code>kube::Api</code> - #455</li> <li><code>kube</code>: <code>DeleteParams</code>'s <code>Preconditions</code> is now public - #459 via #460</li> <li><code>kube</code>: remove dependency on duplicate <code>derive_accept_key</code> for <code>ws</code> - #452</li> <li><code>kube</code>: Properly verify websocket keys in <code>ws</code> handshake - #447</li> <li><code>kube</code>: BREAKING: removed optional, and deprecated <code>runtime</code> module - #454</li> <li><code>kube</code>: BREAKING: <code>ListParams</code> bookmarks default enabled - #226 via #445</li> <li>renames member <code>::allow_bookmarks</code> to <code>::bookmarks</code></li> <li><code>::default()</code> sets <code>bookmark</code> to <code>true</code> to avoid bad bad defaults #219</li> <li>method <code>::allow_bookmarks()</code> replaced by <code>::disable_bookmarks()</code></li> <li><code>kube</code>: <code>DynamicObject</code> and <code>GroupVersionKind</code> introduced for full dynamic object support</li> <li><code>kube-runtime</code>: watchers/reflectors/controllers can be used with dynamic objects from api discovery</li> <li><code>kube</code>: Pluralisation now only happens for <code>k8s_openapi</code> objects by default #481</li> <li>inflector dependency removed #471</li> <li>added internal pluralisation helper for <code>k8s_openapi</code> objects</li> <li><code>kube</code>: BREAKING: Restructuring of low level <code>Resource</code> request builder #474</li> <li><code>Resource</code> renamed to <code>Request</code> and requires only a <code>path_url</code> to construct</li> <li><code>kube</code>: BREAKING: Mostly internal <code>Meta</code> trait revamped to support dynamic types</li> <li><code>Meta</code> renamed to <code>kube::Resource</code> to mimic <code>k8s_openapi::Resource</code> #478</li> <li>The trait now takes an optional associated type for runtime type info: <code>DynamicType</code> #385</li> <li><code>Api::all_with</code> + <code>Api::namespaced_with</code> added for querying with dynamic families</li> <li>see <code>dynamic_watcher</code> + <code>dynamic_api</code> for example usage</li> <li><code>kube-runtime</code>: BREAKING: lower level interface changes as a result of <code>kube::api::Meta</code> trait:</li> <li>THESE SHOULD NOT AFFECT YOU UNLESS YOU ARE IMPLEMENTING / CUSTOMISING LOW LEVEL TYPES DIRECTLY</li> <li><code>ObjectRef</code> now generic over <code>kube::Resource</code> rather than <code>RuntimeResource</code></li> <li><code>reflector::{Writer, Store}</code> takes a <code>kube::Resource</code> rather than a <code>k8s_openapi::Resource</code></li> <li><code>kube-derive</code>: BREAKING: Generated type no longer generates <code>k8s-openapi</code> traits</li> <li>This allows correct pluralisation via <code>#[kube(plural = \"mycustomplurals\")]</code> #467 via #481</li> </ul>"},{"location":"changelog/#0520-migration-guide","title":"0.52.0 Migration Guide","text":"<p>While we had a few breaking changes. Most are to low level internal interfaces and should not change much, but some changes you might need to make:</p>"},{"location":"changelog/#kube","title":"kube","text":"<ul> <li>if using the old, low-level <code>kube::api::Resource</code>, please consider the easier <code>kube::Api</code>, or look at tests in <code>request.rs</code> or <code>typed.rs</code> if you need the low level interface</li> <li>search replace <code>kube::api::Meta</code> with <code>kube::Resource</code> if used - trait was renamed</li> <li>if implementing the trait, add <code>type DynamicType = ();</code> to the impl</li> <li>remove calls to <code>ListParams::allow_bookmarks</code> (allow default)</li> <li>handle <code>WatchEvent::Bookmark</code> or set <code>ListParams::disable_bookmarks()</code></li> <li>look at examples if replacing the long deprecated legacy runtime</li> </ul>"},{"location":"changelog/#kube-derive","title":"kube-derive","text":"<p>The following constants from <code>k8s_openapi::Resource</code> no longer exist. Please <code>use kube::Resource</code> and: - replace <code>Foo::KIND</code> with <code>Foo::kind(&amp;())</code> - replace <code>Foo::GROUP</code> with <code>Foo::group(&amp;())</code> - replace <code>Foo::VERSION</code> with <code>Foo::version(&amp;())</code> - replace <code>Foo::API_VERSION</code> with <code>Foo::api_version(&amp;())</code></p>"},{"location":"changelog/#0510--2021-02-28","title":"0.51.0 / 2021-02-28","text":"<ul> <li><code>kube</code> <code>Config</code> now allows arbirary extension objects - #425</li> <li><code>kube</code> <code>Config</code> now allows multiple yaml documents per kubeconfig - #440 via #441</li> <li><code>kube-derive</code> now more robust and is using <code>darling</code> - #435</li> <li>docs improvements to patch + runtime</li> </ul>"},{"location":"changelog/#0501--2021-02-17","title":"0.50.1 / 2021-02-17","text":"<ul> <li>bug: fix oidc auth provider - #424 via #419</li> </ul>"},{"location":"changelog/#0500--2021-02-10","title":"0.50.0 / 2021-02-10","text":"<ul> <li>feat: added support for stacked kubeconfigs - #132 via #411</li> <li>refactor: authentication logic moved out of <code>kube::config</code> and into into <code>kube::service</code> - #409</li> <li>BREAKING: <code>Config::get_auth_header</code> removed</li> <li>refactor: remove <code>hyper</code> dependency from <code>kube::api</code> - #410</li> <li>refactor: <code>kube::Service</code> simpler auth and gzip handling - #405 + #408</li> </ul>"},{"location":"changelog/#0490--2021-02-08","title":"0.49.0 / 2021-02-08","text":"<ul> <li>dependency on <code>reqwest</code> + removed in favour of <code>hyper</code> + <code>tower</code> #394</li> <li>refactor: <code>kube::Client</code> now uses <code>kube::Service</code> (a <code>tower::Service&lt;http::Request&lt;hyper::Body&gt;&gt;</code>) instead of <code>reqwest::Client</code> to handle all requests</li> <li>refactor: <code>kube::Client</code> now uses a <code>tokio_util::codec</code> for internal buffering</li> <li>refactor: <code>async-tungstenite</code> ws feature dependency replaced with <code>tokio-tungstenite</code>. <code>WebSocketStream</code> is now created from a connection upgraded with <code>hyper</code></li> <li>refactor: <code>oauth2</code> module for GCP OAuth replaced with optional <code>tame-oauth</code> dependency</li> <li>BREAKING: GCP OAuth is now opt-in (<code>oauth</code> feature). Note that GCP provider with command based token source is supported by default.</li> <li>BREAKING: Gzip decompression is now opt-in (<code>gzip</code> feature) because Kubernetes does not have compression enabled by default yet and this feature requires extra dependencies. #399</li> <li>BREAKING: <code>Client::new</code> now takes a <code>Service</code> instead of <code>Config</code> #400. Allows custom service for features not supported out of the box and testing. To create a <code>Client</code> from <code>Config</code>, use <code>Client::try_from</code> instead.</li> <li>BREAKING: Removed <code>Config::proxy</code>. Proxy is no longer supported out of the box, but it should be possible by using a custom Service.</li> <li>fix: Refreshable token from auth provider not refreshing</li> <li>fix: Panic when loading config with non-GCP provider #238</li> <li>feat: subresource support added for <code>Evictable</code> types (marked for <code>Pod</code>) - #393</li> <li><code>kube</code>: subresource marker traits renamed to <code>Loggable</code>, <code>Executable</code>, <code>Attachable</code> (previously <code>LoggingObject</code>, <code>ExecutingObject</code>, <code>AttachableObject</code>) - #395</li> <li><code>examples</code> showcasing <code>kubectl cp</code> like behaviour #381 via #392</li> </ul>"},{"location":"changelog/#0480--2021-01-23","title":"0.48.0 / 2021-01-23","text":"<ul> <li>bump <code>k8s-openapi</code> to <code>0.11.0</code> - #388</li> <li>breaking: <code>kube</code>: no longer necessary to serialize patches yourself - #386<ul> <li><code>PatchParams</code> removes <code>PatchStrategy</code></li> <li><code>Api::patch*</code> methods now take an enum <code>Patch</code> type</li> <li>optional <code>jsonpatch</code> feature added for <code>Patch::Json</code></li> </ul> </li> </ul>"},{"location":"changelog/#0470--2021-01-06","title":"0.47.0 / 2021-01-06","text":"<ul> <li>chore: upgrade <code>tokio</code> to <code>1.0</code> - #363<ul> <li>BREAKING: This requires the whole application to upgrade to <code>tokio</code> 1.0 and <code>reqwest</code> to 0.11.0</li> </ul> </li> <li>docs: fix broken documentation in <code>kube</code> 0.46.0 #367</li> <li>bug: <code>kube</code>: removed panics from <code>ws</code> features, fix <code>rustls</code> support + improve docs #369 via #370 + #373</li> <li>bug: <code>AttachParams</code> now fixes owned method chaining (slightly breaks from 0.46 if using &amp;mut ref before) - #364</li> <li>feat: <code>AttachParams::interactive_tty</code> convenience method added - #364</li> <li>bug: fix <code>Runner</code> (and thus <code>Controller</code> and <code>applier</code>) not waking correctly when starting new tasks - #375</li> </ul>"},{"location":"changelog/#0461--2021-01-06","title":"0.46.1 / 2021-01-06","text":"<ul> <li>maintenance release for 0.46 (last supported tokio 0.2 release) from <code>tokio02</code> branch</li> <li>bug backport: fix <code>Runner</code> (and thus <code>Controller</code> and <code>applier</code>) not waking correctly when starting new tasks - #375</li> </ul>"},{"location":"changelog/#0460--2021-01-02","title":"0.46.0 / 2021-01-02","text":"<ul> <li>feat: <code>kube</code> now has optional websocket support with <code>async_tungstenite</code> under <code>ws</code> and <code>ws-*-tls</code> features #360</li> <li>feat: <code>AttachableObject</code> marker trait added and implemented for <code>k8s_openapi::api::core::v1::Pod</code> #360</li> <li>feat: <code>AttachParams</code> added for <code>Api::exec</code> and <code>Api::attach</code> for <code>AttachableObject</code>s #360</li> <li>examples: <code>pod_shell</code>, <code>pod_attach</code>, <code>pod_exec</code> demonstrating the new features #360</li> </ul>"},{"location":"changelog/#0450--2020-12-26","title":"0.45.0 / 2020-12-26","text":"<ul> <li>feat: <code>kube-derive</code> now has a default enabled <code>schema</code> feature<ul> <li>allows opting out of <code>schemars</code> dependency for handwriting crds - #355</li> </ul> </li> <li>breaking: <code>kube-derive</code> attr <code>struct_name</code> renamed to <code>struct</code> - #359</li> <li>docs: improvements on <code>kube</code>, <code>kube-runtime</code>, <code>kube-derive</code></li> </ul>"},{"location":"changelog/#0440--2020-12-23","title":"0.44.0 / 2020-12-23","text":"<ul> <li>feat: <code>kube-derive</code> now generates openapi v3 schemas and is thus usable with v1 <code>CustomResourceDefinition</code> - #129 and #264 via #348<ul> <li>BREAKING: <code>kube-derive</code> types now require <code>JsonSchema</code> derived via <code>schemars</code> libray (not breaking if going to 0.45.0)</li> </ul> </li> <li>feat: <code>kube_runtime::controller</code>: now reconciles objects in parallel - #346<ul> <li>BREAKING: <code>kube_runtime::controller::applier</code> now requires that the <code>reconciler</code>'s <code>Future</code> is <code>Unpin</code>, <code>Box::pin</code> it or submit it to a runtime if this is not acceptable</li> <li>BREAKING: <code>kube_runtime::controller::Controller</code> now requires that the <code>reconciler</code>'s <code>Future</code> is <code>Send + 'static</code>,             use the low-level <code>applier</code> interface instead if this is not acceptable</li> </ul> </li> <li>bug: <code>kube-runtime</code>: removed accidentally included <code>k8s-openapi</code> default features (you have to opt in to them yourself)</li> <li>feat: <code>kube</code>: <code>TypeMeta</code> now derives additionally <code>Debug, Eq, PartialEq, Hash</code></li> <li>bump: <code>k8s-openapi</code> to <code>0.10.0</code> - #330</li> <li>bump: <code>serde_yaml</code> - #349</li> <li>bump: <code>dirs</code> to <code>dirs-next</code> - #340</li> </ul>"},{"location":"changelog/#0430--2020-10-08","title":"0.43.0 / 2020-10-08","text":"<ul> <li>bug: <code>kube-derive</code> attr <code>#[kube(shortname)]</code> now working correctly</li> <li>bug: <code>kube-derive</code> now working with badly cased existing types - #313</li> <li>missing: <code>kube</code> now correctly exports <code>config::NamedAuthInfo</code> - #323</li> <li>feat: <code>kube</code>: expose <code>Config::get_auth_header</code> for istio use cases - #322</li> <li>feat: <code>kube</code>: local config now tackles gcloud auth exec params - #328 and #84</li> <li><code>kube-derive</code> now actually requires GVK (in particular <code>#[kube(kind = \"Foo\")]</code> which we sometimes inferred earlier, despite documenting the contrary)</li> </ul>"},{"location":"changelog/#0420--2020-09-10","title":"0.42.0 / 2020-09-10","text":"<ul> <li>bug: <code>kube-derive</code>'s <code>Default</code> derive now sets typemeta correctly - #315</li> <li>feat: <code>ListParams</code> now supports <code>continue_token</code> and <code>limit</code> - #320</li> </ul>"},{"location":"changelog/#0410--2020-09-10","title":"0.41.0 / 2020-09-10","text":"<ul> <li>yanked release. failed publish.</li> </ul>"},{"location":"changelog/#0400--2020-08-17","title":"0.40.0 / 2020-08-17","text":"<ul> <li><code>DynamicResource::from_api_resource</code> added to allow apiserver returned resources - #305 via #301</li> <li><code>Client::list_api_groups</code> added</li> <li><code>Client::list_ap_group_resources</code> added</li> <li><code>Client::list_core_api_versions</code> added</li> <li><code>Client::list_core_api_resources</code> added</li> <li><code>kube::DynamicResource</code> exposed at top level</li> <li>Bug: <code>PatchParams::default_apply()</code> now requires a manager and renamed to <code>PatchParams::apply(manager: &amp;str)</code> for #300</li> <li>Bug: <code>DeleteParams</code> no longer missing for <code>Api::delete_collection</code> - #53</li> <li>Removed paramter <code>ListParams::include_uninitialized</code> deprecated since 1.14</li> <li>Added optional <code>PostParams::field_manager</code> was missing for <code>Api::create</code> case</li> </ul>"},{"location":"changelog/#0390--2020-08-05","title":"0.39.0 / 2020-08-05","text":"<ul> <li>Bug: <code>ObjectRef</code> tweak in <code>kube-runtime</code> to allow controllers triggering across cluster and namespace scopes - #293 via #294</li> <li>Feature: <code>kube</code> now has a <code>derive</code> feature which will re-export <code>kube::CustomResource</code> from <code>kube-derive::CustomResource</code>.</li> <li>Examples: revamp examples for <code>kube-runtime</code> - #201</li> </ul>"},{"location":"changelog/#0380--2020-07-23","title":"0.38.0 / 2020-07-23","text":"<ul> <li>Marked <code>kube::runtime</code> module as deprecated - #281</li> <li><code>Config::timeout</code> can now be overridden to <code>None</code> (with caveats) #280</li> <li>Bug: reflector stores could have multiple copies inside datastore - #286<ul> <li><code>dashmap</code> backend Store driver downgraded - #286</li> <li><code>Store::iter</code> temporarily removed</li> </ul> </li> <li>Bug: Specialize WatchEvent::Bookmark so they can be deserialized - #285</li> <li>Docs: Tons of docs for kube-runtime</li> </ul>"},{"location":"changelog/#0370--2020-07-20","title":"0.37.0 / 2020-07-20","text":"<ul> <li>Bump <code>k8s-openapi</code> to <code>0.9.0</code></li> <li>All runtime components now require <code>Sync</code> objects</li> <li>reflector/watcher/Controller streams can be shared in threaded environments</li> </ul>"},{"location":"changelog/#0360--2020-07-19","title":"0.36.0 / 2020-07-19","text":"<ul> <li>https://gitlab.com/teozkr/kube-rt/ merged in for a new <code>kube-runtime</code> crate #258</li> <li><code>Controller&lt;K&gt;</code> added (#148 via #258)</li> <li><code>Reflector</code> api redesigned (#102 via #258)</li> <li>Migration release for <code>Informer</code> -&gt; <code>watcher</code> + <code>Reflector</code> -&gt; <code>reflector</code></li> <li><code>kube::api::CustomResource</code> removed in favour of <code>kube::api::Resource::dynamic</code></li> <li><code>CrBuilder</code> removed in favour of <code>DynamicResource</code> (with new error handling)</li> <li>support level bumped to beta</li> </ul>"},{"location":"changelog/#0351--2020-06-18","title":"0.35.1 / 2020-06-18","text":"<ul> <li>Fix in-cluster Client when using having multiple certs in the chain - #251</li> </ul>"},{"location":"changelog/#0350--2020-06-15","title":"0.35.0 / 2020-06-15","text":"<ul> <li><code>Config::proxy</code> support added - #246</li> <li><code>PartialEq</code> can be derived with <code>kube-derive</code> - #242</li> <li>Windows builds no longer clashes with runtime - #240</li> <li>Rancher hosts (with path specifiers) now works - #244</li> </ul>"},{"location":"changelog/#0340--2020-05-08","title":"0.34.0 / 2020-05-08","text":"<ul> <li>Bump <code>k8s-openapi</code> to <code>0.8.0</code></li> <li><code>Config::from_cluster_env</code> &lt;- renamed from <code>Config::new_from_cluster_env</code></li> <li><code>Config::from_kubeconfig</code> &lt;- renamed from <code>Config::new_from_kubeconfig</code></li> <li><code>Config::from_custom_kubeconfig</code> added - #236</li> <li>Majorly overhauled error handlind in config module - #237</li> </ul>"},{"location":"changelog/#0330--2020-04-27","title":"0.33.0 / 2020-04-27","text":"<ul> <li>documentation fixes for <code>Api::patch</code></li> <li>Config: add automatic token refresh - #72 / #224 / #234</li> </ul>"},{"location":"changelog/#0321--2020-04-15","title":"0.32.1 / 2020-04-15","text":"<ul> <li>add missing tokio <code>signal</code> feature as a dependency</li> <li>upgrade all dependencies, including minor bumps to rustls and base64</li> </ul>"},{"location":"changelog/#0320--2020-04-10","title":"0.32.0 / 2020-04-10","text":"<ul> <li>Major <code>config</code> + <code>client</code> module refactor</li> <li><code>Config</code> is the new <code>Configuration</code> struct</li> <li><code>Client</code> is now just a configured <code>reqwest::Client</code> plus a <code>reqwest::Url</code></li> <li>implement <code>From&lt;Config&gt; for reqwest::ClientBuilder</code></li> <li>implement <code>TryFrom&lt;Config&gt; for Client</code></li> <li><code>Client::try_default</code> or <code>Client::new</code> now recommended constructors</li> <li>People parsing <code>~/.kube/config</code> must use the <code>KubeConfig</code> struct instead</li> <li><code>Reflector&lt;K&gt;</code> now only takes an <code>Api&lt;K&gt;</code> to construct (.params method)</li> <li><code>Informer&lt;K&gt;</code> now only takes an <code>Api&lt;K&gt;</code> to construct (.params method)</li> <li><code>Informer::init_from</code> -&gt; <code>Informer::set_version</code></li> <li><code>Reflector</code> now self-polls #151 + handles signals #152</li> <li><code>Reflector::poll</code> made private in favour of <code>Reflector::run</code></li> <li><code>Api::watch</code> no longer filters out error events (<code>next</code> -&gt; <code>try_next</code>)</li> <li><code>Api::watch</code> returns <code>Result&lt;WatchEvent&gt;</code> rather than <code>WatchEvent</code></li> <li><code>WatchEvent::Bookmark</code> added to enum</li> <li><code>ListParams::allow_bookmarks</code> added</li> <li><code>PatchParams::default_apply</code> ctor added</li> <li><code>PatchParams</code> builder mutators: <code>::force</code> and <code>::dry_run</code> added</li> </ul>"},{"location":"changelog/#0310--2020-03-27","title":"0.31.0 / 2020-03-27","text":"<ul> <li>Expose <code>config::Configuration</code> at root level</li> <li>Add <code>Configuration::infer</code> as a recommended constructor</li> <li>Rename <code>client::APIClient</code> to <code>client::Client</code></li> <li>Expose <code>client::Client</code> at root level</li> <li><code>Client</code> now implements <code>From&lt;Configuration&gt;</code></li> <li>Added comprehensive documentation on <code>Api</code></li> <li>Rename <code>config::KubeConfigLoader</code> -&gt; <code>config::ConfigLoader</code></li> <li>removed <code>futures-timer</code> dependency for <code>tokio</code> (feature=timer)</li> </ul>"},{"location":"changelog/#0300--2020-03-17","title":"0.30.0 / 2020-03-17","text":"<ul> <li>Fix <code>#[kube(printcolumn)]</code> when <code>#[kube(apiextensions = \"v1beta1\")]</code></li> <li>Fix <code>#[kube(status)]</code> causing serializes of empty optional statuses</li> </ul>"},{"location":"changelog/#0290--2020-03-12","title":"0.29.0 / 2020-03-12","text":"<ul> <li><code>Api::log</code> -&gt; <code>Api::logs</code> (now matches <code>Resource::logs</code>)</li> <li><code>Object&lt;FooSpec, FooStatus&gt;</code> back for ad-hoc ser/de</li> <li>kube-derive now derives <code>Debug</code> (requires <code>Debug</code> on spec struct)</li> <li>kube-derive now allows multiple derives per file</li> <li><code>Api::create</code> now takes data <code>K</code> rather than bytes</li> <li><code>Api::replace</code> now takes data <code>K</code> rather than bytes<ul> <li>(note that <code>Resource::create</code> and <code>Resource::replace</code> still takes bytes)</li> </ul> </li> </ul>"},{"location":"changelog/#0281--2020-03-07","title":"0.28.1 / 2020-03-07","text":"<ul> <li><code>#[derive(CustomResource)]</code> now implements <code>::new</code> on the generated <code>Kind</code></li> <li>derived <code>Kind</code> now properly contains <code>TypeMeta</code> - #170</li> </ul>"},{"location":"changelog/#0280--2020-03-05","title":"0.28.0 / 2020-03-05","text":"<ul> <li><code>RawApi</code> removed -&gt; <code>Resource</code> added</li> <li><code>Resource</code> implements <code>k8s_openapi::Resource</code></li> <li>ALL OBJECTS REMOVED -&gt; Depening on light version of <code>k8s-openapi</code> now<ul> <li>NB: should generally just mean a few import changes (+casings / unwraps)</li> </ul> </li> <li><code>openapi</code> feature removed (light dependency mandatory now)</li> <li>LIBRARY WORKS WITH ALL <code>k8s_openapi</code> KUBERNETES OBJECTS</li> <li><code>KubeObject</code> trait removed in favour of <code>Meta</code> trait</li> <li><code>Object&lt;FooSpec, FooStatus&gt;</code> removed -&gt; types implementing <code>k8s_openapi::Resource</code> required instead</li> <li><code>kube-derive</code> crate added to derive this trait + other kubebuilder like codegen</li> </ul>"},{"location":"changelog/#0270--2020-02-26","title":"0.27.0 / 2020-02-26","text":"<ul> <li><code>Reflector</code> + <code>Informer</code> moved from <code>kube::api</code> to <code>kube::runtime</code></li> <li><code>Informer</code> now resets the version to 0 rather than dropping events - #134</li> <li>Removed <code>Informer::init</code>, since it is now a no-op when building the <code>Informer</code></li> <li>Downgrade spurious log message when using service account auth</li> </ul>"},{"location":"changelog/#0260--2020-02-25","title":"0.26.0 / 2020-02-25","text":"<ul> <li>Fix a large percentage of EOFs from watches #146</li> <li>=&gt; default timeout down to 290s from 300s</li> <li>=&gt; <code>Reflector</code> now re-lists a lot less #146</li> <li>Fix decoder panic with async-compression (probably) #144</li> <li><code>Informer::poll</code> can now be used with <code>TryStream</code></li> <li>Exposed <code>Config::read</code> and <code>Config::read_from</code> - #124</li> <li>Fix typo on <code>Api::StatefulSet</code></li> <li>Fix typo on <code>Api::Endpoints</code></li> <li>Add <code>Api::v1CustomResourceDefinition</code> when on k8s &gt;= 1.17</li> <li>Renamed <code>Void</code> to <code>NotUsed</code></li> </ul>"},{"location":"changelog/#0250--2020-02-09","title":"0.25.0 / 2020-02-09","text":"<ul> <li>initial rustls support #114 (some local kube config issues know #120)</li> <li>crate does better version checking against openapi features - #106</li> <li>initial <code>log_stream</code> support - #109</li> </ul>"},{"location":"changelog/#0240--2020-01-26","title":"0.24.0 / 2020-01-26","text":"<ul> <li>Add support for ServiceAccount, Role, ClusterRole, RoleBinding, Endpoint - #113 + #111</li> <li>Upgrade k8s-openapi to 0.7 =&gt; breaking changes: https://github.com/Arnavion/k8s-openapi/blob/master/CHANGELOG.md#v070-2020-01-23</li> </ul>"},{"location":"changelog/#0230--2019-12-31","title":"0.23.0 / 2019-12-31","text":"<ul> <li>Bump tokio and reqwest to 0.2 and 0.10</li> <li>Fix bug in <code>log</code> fetcher - #107</li> <li>Temporarily allow invalid certs when testing on macosx - #105</li> </ul>"},{"location":"changelog/#0222--2019-12-04","title":"0.22.2 / 2019-12-04","text":"<ul> <li>Allow sharing Reflectors between threads - #97</li> <li>Fix Reflector pararall lock issue (<code>poll</code> no longer blocks <code>state</code>)</li> </ul>"},{"location":"changelog/#0221--2019-11-30","title":"0.22.1 / 2019-11-30","text":"<ul> <li>Improve Reflector reset algorithm (clear history less)</li> </ul>"},{"location":"changelog/#0220--2019-11-29","title":"0.22.0 / 2019-11-29","text":"<ul> <li>Default watch timeouts changed to 300s everywhere</li> <li>This increases efficiency of Informers and Reflectors by keeping the connection open longer.</li> <li>However, if your Reflector relies on frequent polling you can set <code>timeout</code> or hide the <code>poll()</code> in a different context so it doesn't block your main work</li> <li>Internal <code>RwLock</code> changed to a <code>futures::Mutex</code> for soundness / proper non-blocking - #94</li> <li>blocking <code>Reflector::read()</code> renamed to <code>async Reflector::state()</code></li> <li>Expose <code>metadata.creation_timestamp</code> and <code>.deletion_timestamp</code> (behind openapi flag) - #93</li> </ul>"},{"location":"changelog/#0210--2019-11-29","title":"0.21.0 / 2019-11-29","text":"<ul> <li>All watch calls returns a stream of <code>WatchEvent</code> - #92</li> <li><code>Informer::poll</code> now returns a stream - #92</li> </ul>"},{"location":"changelog/#0201--2019-11-21","title":"0.20.1 / 2019-11-21","text":"<ul> <li>ObjectList now implements Iterator - #91</li> <li>openapi feature no longer accidentally hardcoded to v1.15 feature - #90</li> </ul>"},{"location":"changelog/#0190--2019-11-15","title":"0.19.0 / 2019-11-15","text":"<ul> <li>kube::Error is now a proper error enum and not a Fail impl (thiserror)</li> <li>soft-tokio dependency removed for futures-timer</li> <li>gzip re-introduced</li> </ul>"},{"location":"changelog/#0181--2019-11-11","title":"0.18.1 / 2019-11-11","text":"<ul> <li>Fix unpinned gzip dependency breakage - #87</li> </ul>"},{"location":"changelog/#0180--2019-11-07","title":"0.18.0 / 2019-11-07","text":"<ul> <li>api converted to use async/await with 1.39.0 (primitively)</li> <li>hyper upgraded to 0.10-alpha</li> <li>synchronous sleep replaced with tokio timer</li> <li><code>Log</code> trait removed in favour of internal marker trait</li> </ul>"},{"location":"changelog/#0170--2019-10-22","title":"0.17.0 / 2019-10-22","text":"<ul> <li>Add support for oidc providerss with <code>auth-provider</code> w/o <code>access-token</code> - #70</li> <li>Bump most dependencies to more recent versions</li> <li>Expose custom client creation</li> <li>Added support for <code>v1beta1Ingress</code></li> <li>Expose incluster_config::load_default_ns - #74</li> </ul>"},{"location":"changelog/#0161--2019-08-09","title":"0.16.1 / 2019-08-09","text":"<ul> <li>Add missing <code>uid</code> field on <code>ObjectMeta::ownerReferences</code></li> </ul>"},{"location":"changelog/#0160--2019-08-09","title":"0.16.0 / 2019-08-09","text":"<ul> <li>Add <code>Reflector::get</code> and <code>Reflector::get_within</code> as cheaper getters</li> <li>Add support for OpenShift kube configs with multiple CAs - via #64</li> <li>Add missing <code>ObjectMeta::ownerReferences</code></li> <li>Reduced memory consumption during compile with <code>k8s-openapi@0.5.1</code> - #62</li> </ul>"},{"location":"changelog/#0151--2019-08-18","title":"0.15.1 / 2019-08-18","text":"<ul> <li>Fix compile issue on <code>1.37.0</code> with <code>Utc</code> serialization</li> <li>Fix <code>Void</code> not having <code>Serialize</code> derive</li> </ul>"},{"location":"changelog/#0150--2019-08-11","title":"0.15.0 / 2019-08-11","text":"<ul> <li>Added support for <code>v1Job</code> resources - via #58</li> <li>Added support for <code>v1Namespace</code>, <code>v1DaemonSet</code>, <code>v1ReplicaSet</code>, <code>v1PersistentVolumeClaim</code>, <code>v1PersistentVolume</code>, <code>v1ResourceQuota</code>, <code>v1HorizontalPodAutoscaler</code> - via #59</li> <li>Added support for <code>v1beta1CronJob</code>, <code>v1ReplicationController</code>, <code>v1VolumeAttachment</code>, <code>v1NetworkPolicy</code> - via #60</li> <li><code>k8s-openapi</code> optional dependency bumped to <code>0.5.0</code> (for kube 1.14 structs)</li> </ul>"},{"location":"changelog/#0140--2019-08-03","title":"0.14.0 / 2019-08-03","text":"<ul> <li><code>Reflector::read</code> now returns a <code>Vec&lt;K&gt;`` rather than a</code>Vec&lt;(name, K)&gt;`:     This fixes an unsoundness bug internally - #56 via @gnieto</li> </ul>"},{"location":"changelog/#0130--2019-07-22","title":"0.13.0 / 2019-07-22","text":"<ul> <li>Experimental oauth2 support for some providers - via #44 :<ul> <li>a big cherry-pick from various prs upstream originally for GCP</li> <li>EKS works with setup in https://github.com/kube-rs/kube/pull/20#issuecomment-511767551</li> </ul> </li> </ul>"},{"location":"changelog/#0120--2019-07-18","title":"0.12.0 / 2019-07-18","text":"<ul> <li>Added support for <code>Log</code> subresource - via #50</li> <li>Added support for <code>v1ConfigMap</code> with example - via #49</li> <li>Demoted some spammy info messages from Reflector</li> </ul>"},{"location":"changelog/#0110--2019-07-10","title":"0.11.0 / 2019-07-10","text":"<ul> <li>Added <code>PatchParams</code> with <code>PatchStrategy</code> to allow arbitrary patch types - #24 via @ragne</li> <li><code>Event</code> renamed to <code>v1Event</code> to match non-slowflake type names</li> <li><code>v1Service</code> support added</li> <li>Added <code>v1Secret</code> snowflake type and a <code>secret_reflector</code> example</li> </ul>"},{"location":"changelog/#0100--2019-06-03","title":"0.10.0 / 2019-06-03","text":"<ul> <li><code>Api&lt;P, U&gt;</code> is now <code>Api&lt;K&gt;</code> for some <code>KubeObject</code> K:<ul> <li>Big change to allow snowflake objects (#35) - but also slightly nicer</li> <li>You want aliases <code>type Pod = Object&lt;PodSpec, PodStatus&gt;</code></li> <li>This gives you the required <code>KubeObject</code> trait impl for free</li> </ul> </li> <li> <p>Added <code>Event</code> native type to prove snowflakes can be handled - #35</p> </li> <li> <p><code>ApiStatus</code> renamed to <code>Status</code> to match kube api conventions #36</p> </li> <li>Rename <code>Metadata</code> to <code>ObjectMeta</code> #36</li> <li>Added <code>ListMeta</code> for <code>ObjectList</code> and <code>Status</code> #36</li> <li>Added <code>TypeMeta</code> object which is flattened onto <code>Object</code>, so:<ul> <li><code>o.types.kind</code> rather than <code>o.kind</code></li> <li><code>o.types.version</code> rather than <code>o.version</code></li> </ul> </li> </ul>"},{"location":"changelog/#090--2019-06-02","title":"0.9.0 / 2019-06-02","text":"<ul> <li>Status subresource api commands added to <code>Api</code>:<ul> <li><code>patch_status</code></li> <li><code>get_status</code></li> <li><code>replace_status</code>   ^ See <code>crd_openapi</code> or <code>crd_api</code> examples</li> </ul> </li> <li>Scale subresource commands added to <code>Api</code>:<ul> <li><code>patch_scale</code></li> <li><code>get_scale</code></li> <li><code>replace_scale</code>   ^ See <code>crd_openapi</code> example</li> </ul> </li> </ul>"},{"location":"changelog/#080--2019-05-31","title":"0.8.0 / 2019-05-31","text":"<ul> <li>Typed <code>Api</code> variant called <code>OpenApi</code> introduced (see crd_openapi example)</li> <li>Revert <code>client.request</code> return type change (back to response only from pre-0.7.0 #28)</li> <li><code>delete</code> now returns `Either, ApiStatus&gt; - for bug#32 <li><code>delete_collection</code> now returns `Either&gt;, ApiStatus&gt; - for bug#32 <li><code>Informer::new</code> renamed to <code>Informer::raw</code></li> <li><code>Reflector::new</code> renamed to <code>Reflector::raw</code></li> <li><code>Reflector::new</code> + <code>Informer::new</code> added for \"openapi\" compile time feature (does not require specifying the generic types)</li>"},{"location":"changelog/#070--2019-05-27","title":"0.7.0 / 2019-05-27","text":"<ul> <li>Expose list/watch parameters #11</li> <li>Many API struct renames:<ul> <li><code>ResourceMap</code> -&gt; <code>Cache</code></li> <li><code>Resource</code> -&gt; <code>Object</code></li> <li><code>ResourceList</code> -&gt; <code>ObjectList</code></li> <li><code>ApiResource</code> -&gt; <code>Api</code></li> </ul> </li> <li><code>ResourceType</code> has been removed in favour of <code>Api::v1Pod()</code> say</li> <li><code>Object::status</code> now wrapped in an <code>Option</code> (not present everywhere)</li> <li><code>ObjectList</code> exposed</li> <li>Major API overhaul to support generic operations on <code>Object</code></li> <li>Api can be used to perform generic actions on resources:<ul> <li><code>create</code></li> <li><code>get</code></li> <li><code>delete</code></li> <li><code>watch</code></li> <li><code>list</code></li> <li><code>patch</code></li> <li><code>replace</code></li> <li><code>get_scale</code> (when scale subresource exists)</li> <li><code>patch_scale</code> (ditto)</li> <li><code>replace_scale</code> (ditto)</li> <li><code>get_status</code> (when status subresource exists)</li> <li><code>patch_status</code> (ditto)</li> <li><code>replace_status</code> (ditto)</li> </ul> </li> <li>crd_api example added to track the action api</li> <li>Bunch of generic parameter structs exposed for common operations:<ul> <li><code>ListParams</code> exposed</li> <li><code>DeleteParams</code> exposed</li> <li><code>PostParams</code> exposed</li> </ul> </li> <li>Errors from <code>Api</code> exposed in <code>kube::Error</code>:<ul> <li><code>Error::api_error -&gt; Option&lt;ApiError&gt;</code> exposed</li> <li>Various other error types also in there (but awkward setup atm)</li> </ul> </li> <li><code>client.request</code> now returns a tuple <code>(T, StatusCode)</code> (before only <code>T</code>)</li> </ul>"},{"location":"changelog/#060--2019-05-12","title":"0.6.0 / 2019-05-12","text":"<ul> <li>Expose getter <code>Informer::version</code></li> <li>Exose ctor <code>Informer::from_version</code></li> <li>Expose more attributes in <code>Metadata</code></li> <li><code>Informer::reset</code> convenience method added</li> <li><code>Informer::poll</code> no longer returns events straight</li> <li>an <code>Informer</code> now caches <code>WatchEvent</code> elements into an internal queue</li> <li><code>Informer::pop</code> pops a single element from its internal queue</li> <li><code>Reflector::refresh</code> renamed to <code>Reflector::reset</code> (matches <code>Informer</code>)</li> <li><code>Void</code> type added so we can use <code>Reflector&lt;ActualSpec, Void&gt;</code><ul> <li>removes need for Spec/Status structs:</li> <li><code>ReflectorSpec</code>, <code>ReflectorStatus</code> removed</li> <li><code>InformerSpec</code>, <code>InformerStatus</code> removed</li> <li><code>ResourceSpecMap</code>, <code>ResourceStatusMap</code> removed</li> </ul> </li> <li><code>WatchEvents</code> removed</li> <li><code>WatchEvent</code> exposed, and now wraps `Resource``"},{"location":"changelog/#050--2019-05-09","title":"0.5.0 / 2019-05-09","text":"<ul> <li>added <code>Informer</code> struct dedicated to handling events</li> <li>Reflectors no longer cache <code>events</code> - see #6</li> </ul>"},{"location":"changelog/#040--2019-05-09","title":"0.4.0 / 2019-05-09","text":"<ul> <li>ResourceMap now contains the full Resource struct rather than a tuple as the value. =&gt; <code>value.metadata</code> is available in the cache. <li>Reflectors now also cache <code>events</code> to allow apps to handle them</li>"},{"location":"changelog/#030--2019-05-09","title":"0.3.0 / 2019-05-09","text":"<ul> <li><code>Named</code> trait removed (inferring from metadata.name now)</li> <li>Reflectors now take two type parameters (unless you use <code>ReflectorSpec</code> or <code>ReflectorStatus</code>) - see examples for usage</li> <li>Native kube types supported via <code>ApiResource</code></li> <li>Some native kube resources have easy converters to <code>ApiResource</code></li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing-guide","title":"Contributing Guide","text":"<p>This document describes the requirements for committing to this repository.</p>"},{"location":"contributing/#developer-certificate-of-origin-dco","title":"Developer Certificate of Origin (DCO)","text":"<p>In order to contribute to this project, you must sign each of your commits to attest that you have the right to contribute that code. This is done with the <code>-s</code>/<code>--signoff</code> flag on <code>git commit</code>. More information about <code>DCO</code> can be found here</p>"},{"location":"contributing/#pull-request-management","title":"Pull Request Management","text":"<p>All code that is contributed to kube-rs must go through the Pull Request (PR) process. To contribute a PR, fork this project, create a new branch, make changes on that branch, and then use GitHub to open a pull request with your changes.</p> <p>Every PR must be reviewed by at least one Maintainer of the project. Once a PR has been marked \"Approved\" by a Maintainer (and no other Maintainer has an open \"Rejected\" vote), the PR may be merged. While it is fine for non-maintainers to contribute their own code reviews, those reviews do not satisfy the above requirement.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the CNCF Code of Conduct.</p>"},{"location":"contributing/#rust-guidelines","title":"Rust Guidelines","text":"<ul> <li>Channel: Code is built and tested using the stable channel of Rust, but documented and formatted with nightly <sup>*</sup></li> <li>Formatting: To format the codebase, run <code>just fmt</code></li> <li>Documentation To check documentation, run <code>just doc</code></li> <li>Testing: To run tests, run <code>just test</code> and see below.</li> </ul> <p>For a list of tooling that we glue together everything see TOOLS.md.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>We have 3 classes of tests.</p> <ul> <li>Unit tests &amp; Documentation Tests</li> <li>Integration tests (requires Kubernetes)</li> <li>End to End tests (requires Kubernetes)</li> </ul> <p>The last two will try to access the Kubernetes cluster that is your <code>current-context</code>; i.e. via your local <code>KUBECONFIG</code> evar or <code>~/.kube/config</code> file.</p> <p>The easiest way set up a minimal Kubernetes cluster for these is with <code>k3d</code> (<code>just k3d</code>).</p>"},{"location":"contributing/#unit-tests--documentation-tests","title":"Unit Tests &amp; Documentation Tests","text":"<p>Unit and doc tests are run against a particular crate with <code>cargo test -p KUBECRATE --lib --doc</code>, but because of feature-sets, you will need a couple of extra flags and invocations to replicate all our CI conditions.</p> <p>To run all unit tests, call: <code>just test</code></p> <p>All public interfaces must be documented, and most should have minor documentation examples to show usage.</p>"},{"location":"contributing/#integration-tests","title":"Integration Tests","text":"<p>Slower set of tests within the crates marked with an <code>#[ignore]</code> attribute.</p> <p> These  WILL try to modify resources in your current cluster </p> <p>Integration tests are run against a crate with <code>cargo test -p KUBECRATE --lib -- --ignored</code>, but because of feature-sets, you will need a few invocations of these to replicate our CI.</p> <p>To run all integration tests, call: <code>just test-integration</code></p>"},{"location":"contributing/#end-to-end-tests","title":"End to End Tests","text":"<p>We have a small set of e2e tests that tests difference between in-cluster and local configuration.</p> <p>These tests are the heaviest tests we have because they require a full <code>docker build</code>, image import (or push/pull flow), yaml construction, and <code>kubectl</code> usage to verify that the outcome was sufficient.</p> <p>To run E2E tests, use (or follow) <code>just e2e</code> as appropriate.</p>"},{"location":"contributing/#test-guidelines","title":"Test Guidelines","text":""},{"location":"contributing/#when-to-add-a-test","title":"When to add a test","text":"<p>All public interfaces should have doc tests with examples for docs.rs.</p> <p>When adding new non-trivial pieces of logic that results in a drop in coverage you should add a test.</p> <p>Cross-reference with the coverage build  and go to your branch. Coverage can also be run locally with <code>cargo tarpaulin</code> at project root. This will use our tarpaulin.toml config, and will run both unit and integration tests.</p>"},{"location":"contributing/#what-type-of-test","title":"What type of test","text":"<ul> <li>Unit tests MUST NOT try to contact a Kubernetes cluster</li> <li>Doc tests MUST be marked as <code>no_run</code> when they need to contact a Kubernetes cluster</li> <li>Integration tests MUST NOT be used when a unit test is sufficient</li> <li>Integration tests MUST NOT assume existence of non-standard objects in the cluster</li> <li>Integration tests MUST NOT cross-depend on other unit tests completing (and installing what you need)</li> <li>E2E tests MUST NOT be used where an integration test is sufficient</li> </ul> <p>In general: use the least powerful method of testing available to you:</p> <ul> <li>use unit tests in <code>kube-core</code></li> <li>use unit tests in <code>kube-client</code> (and in rare cases integration tests)</li> <li>use unit tests in <code>kube-runtime</code> (and occassionally integration tests)</li> <li>use e2e tests when testing differences between in-cluster and local configuration</li> </ul>"},{"location":"contributing/#support","title":"Support","text":""},{"location":"contributing/#documentation","title":"Documentation","text":"<p>The high-level architecture document is written for contributors.</p>"},{"location":"contributing/#contact","title":"Contact","text":"<p>You can ask general questions / share ideas / query the community at the kube-rs discussions forum. You can reach the maintainers of this project at #kube channel on the Tokio discord.</p>"},{"location":"features/","title":"Features","text":"<p>All public features are exposed through <code>kube</code> in kube's Cargo.toml.</p>"},{"location":"features/#stable-features","title":"Stable Features","text":"Feature Enables Default Significant Inclusions config Config yes kube-client partial client Client + Api yes kube-client, hyper, tower runtime Controller + watcher no kube-runtime derive CustomResource no kube-derive, syn, quote openssl-tls tls via openssl no openssl, hyper-openssl rustls-tls tls via rustls yes rustls, hyper-rustls ring rustls via ring yes ring aws-lc-rs rustls via aws-lc-rs no aws-lc-rs ws Execute, Attach, Portforward no tokio-tungstenite gzip gzip compressed transport no tower-http feature jsonpatch Patch using jsonpatch no json_patch admission admission module no json_patch socks5 local cluster socks5 proxying no hyper-socks2 http-proxy local cluster http proxying no hyper-http-proxy oauth local cluster oauth for GCP no tame-oauth oidc local cluster oidc auth no none webpki-roots Mozilla's root certificates no webpki-roots <p>Client dependencies</p> <p>Most of these features depend on having the <code>client</code> feature (and thus the <code>config</code> feature) enabled as they would not be much use without them.</p> <p>--no-default-features</p> <p>If you turn off all default features (say, to change tls stacks), you also turn off the normal <code>client</code> default feature. Without default features you get a crate roughly equivalent to kube-core.</p>"},{"location":"features/#unstable-features","title":"Unstable Features","text":"<ul> <li><code>unstable-runtime</code> for stream sharing and controller streams interface tracked in #1080</li> <li><code>unstable-client</code> for client exts tracked in #1032</li> <li><code>kubelet-debug</code> for kubelet debug api access - untracked</li> </ul>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"getting-started/#getting-started","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Select a version of <code>kube</code> along matching versions of k8s-openapi and schemars for Kubernetes structs and matching schemas. See also historical Kubernetes versions.</p> <pre><code>[dependencies]\nkube = { version = \"2.0.1\", features = [\"runtime\", \"derive\"] }\nk8s-openapi = { version = \"0.26.0\", features = [\"latest\", \"schemars\"] }\nschemars = { version = \"1\" }\n</code></pre> <p>See features for a quick overview of default-enabled / opt-in functionality. You can remove <code>schemars</code> parts if you do not need the <code>kube/derive</code> feature.</p>"},{"location":"getting-started/#upgrading","title":"Upgrading","text":"<p>See kube.rs/upgrading. Noteworthy changes are highlighted in releases, and archived in the changelog.</p>"},{"location":"getting-started/#usage","title":"Usage","text":"<p>See the examples directory for how to use any of these crates.</p> <ul> <li>kube API Docs</li> <li>kube.rs</li> </ul> <p>Official examples:</p> <ul> <li>version-rs: lightweight deployment <code>reflector</code> using axum</li> <li>controller-rs: <code>Controller</code> of a crd inside actix</li> </ul> <p>For real world projects see ADOPTERS.</p>"},{"location":"getting-started/#api","title":"Api","text":"<p>The <code>Api</code> is what interacts with Kubernetes resources, and is generic over <code>Resource</code>:</p> <pre><code>use k8s_openapi::api::core::v1::Pod;\nlet pods: Api&lt;Pod&gt; = Api::default_namespaced(client);\n\nlet pod = pods.get(\"blog\").await?;\nprintln!(\"Got pod: {pod:?}\");\n\nlet patch = json!({\"spec\": {\n    \"activeDeadlineSeconds\": 5\n}});\nlet pp = PatchParams::apply(\"kube\");\nlet patched = pods.patch(\"blog\", &amp;pp, &amp;Patch::Apply(patch)).await?;\nassert_eq!(patched.spec.active_deadline_seconds, Some(5));\n\npods.delete(\"blog\", &amp;DeleteParams::default()).await?;\n</code></pre> <p>See the examples ending in <code>_api</code> examples for more detail.</p>"},{"location":"getting-started/#custom-resource-definitions","title":"Custom Resource Definitions","text":"<p>Working with custom resources uses automatic code-generation via proc_macros in kube-derive.</p> <p>You need to add <code>#[derive(CustomResource, JsonSchema)]</code> and some <code>#[kube(attrs..)]</code> on a spec struct:</p> <pre><code>#[derive(CustomResource, Debug, Serialize, Deserialize, Default, Clone, JsonSchema)]\n#[kube(group = \"kube.rs\", version = \"v1\", kind = \"Document\", namespaced)]\npub struct DocumentSpec {\n    title: String,\n    content: String,\n}\n</code></pre> <p>Then you can use the generated wrapper struct <code>Document</code> as a <code>kube::Resource</code>:</p> <pre><code>let docs: Api&lt;Document&gt; = Api::default_namespaced(client);\nlet d = Document::new(\"guide\", DocumentSpec::default());\nprintln!(\"doc: {:?}\", d);\nprintln!(\"crd: {:?}\", serde_yaml::to_string(&amp;Document::crd()));\n</code></pre> <p>There are a ton of kubebuilder-like instructions that you can annotate with here. See the documentation or the <code>crd_</code> prefixed examples for more.</p> <p>NB: <code>#[derive(CustomResource)]</code> requires the <code>derive</code> feature enabled on <code>kube</code>.</p>"},{"location":"getting-started/#runtime","title":"Runtime","text":"<p>The <code>runtime</code> module exports the <code>kube_runtime</code> crate and contains higher level abstractions on top of the <code>Api</code> and <code>Resource</code> types so that you don't have to do all the <code>watch</code>/<code>resourceVersion</code>/storage book-keeping yourself.</p>"},{"location":"getting-started/#watchers","title":"Watchers","text":"<p>A streaming interface (similar to informers) that presents <code>watcher::Event</code>s and does automatic relists under the hood.</p> <pre><code>let api = Api::&lt;Pod&gt;::default_namespaced(client);\nlet stream = watcher(api, Config::default()).default_backoff().applied_objects();\n</code></pre> <p>This now gives a continual stream of events and you do not need to care about the watch having to restart, or connections dropping.</p> <pre><code>while let Some(event) = stream.try_next().await? {\n    println!(\"Applied: {}\", event.name_any());\n}\n</code></pre> <p>Note the base items from a <code>watcher</code> stream are an abstraction above the native <code>WatchEvent</code> to allow for store buffering. If you are following along to \"see what changed\", you can use utilities from <code>WatchStreamExt</code>, such as <code>applied_objects</code> to get a more conventional stream.</p>"},{"location":"getting-started/#reflectors","title":"Reflectors","text":"<p>A <code>reflector</code> is a <code>watcher</code> with <code>Store</code> on <code>K</code>. It acts on all the <code>Event&lt;K&gt;</code> exposed by <code>watcher</code> to ensure that the state in the <code>Store</code> is as accurate as possible.</p> <pre><code>let nodes: Api&lt;Node&gt; = Api::all(client);\nlet lp = Config::default().labels(\"kubernetes.io/arch=amd64\");\nlet (reader, writer) = reflector::store();\nlet rf = reflector(writer, watcher(nodes, lp));\n</code></pre> <p>At this point you can listen to the <code>reflector</code> as if it was a <code>watcher</code>, but you can also query the <code>reader</code> at any point.</p>"},{"location":"getting-started/#controllers","title":"Controllers","text":"<p>A <code>Controller</code> is a <code>reflector</code> along with an arbitrary number of watchers that schedule events internally to send events through a reconciler:</p> <pre><code>Controller::new(root_kind_api, Config::default())\n    .owns(child_kind_api, Config::default())\n    .run(reconcile, error_policy, context)\n    .for_each(|res| async move {\n        match res {\n            Ok(o) =&gt; info!(\"reconciled {:?}\", o),\n            Err(e) =&gt; warn!(\"reconcile failed: {}\", Report::from(e)),\n        }\n    })\n    .await;\n</code></pre> <p>Here <code>reconcile</code> and <code>error_policy</code> refer to functions you define. The first will be called when the root or child elements change, and the second when the <code>reconciler</code> returns an <code>Err</code>.</p> <p>See the controller guide for how to write these.</p>"},{"location":"getting-started/#tls","title":"TLS","text":"<p>Uses rustls with <code>ring</code> provider (default) or <code>aws-lc-rs</code> provider (optional).</p> <p>To switch rustls providers, turn off <code>default-features</code> and enable the <code>aws-lc-rs</code> feature:</p> <pre><code>kube = { version = \"2.0.1\", default-features = false, features = [\"client\", \"rustls-tls\", \"aws-lc-rs\"] }\n</code></pre> <p>To switch to <code>openssl</code>, turn off <code>default-features</code>, and enable the <code>openssl-tls</code> feature:</p> <pre><code>kube = { version = \"2.0.1\", default-features = false, features = [\"client\", \"openssl-tls\"] }\n</code></pre> <p>This will pull in <code>openssl</code> and <code>hyper-openssl</code>. If <code>default-features</code> is left enabled, you will pull in two TLS stacks, and the default will remain as <code>rustls</code>.</p>"},{"location":"getting-started/#musl-libc","title":"musl-libc","text":"<p>Kube will work with distroless, scratch, and <code>alpine</code> (it's also possible to use alpine as a builder with some caveats).</p>"},{"location":"getting-started/#license","title":"License","text":"<p>Apache 2.0 licensed. See LICENSE for details.</p>"},{"location":"governance/","title":"Governance","text":"<p>This document defines project governance for Kube-rs.</p>"},{"location":"governance/#contributors","title":"Contributors","text":"<p>Kube-rs is for everyone. Anyone can become a Kube-rs contributor simply by contributing to the project, whether through code, documentation, blog posts, community management, or other means. As with all Kube-rs community members, contributors are expected to follow the Kube-rs Code of Conduct.</p> <p>All contributions to Kube-rs code, documentation, or other components in the Kube-rs GitHub org must follow the guidelines in CONTRIBUTING.md. Whether these contributions are merged into the project is the prerogative of the maintainers.</p>"},{"location":"governance/#maintainer-expectations","title":"Maintainer Expectations","text":"<p>Maintainers have the ability to merge code into the project. Anyone can become a Kube-rs maintainer (see \"Becoming a maintainer\" below.)</p> <p>As such, there are certain expectations for maintainers. Kube-rs maintainers are expected to:</p> <ul> <li>Review pull requests, triage issues, and fix bugs in their areas of expertise, ensuring that all changes go through the project's code review and integration processes.</li> <li>Monitor the Kube-rs Discord, and Discussions and help out when possible.</li> <li>Rapidly respond to any time-sensitive security release processes.</li> <li>Participate on discussions on the roadmap.</li> </ul> <p>If a maintainer is no longer interested in or cannot perform the duties listed above, they should move themselves to emeritus status. If necessary, this can also occur through the decision-making process outlined below.</p>"},{"location":"governance/#maintainer-decision-making","title":"Maintainer decision-making","text":"<p>Ideally, all project decisions are resolved by maintainer consensus. If this is not possible, maintainers may call a vote. The voting process is a simple majority in which each maintainer receives one vote.</p>"},{"location":"governance/#special-tasks","title":"Special Tasks","text":"<p>In addition to the outlined abilities and responsibilities outlined above, some maintainers take on additional tasks and responsibilities.</p>"},{"location":"governance/#release-tasks","title":"Release Tasks","text":"<p>As a maintainer on the release team, you are expected to be cut releases. In particular:</p> <ul> <li>Cut releases, and update the CHANGELOG</li> <li>Pre-verify big releases against example repos</li> <li>Publish and update versions in example repos</li> <li>Verify the release</li> </ul>"},{"location":"governance/#becoming-a-maintainer","title":"Becoming a maintainer","text":"<p>Anyone can become a Kube-rs maintainer. Maintainers should be highly proficient in Rust; have relevant domain expertise; have the time and ability to meet the maintainer expectations above; and demonstrate the ability to work with the existing maintainers and project processes.</p> <p>To become a maintainer, start by expressing interest to existing maintainers. Existing maintainers will then ask you to demonstrate the qualifications above by contributing PRs, doing code reviews, and other such tasks under their guidance. After several months of working together, maintainers will decide whether to grant maintainer status.</p>"},{"location":"guides/","title":"Guides and Tutorials","text":"<p>List of third party guides and tutorials for using the <code>kube-rs</code> that holds up to scrutiny.</p> Title Year Use Writing a Kubernetes Scheduler in Rust 2023 Scheduler PoC Writing a Kubernetes Operator 2023 Operator with extension api-server Oxidizing the Kubernetes operator 2021 Controller guide with finalizers, state machines A Rust controller for Kubernetes 2021 Introductory; project setup + api use"},{"location":"guides/#presentations","title":"Presentations","text":"Title Source Content Kubernetes Controllers in Rust: Fast, Safe, Sane KubeCon EU 2024 Controller building from Linkerd POV Rust operators for Kubernetes PlatformCon 2023 Rust benefits + kubebuilder comparisons Introduction to rust operators for Kubernetes Cloud Native Skunkworks 2023 Introductory runtime structuring Lightning Talk: Talking to Kubernetes with Rust KubeCon EU 2023 Introductory; api usage Why the future of the cloud will be built on Rust Cloud Native Rust Day 2021 Cloud History + Linkerd + Rust Ecosystem Overview The Hidden Generics in Kubernetes' API KubeCon Virtual 2020 Early stage kube-rs presentation on apimachinery + client-go translation"},{"location":"guides/#ai-agents","title":"AI Agents","text":"<p>Generative AI Generates Errors</p> <p>Please be skeptical with output from AI tools or agents. They sound confident, while frequently presenting severely misleading errors, and often erase the sources. Do not be surprised if its solutions turn out to be total nonsense.</p> <p>Rust documentation and kube documentation is already extensive. Consider searching docs.rs/kube, kube.rs, and kube/discussions for important context.</p> Title Provider Description kube-rs Guru Gurubase.io kube-rs Guru is a kube-rs focused AI to answer user queries based on the data on kube-rs documentation."},{"location":"kubernetes-version/","title":"Kubernetes Version","text":""},{"location":"kubernetes-version/#policy","title":"Policy","text":"<p>Our Kubernetes version compatibility is following a similar strategy to the one employed by client-go and can interoperate under a wide range of Kubernetes versions. We define by a soft minimum (MK8SV) based on the current latest available Kubernetes version in the generated source.</p> <p>Minimum Kubernetes Version Policy</p> <p>The Minimum Supported Kubernetes Version (MK8SV) is 5 releases less than the latest Kubernetes version.</p> <p>The minimum indicates the lower bound of our testing range, and the latest is the maximum Kubernetes version selectable as a target version. The minimum has evolved like this:</p> kube version MK8SV Latest Generated Source Requirements <code>main</code> X 1.36 k8s-openapi@main 2.0.0 <code>1.30</code>* <code>1.34</code> k8s-openapi@0.26.0 schemars@1 1.0.0 <code>1.30</code>* <code>1.33</code> k8s-openapi@0.25.0 schemars@0.8 0.98.0 <code>1.28</code> <code>1.32</code> k8s-openapi@0.24.0 0.95.0 <code>1.26</code> <code>1.31</code> k8s-openapi@0.23.0 0.91.0 <code>1.25</code> <code>1.30</code> k8s-openapi@0.22.0 hyper@1 0.88.0 <code>1.24</code> <code>1.29</code> k8s-openapi@0.21.0 hyper@0.14 0.86.0 <code>1.23</code> <code>1.28</code> k8s-openapi@0.20.0 0.85.0 <code>1.22</code> <code>1.27</code> k8s-openapi@0.19.0 0.78.0 <code>1.21</code> <code>1.26</code> k8s-openapi@0.17.0 0.75.0 <code>1.20</code> <code>1.25</code> k8s-openapi@0.16.0 0.73.0 <code>1.19</code> <code>1.24</code> k8s-openapi@0.15.0 0.67.0 <code>1.18</code> <code>1.23</code> k8s-openapi@0.14.0 <p>This policy is intended to match stable channel support within major cloud providers. Compare with: EKS, AKS, GKE, upstream Kubernetes.</p> <p>It is displayed in the main README as a badge: </p>"},{"location":"kubernetes-version/#picking-versions","title":"Picking Versions","text":"<p>Given a <code>kube</code> version, you may choose a target Kubernetes version from the available features in the generated source that is used by that kube version.</p>"},{"location":"kubernetes-version/#example","title":"Example","text":"<p>When using <code>kube@0.87.1</code>, the generated source is <code>k8s-openapi@0.20.0</code>, which exports the following version features. The <code>latest</code> supported version feature is here aliased to <code>v1_28</code>, our minimum tested version is <code>v1_23</code>.</p>"},{"location":"kubernetes-version/#guideline","title":"Guideline","text":"<p>Recommendation is <code>latest</code></p> <p>The <code>latest</code> feature as your target version is a good default choice, even when running against older clusters. Consider pinning to a specific cluster version if you are programming explicitly against deprecated or alpha apis.</p> <p>See the version skew outcomes if you are unsure whether you need to pin a version.</p>"},{"location":"kubernetes-version/#version-skew","title":"Version Skew","text":"<p>How kube version skew interacts with clusters is largely determined by how Kubernetes deprecates api versions upstream.</p> <p>Consider the following outcomes when picking target versions based on your cluster version:</p> <ol> <li>if <code>target version == cluster version</code> (cluster in sync with kube), then:<ul> <li>kube has api parity with cluster</li> <li>Rust structs are all queryable via kube</li> </ul> </li> <li>if <code>target version &gt; cluster version</code> (cluster behind kube), then:<ul> <li>kube has more recent api features than the cluster supports</li> <li>recent Rust api structs might not work with the cluster version yet</li> <li>deprecated/alpha apis might have been removed from Rust structs \u26a1</li> </ul> </li> <li>if <code>target version &lt; cluster version</code> (cluster ahead of kube), then:<ul> <li>kube has less recent api features than the cluster supports</li> <li>recent Kubernetes resources might not have Rust struct counterparts</li> <li>deprecated/alpha apis might have been removed from the cluster \u26a1</li> </ul> </li> </ol> <p>Kubernetes takes a long time to remove deprecated apis (unless they alpha or beta apis), so the acceptable distance from your cluster version depends on what apis you target.</p> <p>In particular, when using stable (or your own custom) api resources, exceeding the range will have little impact.</p> <p>If you are targeting deprecated/alpha apis on the other hand, then you should pick a target version in sync with your cluster, as alpha apis may vanish or change significantly in a single release, and is not covered by any guarantees.</p> <p>Relying on alpha apis will make the amount of upgrades required to an application more frequent. To alleviate this; consider using api discovery to match on available api versions rather than writing code against each Kubernetes version.</p>"},{"location":"kubernetes-version/#outside-the-range","title":"Outside The Range","text":"<p>We recommend developers stay within the supported version range for the best experience, but it is technically possible to operate outside the bounds of this range (by picking older features from <code>k8s-openapi</code>, or by running against older clusters).</p> <p>Untested Version Combinations</p> <p>While exceeding the supported version range is likely to work for most api resources: we do not test kube's functionality outside this version range.</p> <p>In minor skews, kube and Kubernetes will share a large functioning API surface, while relying on deprecated apis to fill the gap. However, the further you stray from the range you are more likely to encounter Rust structs that doesn't work against your cluster, or miss support for resources entirely.</p>"},{"location":"kubernetes-version/#abstractions","title":"Abstractions","text":"<p>For a small number of api resources, kube provides abstractions that are not managed along with the generated sources. For these cases we track the source and remove when Kubernetes removes them (to avoid double dipping on deprecation time).</p> <p>This affects a small number of special resources such as <code>CustomResourceDefinition</code>, <code>Event</code>, <code>Lease</code>, <code>AdmissionReview</code>.</p>"},{"location":"kubernetes-version/#example_1","title":"Example","text":"<p>The <code>CustomResourceDefinition</code> resource at <code>v1beta1</code> was removed in Kubernetes <code>1.22</code>:</p> <p>The apiextensions.k8s.io/v1beta1 API version of CustomResourceDefinition is no longer served as of v1.22.</p> <p>Their replacement; in <code>v1</code> was released in Kubernetes <code>1.16</code>.</p> <p>Kube had special support for both versions of <code>CustomResourceDefinition</code> from <code>0.26.0</code> up until <code>0.72.0</code> when kube supported structs from Kubernetes &gt;= 1.22.</p> <p>This special support took the form of the proc macro CustomResource and associated helpers that allowing pinning the crd version to <code>v1beta1</code> up until its removal. It is now <code>v1</code> only.</p>"},{"location":"maintainers/","title":"Maintainers","text":"<p>The Kube-rs maintainers are:</p> <ul> <li>Eirik Albrigtsen sszynrae@gmail.com @clux</li> <li>Natalie Klestrup R\u00f6ijezon nat@nullable.se @nightkr</li> <li>Kaz Yoshihara kazk.dev@gmail.com @kazk</li> <li>Matei David dev.matei@pm.me @mateiidavid</li> <li>Danil Grigorev daniil.grigorev.dev@gmail.com @Danil-Grigorev</li> </ul>"},{"location":"maintainers/#emeriti","title":"Emeriti","text":"<p>Former maintainers include:</p> <ul> <li>Ryan Levick ryan.levick@gmail.com @rylev</li> </ul>"},{"location":"profiling/","title":"Profiling","text":""},{"location":"profiling/#local-cpu-profiling-with-samply","title":"Local CPU Profiling with Samply","text":"<p>Using samply we can run kube applications to generate cpu profiles for the firefox profiler.</p> <p>The documentation above is canonical, but here's a tiny TL;DR:</p> <ol> <li>build/install samply</li> <li>Linux: provide necessary perf_ kernel parameters</li> <li>create a profiling profile (debug symbols in release mode)</li> </ol> <pre><code>[profile.profiling]\ninherits = \"release\"\ndebug = true\n</code></pre> <ol> <li><code>cargo build --profile profiling myapp</code></li> <li><code>samply record ./target/profiling/myapp</code></li> <li>wait for myapp to run code you wish to profile, then terminate it</li> <li><code>samply load profile.json</code> (automatic on termination)</li> </ol>"},{"location":"quick-tutorial/","title":"Quick Tutorial","text":""},{"location":"release-process/","title":"Release Process","text":"<p>This document elaborates on the process defined by release.toml in the kube workspace.</p>"},{"location":"release-process/#versioning","title":"Versioning","text":"<p>We release all public workspace crates with the same version.</p> <p>The crates are published in reverse order of inclusion, releasing the final facade crate <code>kube</code> last, so users do not notice any version-mismatches during releases.</p> <p>You should only need to depend on <code>kube</code></p> <p>Only <code>kube = {...}</code> needs an entry in your <code>Cargo.toml</code>, as <code>kube</code> re-exports the sub-crates.</p>"},{"location":"release-process/#cadence","title":"Cadence","text":"<p>We have no fixed release cadence, but we still try to release roughly once a month, or whenever important PRs are merged (whichever is earliest).</p>"},{"location":"release-process/#for-maintainers-cutting-releases","title":"For maintainers: Cutting Releases","text":"<p>Cutting releases is a task for the maintenance team (contributing) and requires developer tools installed.</p> <p>The process is mostly automated once the preliminary steps are taken, and a publish should complete in 5m with everything setup.</p> <p>You have the option to write highlights in the release notes, but this can be elided with a \"Publish\" button press on the draft release.</p>"},{"location":"release-process/#preliminary-steps","title":"Preliminary Steps","text":"<p>Close the current ongoing milestone, and create the next one. Move over any unmerged PRs to the next milestone.</p> <p>Release Notes Depend on Github Release Notes Labels</p> <p>For best success, every PR should have one <code>changelog-*</code> label and should be included in the current milestone (to avoid scouring prs merged since the last version later).</p> <p>Ensure the PRs in the milestone all have exactly one <code>changelog-*</code> label to ensure the release notes are generated correctly (we follow Keep a Changelog with the setup as outlined in #754).</p>"},{"location":"release-process/#publishing-crates","title":"Publishing Crates","text":"<p>Start the process on latest <code>main</code> by publishing to crates.io with <code>cargo-release</code> locally using the latest stable rust toolchain installed, and git setup to be able to sign tags. Run:</p> <pre><code>PUBLISH_GRACE_SLEEP=20 cargo release minor --execute\n</code></pre> <p>once this completes, the following crates should all have been published to crates.io:</p> <ul> <li>kube-core</li> <li>kube-derive</li> <li>kube-client</li> <li>kube-runtime</li> <li>kube</li> </ul> <p>The crate publish will enqueue a documentation build in docs.rs.</p> <p>docs.rs failures</p> <p>If the <code>docs.rs</code> build fails, please create an issue for it with the build log from docs.rs.</p> <p>As part of the <code>cargo release</code>, a signed git tag will automatically be pushed to kube/tags.</p>"},{"location":"release-process/#publishing-the-release","title":"Publishing the Release","text":"<p>Once crates have been published and the corresponding tag pushed, a GitHub Release will be generated by the release job in the draft  state. This can be edited for extra documentation / migration notes. The section below contains ideas for what to include.</p> <p>Publish Soon</p> <p>We should not spend too much time adding extra information (&lt;1h), because dependabot will quickly pick up on a release, and start publishing dependency update PRs to downstream users. If our release notes are not ready, they might see a PR without release notes.</p>"},{"location":"release-process/#adding-documentation","title":"Adding Documentation","text":"<p>If you are writing extra documentation, then start editing the draft release.</p> <p>You will notice auto-generated notes already present in the <code>textarea</code> along with new contributors - please leave these lines intact!</p> <p>Check if any of the PRs in the release contain any notices or are particularly noteworthy.</p> <p>We advocate for highlighting some or more of the following, as part of the manually written header:</p> <ul> <li>big features</li> <li>big fixes</li> <li>contributor recognition</li> <li>interface changes</li> </ul> <p>See the appendix below for ideas.</p> <p>Symbol links from the Release</p> <p>If the enqueued documentation build in docs.rs done, then it is possible to link to them in the release.</p> <p>docs.rs build time</p> <p>The docs.rs build usually completes in less than <code>30m</code> after publishing, but we have seen it take around half a day when publishing during an anticipated Rust release.</p> <p>A release is more than just a <code>git tag</code>, it should be something to celebrate, for the maintainers, the contributors, and the community.</p> <p>Of course, not every release is going to be noteworthy. For these cases, it's perfectly OK to just hit Publish without much ceremony.</p>"},{"location":"release-process/#completion-steps","title":"Completion Steps","text":"<ul> <li>Press Publish on the GitHub Release</li> <li>Run <code>./scripts/release-afterdoc.sh NEW_VERSION</code> to port the changed release notes into the <code>CHANGELOG.md</code> and push to <code>main</code></li> <li>Run <code>./sync.sh</code> in the website repo to pull the new release notes to kube.rs/changelog</li> </ul>"},{"location":"release-process/#appendix","title":"Appendix","text":""},{"location":"release-process/#header-formatting-tips","title":"Header Formatting Tips","text":"<p>Some example release notes from recent history has some ideas:</p> <ul> <li>0.68.0</li> <li>0.66.0</li> </ul> <p>Note that headers should link to PRs/important documents, but it is not necessary to link into the release or the milestone in this document yourself (the <code>afterdoc</code> step automates this).</p> <p>For breaking changes; consider including migration code samples for users if it provides an easier way to understand the changes. Fenced code blocks with <code>diff</code> language are easy to scan:</p> <pre><code>-async fn reconcile(myobj: MyK, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction&gt;\n+async fn reconcile(myobj: Arc&lt;MyK&gt;, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;ReconcilerAction&gt;\n</code></pre> <p>New features should link to the new additions under docs.rs/kube once the documentation build completes.</p> <p>...</p>"},{"location":"rust-version/","title":"Rust Version","text":"<p>Builds on CI always run against the most recent stable rust version, but we also support stable versions down to a specified Minimum Supported Rust Version (MSRV).</p>"},{"location":"rust-version/#minimum-supported-rust-version","title":"Minimum Supported Rust Version","text":"<p>The MSRV is shown in the main Cargo.toml and as a readme badge:</p> <p></p> <p>Our MSRV policy is to try to let our MSRV trail 2 stable versions behind the latest stable as a convenience to users downstream.</p> <p>Best Effort Policy</p> <p>Note that this policy, while sometimes more lenient than 2 versions, it is also not guaranteed to hold due to dependencies we have to upgrade. The version shown should be taken as best effort and descriptive, and it is verified as buildable by CI.</p>"},{"location":"rust-version/#for-maintainers-bumping-the-msrv","title":"For maintainers: Bumping the MSRV","text":"<p>Bumping the MSRV is done either as:</p> <ul> <li>a response to a PR that brings in a dependency with a higher MSRV</li> <li>an explicit choice to get new rust features</li> </ul> <p>Performing the change requires developer tools, and is done by running <code>just bump-msrv 1.60.0</code> to bump the all the <code>Cargo.toml</code> files:</p> <pre><code>-rust-version = \"1.56.0\"\n+rust-version = \"1.60.0\"\n</code></pre> <p>as well as the badge plus devcontainer. If the bump is sufficient, CI will verify that the specified MSRV works with the current dependencies and feature usage before it can be merged.</p> <p>NB: An MSRV change must have the <code>changelog-change</code> label so that it is sufficiently highlighted in our changelog.</p>"},{"location":"rust-version/#for-contributors-nightly-tooling","title":"For contributors: Nightly tooling","text":"<p>Due to some limitations in stable <code>rustdoc</code> and <code>rustfmt</code>, we use the <code>nightly</code> toolchain for auto-formatting and documentation.</p> <p>NB: This is contributor quirk only. All crates will always build with the stable toolchain.</p> <p>CI always runs documentation and formatting builds against the latest <code>nightly</code>, but in general, the relevant features change very infrequently, so it's generally safe to rely on older nightly builds unless CI complains.</p> <pre><code>rustup update nightly\n</code></pre>"},{"location":"security/","title":"Security","text":""},{"location":"security/#security-policy","title":"Security Policy","text":""},{"location":"security/#supported-versions","title":"Supported Versions","text":"<p>We provide security updates for the two most recent minor versions released on <code>crates.io</code>.</p> <p>For example, if <code>0.70.1</code> is the most recent stable version, we will address security updates for <code>0.69</code> and later. Once <code>0.71.1</code> is released, we will no longer provide updates for <code>0.69</code> releases.</p>"},{"location":"security/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>To report a security problem in Kube-rs, please contact at least two maintainers.</p> <p>These people will help diagnose the severity of the issue and determine how to address the issue. Issues deemed to be non-critical will be filed as GitHub issues. Critical issues will receive immediate attention and be fixed as quickly as possible.</p>"},{"location":"security/#security-advisories","title":"Security Advisories","text":"<p>When serious security problems in Kube-rs are discovered and corrected, we issue a security advisory, describing the problem and containing a pointer to the fix.</p> <p>These are announced the RustSec Advisory Database, to our github issues under the label <code>critical</code>, as well as discord and other primary communication channels.</p> <p>Security issues are fixed as soon as possible, and the fixes are propagated to the stable branches as fast as possible. However, when a vulnerability is found during a code audit, or when several other issues are likely to be spotted and fixed in the near future, the security team may delay the release of a Security Advisory, so that one unique, comprehensive Security Advisory covering several vulnerabilities can be issued. Communication with vendors and other distributions shipping the same code may also cause these delays.</p>"},{"location":"stability/","title":"Stability","text":"<p><code>kube</code> satisfies the client level requirements for a Stable Client.</p>"},{"location":"stability/#platform-support-level","title":"Platform Support Level","text":"<p>Our support level is determined by our continuous integration.</p> <p>Github Actions continually builds and tests <code>kube</code> against the LTS supported environments for Ubuntu, and macOS, and Windows:</p> Support Source Guarantee Upstream Policy  supported Linux <code>ubuntu-latest</code> 2 years  supported Windows <code>windows-latest</code> 3 years  supported macOS <code>macos-latest</code> 1 year"},{"location":"stability/#kubernetes-distribution-support","title":"Kubernetes Distribution Support","text":"<p>We follow upstream api-conventions and is designed to work for any Kubernetes conformant distribution.</p> <p>Apart from a single upstream deprecated auth plugin for GCP (that we maintain compatibility for), all <code>kube</code> logic is otherwise distribution agnostic.</p> <p>For version compatibility against <code>EKS</code>, <code>GKE</code>, <code>AKS</code>, you may cross-reference with our kubernetes-version policy (designed to match their lifecycles).</p>"},{"location":"stability/#interface-changes","title":"Interface Changes","text":"<p>Public interfaces from <code>kube</code> is allowed to change between semver breaking versions, provided github release &amp; changelog provides adequate guidance on the change, and the amount of user facing changes is minimized and trivialised. In particular:</p> <ul> <li>PRs that perform breaking changes must have the <code>changelog-change</code> label</li> <li>changes needed to user code should have a diff code comment on the change</li> <li>changes as a result of interface changes should be explained</li> <li>changes that affect controller-rs or version-rs, should link to a fixing commit</li> <li>renamed functions/changed arguments/changing imports, should show what to search/replace in the PR</li> <li>changes to experimental features using #unstable-features should have an <code>unstable</code> label</li> </ul> <p>We prefer deprecations and duplication of logic where it avoids confusion.</p>"},{"location":"stability/#deprecation-strategy","title":"Deprecation Strategy","text":"<p>Altered methods/fns/constants should in general not be changed directly, but instead have a new alternative implementation introduced to avoid users receiving confusing compile errors.</p> <p>New variants should have a new name, and the old variant should remain with a deprecation attribute that can guide users towards the new behaviour before the deprecated variant disappears.</p> <p>Deprecation Duration</p> <p>Deprecated functionality must stick around for at least 3 major releases.</p> <p>For instance, if we deprecate a method in <code>1.0.0</code> (the major released for Kubernetes 1.33), then the deprecated method must still exist in future <code>1.X</code> releases as well as <code>2.X</code> (the major release for Kubernetes 1.34) and <code>3.X</code> (the major release for Kubernetes 1.35). We should only remove this method for a <code>4.0</code> release.</p> <p>Deprecations should use a deprecated attribute with a note of removal. Ala <code>runtime::utils::try_flatten_applied</code> which was slated for removal in <code>0.75.0</code>.</p> <pre><code>/// Flattens each item in the list following the rules of [`watcher::Event::into_iter_applied`].\n#[deprecated(\n    since = \"0.72.0\",\n    note = \"fn replaced with the WatchStreamExt::applied_objects which can be chained onto watcher. Add `use kube::runtime::WatchStreamExt;` and call `stream.applied_objects()` instead. This function will be removed in 0.75.0.\"\n)]\npub fn try_flatten_applied&lt;K, S: TryStream&lt;Ok = watcher::Event&lt;K&gt;&gt;&gt;(\n    stream: S,\n) -&gt; impl Stream&lt;Item = Result&lt;K, S::Error&gt;&gt; {\n    stream\n        .map_ok(|event| stream::iter(event.into_iter_applied().map(Ok)))\n        .try_flatten()\n}\n</code></pre> <p>Internal usage of this function was removed when it was deprecated, but it is kept in place as a convenience to people upgrading for at least 3 versions.</p> <p>The deprecation note must point out viable alternatives, and must point out the scheduled removal version.</p>"},{"location":"stability/#unstable-features","title":"Unstable Features","text":"<p>When adding new, experimental functionality, we export them through optional features that must be explicitly enabled.</p> <p>These features are opt-in using cargo features, and can be enabled from your <code>Cargo.toml</code>:</p> <pre><code>kube = { version = \"1\", features = [\"runtime\", \"unstable-runtime\"] }\n</code></pre> <p>Functionality released under unstable features can change between any version and are not subject to any of the usual guarantees.</p>"},{"location":"stability/#feature-selection-vs-compile-flags","title":"Feature Selection vs. Compile Flags","text":"<p>We acknowledge that using features selection for unstable functionality does come with the downside that users can sometime have these features selected for them through intermediary crates that also depend on kube.</p> <p>Tokio for instance, instead relies on compile flags due to this possibility, even though it is slightly more awkward for users.</p> <p>For <code>kube</code>, our position is more frequently a direct application dependency rather than as another library building-block. Thus, we feel the possibility of unstable features being accidentally enabled for end-users to be low enough to not warrant the extra hassle of using compile flags.</p> <p>That said, library publishers that depend on <code>kube</code> should only publish unstable kube behavior under their own unstable feature sets, or in pre-1.0 crates.</p>"},{"location":"stability/#feature-exporting","title":"Feature Exporting","text":"<p>Features are exported by each crate initially;</p> <p>E.g. from <code>kube-runtime</code>:</p> <pre><code>kube-runtime/Cargo.toml\n18:unstable-runtime = [\"unstable-runtime-subscribe\"]\n19:unstable-runtime-subscribe = []\n</code></pre> <p>then the major feature is re-exported from <code>kube</code>:</p> <pre><code>kube/Cargo.toml\n31:unstable-runtime = [\"kube-runtime/unstable-runtime\"]\n</code></pre>"},{"location":"stability/#major-release-cycle","title":"Major Release Cycle","text":"<p>Our codebase has generally stabilised, we officially satisfy the upstream requirements for stability, and we have started releasing major versions. However, we still have sub-1.0 dependencies. How does this work?</p> <ol> <li>Any future minor bump of sub-1.0 dependencies that have a public/peer boundary (such as k8s-openapi, schemars) will coincide with a new <code>kube</code> major version.</li> <li>Every 3 months there's a new Kubernetes version resulting in a new semver breaking <code>k8s-openapi</code>.</li> </ol> <p>Major Release Policy</p> <p>To align breaking changes to a predictable cycle, we aim to ship these semver breaking upgrades for new Kubernetes versions as a major kube version. Additional care to limit any non-mandatory, internal breaking changes still follow our normal guidelines above.</p> <p>This means that occasionally we are behind on some unstable dependencies, but this should be rectified in the new version. Any relevant required upgrades will be listed in the version table.</p> <p>We feel this currently best reflects the state of reality; <code>kube</code> mirrors a large part of the api surface of Kubernetes, so our major releases should correspond to large changes in Kubernetes.</p> <p>Currently this means having to do semver breaking releases, but this might not remain true forever.</p>"},{"location":"stability/#future-features","title":"Future Features","text":"<p>There are also some features that we have wanted to get properly in place, but they are so far proving elusive. These are</p> <ol> <li>protobuf serialization layer is WIP    a). This is likely to force new features (possibly making <code>k8s-openapi</code> opt-in)    b). This might require core traits to be moved out of <code>kube-core</code></li> <li>Client Api Methods is WIP    a). This might require some re-work of the dynamic api    b). This might change how we advertise how users should use our kube-client (though we are unlikely to ever remove <code>Api</code>)</li> <li><code>Controller</code> runtime features for stream sharing are WIP    a). This is being tested out through unstable features    b). Controller signatures might need tweaking to accommodate these</li> </ol> <p>If/when these land, they are likely to live under unstable features for a while, and only properly introduced as breaking changes in major versions.</p>"},{"location":"stability/#summary","title":"Summary","text":"<p>As a brief summary to these policies and constraints, our approach to stability is to:</p> <ul> <li>track our breaking changes with changelog-change labeled PRs</li> <li>inform on necessary changes in releases and the changelog</li> <li>carefully change according to our guidelines on interface changes</li> <li>experiment with new functionality under unstable feature flags</li> <li>deprecate according to our deprecation strategy</li> </ul>"},{"location":"tools/","title":"Tools","text":"<p>All repositories under kube-rs are buildable using FLOSS tools, and they are listed herein.</p>"},{"location":"tools/#user-dependencies","title":"User Dependencies","text":"<p>Dependencies a user needs to use kube-rs.</p> <ul> <li>Rust</li> <li>various crates satisfying our license allowlist</li> </ul>"},{"location":"tools/#development-dependencies","title":"Development Dependencies","text":"<p>Dependencies a developer might find helpful to develop on kube-rs.</p>"},{"location":"tools/#build-dependencies","title":"Build Dependencies","text":"<p>CLIs that are used for occasional build or release time manipulation. Maintainers need these.</p> <ul> <li>fd</li> <li>jq</li> <li>just</li> <li>sd</li> <li>rg</li> <li>choose</li> <li>fastmod</li> <li>curl</li> <li>cargo-release</li> </ul> <p>GNU tools like <code>grep</code> + <code>head</code> + <code>tail</code> + <code>awk</code> + <code>sed</code>, are also referenced a handful of times, but are generally avoided due to cross-platform compatibility issues, plus the existence of more modern tools above.</p>"},{"location":"tools/#ci-dependencies","title":"CI Dependencies","text":"<ul> <li>cargo-audit</li> <li>cargo-deny</li> <li>cargo-msrv</li> <li>cargo-tarpaulin</li> </ul>"},{"location":"tools/#integration-tests","title":"Integration Tests","text":"<p>CLIs that are used in integration tests, or referenced as ways recommended to test locally.</p> <ul> <li>k3d</li> <li>tilt</li> <li>docker/cli</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Problems with Api commands failing is often RBAC, misspelled names (used for url construction) and can be usually be identified via error codes and logs. Some common problems and solutions are explored herein.</p> <p>Logs are a prerequisite</p> <p>See observability#adding-logs for how to setup tracing subscribers properly (env_logger works also).</p>"},{"location":"troubleshooting/#request-inspection","title":"Request Inspection","text":"<p>If you are replicating <code>kubectl</code> behaviour, then you can cross-reference with logs.</p> <p>Given an example alpine pod, we will run <code>exec</code> on a shell loop, and use <code>-v=9</code> to look for a <code>curl</code> expression in a large (and abbreviated) amount of debug output to see what we actually tell the apiserver to do:</p> <pre><code>$ kubectl exec example -it -v=9 -- sh -c 'for i in $(seq 1 3); do date; done'\n\nround_trippers.go:466] curl -v -XPOST -H \"X-Stream-Protocol-Version: v4.channel.k8s.io\" \\\n  'https://0.0.0.0:64262/api/v1/namespaces/kube-system/pods/example/exec?command=sh&amp;command=-c&amp;command=for+i+in+%24%28seq+1+3%29%3B+do+date%3B+done&amp;container=example&amp;stdin=true&amp;stdout=true&amp;tty=true'\n</code></pre> <p>This url and query parameters can be cross-referenced in the logs from <code>kube_client</code>.</p> <p>A very similar call is here being done from the <code>pod_exec</code> example, and when running with <code>RUST_LOG=debug</code> we can find a \"requesting\" debug line with the url used:</p> <pre><code>$ RUST_LOG=debug cargo run --example pod_exec\nDEBUG HTTP{http.method=GET http.url=https://0.0.0.0:64262/api/v1/namespaces/kube-system/pods/example/exec?&amp;stdout=true&amp;command=sh&amp;command=-c&amp;command=for+i+in+%24%28seq+1+3%29%3B+do+date%3B+done otel.name=\"exec\" otel.kind=\"client\"}: kube_client::client::builder: requesting\n</code></pre> <p>Then we can investigate whether our query parameters matches what is expected (in this case stream differences and tty differences).</p>"},{"location":"troubleshooting/#access","title":"Access","text":"<p>Access issues is a result of misconfigured access or lacking manifests#RBAC and will bubble up as a kube::Error::Api where the underlying error code will contain a <code>403</code>.</p> <p>In print, they look something like this:</p> <pre><code>ErrorResponse { status: \"Failure\", message: \"documents.kube.rs \\\"samuel\\\" is forbidden: User \\\"system:serviceaccount:default:doc-controller\\\" cannot patch resource \\\"documents\\\" in API group \\\"kube.rs\\\" in the namespace \\\"default\\\"\", reason: \"Forbidden\", code: 403 }\n</code></pre> <p>And they should be visible directly provided you are actully printing your error objects somewhere (rather than discarding them).</p> <p>If you turn up logging to <code>RUST_LOG=kube=debug</code> you should also see most errors internally.</p>"},{"location":"troubleshooting/#watcher-errors","title":"Watcher Errors","text":"<p>A watcher will expose watcher::Error as the error part of it's <code>Stream</code> items. If these errors are discarded, it might lead to a continuously failing and retrying program.</p> <p>Watcher errors are soft errors</p> <p>A watcher will retry on all failures (including 403s, network failures) because the watcher can recover if external circumstances improve (for instance by an admin tweaking a <code>Role</code> object, or the network improving). These errors are therefore often optimistically ignored, but they should never be silently ignored.</p> <p>When matching on items from the stream and printing the errors, the errors can look like:</p> <pre><code>WatchFailed(Api(ErrorResponse { status: \"Failure\", message: \"ListOptions.meta.k8s.io \\\"\\\" is invalid: resourceVersionMatch: Forbidden: resourceVersionMatch is forbidden for watch\", reason: \"Invalid\", code: 422 }))\n</code></pre> <p>If you are not printing the watcher errors yourself, you can get them via logs from <code>kube_runtime</code> (available with <code>RUST_LOG=warn</code> for the most common errors like RBAC, or <code>RUST_LOG=kube=debug</code> for the more obscure errors). It will look something like this:</p> <pre><code>WARN kube_runtime::watcher: watcher error 403: Api(ErrorResponse { status: \"Failure\", message: \"documents.kube.rs is forbidden: User \\\"system:serviceaccount:default:doc-controller\\\" cannot watch resource \\\"documents\\\" in API group \\\"kube.rs\\\" at the cluster scope\", reason: \"Forbidden\", code: 403 })\n</code></pre>"},{"location":"troubleshooting/#stream-errors","title":"Stream Errors","text":"<p>Because of the soft-error policy on stream errors, it's useful to consider what to do with errors in general from infinite streams.</p> <p>The easiest error handling setup is to tear down the application on any errors by (say) passing stream errors through a <code>try_for_each</code> (ala pod_watcher) or a <code>try_next</code> loop (ala event_watcher).</p> <p>Crashing in-cluster</p> <p>If you are deployed in-cluster, don't be afraid to exit(1)/crash early on errors you don't expect. Exits are easier to handle than a badly running app in a confusing state. By crashing, you get retry with backoff for free, plus you often get alerts such as KubePodCrashLooping triggering (without instrumentation needed).</p> <p>While easy, early exits is not the best solution;</p> <ul> <li>Locally, having a CLI abruptly exit is a bad user experience.</li> <li>In-cluster, frequent restarts of a large app with many spurious non-fatal conditions can mask underlying problems.</li> <li>early exits throw cancel-safety and state transaction concerns out the window</li> </ul> <p>For controllers with multiple watchers, observability#Adding Metrics is instead customary, so that you can alert on percentage error rates over a time span (telling the operator to go look at logs for why).</p> <p>It is also common to check for blocker errors up-front before starting an infinite watch stream;</p> <ol> <li>did you install the crd before trying to start a watcher? do a naked list first as a sanity:</li> </ol> <pre><code>if let Err(e) = docs.list(&amp;ListParams::default().limit(1)).await {\n    error!(\"CRD is not queryable; {e:?}. Is the CRD installed?\");\n    info!(\"Installation: cargo run --bin crdgen | kubectl apply -f -\");\n    std::process::exit(1);\n}\nwatcher(docs, conf).try_for_each(|_| future::ready(Ok(()))).await?;\n</code></pre> <p>This is a particularly common error case since CRD installation is often managed out-of-band with the application and thus often neglected.</p>"},{"location":"upgrading/","title":"Upgrading","text":"<p>You can upgrade <code>kube</code> using normal Rust methods to upgrade, as long as you stick to semver compatible versions of <code>k8s-openapi</code> and <code>schemars</code> (if using custom resource derive).</p> <p>Peer dependencies</p> <p><code>kube</code> depends on a handful of unstable crates at certain versions, but new versions of these crates have not always propagated into kube. To avoid build issues, <code>k8s-openapi</code> and <code>schemars</code> must NOT exist at multiple semver incompatible versions in your dependency tree.</p> <p>We recommend you bump <code>kube</code>, <code>k8s-openapi</code> and <code>schemars</code> crates at the same time to avoid build issues.</p> <p>Consider bumping the kubernetes-version feature pin on <code>k8s-openapi</code> unless you are using its <code>latest</code> feature.</p>"},{"location":"upgrading/#command-line","title":"Command Line","text":"<p>Using <code>cargo upgrade</code> via cargo-edit:</p> <pre><code>cargo upgrade -p kube -p k8s-openapi -p schemars -i\n</code></pre> <p>Schemars 1</p> <p>The major version of schemars 1 is expected to be released in the next Kubernetes version (1.34) in kube 2.0. Until then, if you need schemars 1, pinning to git is an option. Schemars 1 will not work with kube 1.</p>"},{"location":"upgrading/#dependabot","title":"Dependabot","text":"<p>Configure the <code>cargo</code> ecosystem on dependabot and group codependent crates:</p> <pre><code>  - package-ecosystem: \"cargo\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    groups:\n      kube:\n        patterns:\n          - kube\n          - k8s-openapi\n          - schemars\n</code></pre>"},{"location":"upgrading/#renovate","title":"Renovate","text":"<p>Add package rules for Kubernetes crates that match on prefixes:</p> <pre><code>packageRules: [\n        {\n            matchPackagePrefixes: [\n                \"kube\",\n                \"k8s\",\n                \"schemars\",\n            ],\n            groupName: \"kubernetes crates\",\n            matchManagers: [\n                \"cargo\"\n            ],\n        }\n]\n</code></pre>"},{"location":"website/","title":"Website","text":"<p>This website is hosted on kube.rs via github pages from the kube-rs/website repo, and accepts contributions in the form of pull requests to change the markdown files that generate the website.</p>"},{"location":"website/#structure","title":"Structure","text":"<p>The docs folder contains all the resources that's inlined on the webpage and can be edited on this page using any editor.</p> <p>It is recommended having markdown preview, wikilink, and the foam extension, but what is rendered is ultimately just generated from markdown with wikilinks.</p>"},{"location":"website/#synchronization","title":"Synchronization","text":"<p>A subset of markdown documents show up in certain paths of the github contribution process and must remain in these original repos.</p> <p>Synchronized markdown documents will be overwritten if edited herein!</p> <p>Notice the first line of these files contain a line like the following:</p> <pre><code>&lt;!--GENERATED FROM\nhttps://github.com/kube-rs/kube/blob/main/CONTRIBUTING.md\nCHANGES MUST BE MADE THERE --&gt;\n</code></pre> <p>These files must be edited upstream at the given path, and will be synchronized to this site on the next kube release or sooner.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/06/11/watcher-memory-improvements/","title":"Watcher Memory Improvements","text":"<p>In 0.92.0, the watcher dropped its internal buffering of state and started to fully delegating any potential buffering to the associated Store.</p> <p>This can cause a decent memory use reduction for direct users of watcher, but also (somewhat unintuitively) for users of reflectors and stores.</p> <p>In this post, we explore the setup, current solutions, and some future work. It has been updated in light of 0.92.1.</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#runtime-memory-performance","title":"Runtime Memory Performance","text":"<p>The memory profile of any application using <code>kube::runtime</code> is often dominated by the memory usage from buffers of the Kubernetes objects that is needed to be watched. The main offender is the reflector, with a literal <code>type Cache&lt;K&gt; = Arc&lt;RwLock&lt;AHashMap&lt;ObjectRef&lt;K&gt;, Arc&lt;K&gt;&gt;&gt;&gt;</code> hiding internally as the lookup used by Stores and Controllers.</p> <p>We have lots of advice on how to reduce the size of this cache. The optimization guide shows how to:</p> <ul> <li>minimize what you watch :: by constraining watch parameters with selectors</li> <li>minimize what you ask for :: use metadata_watcher on watches that does not need the .spec</li> <li>minimize what you store :: by dropping fields before sending to stores</li> </ul> <p>These are quick and easy steps improve the memory profile that is worth checking out (the benefits of doing these will further increase in 0.92).</p> <p>Improving what is stored in the <code>Cache</code> above is important, but it is not the full picture...</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#the-watch-api","title":"The Watch API","text":"<p>The Kubernetes watch API is an interesting beast. You have no guarantees you'll get every event, and you must be able to restart from a potentially new checkpoint without telling you what changes happened in the downtime. This is mentioned briefly in Kubernetes API concepts as an implication of its <code>410 Gone</code> responses.</p> <p>When <code>410 Gone</code> responses happen we need to trigger a re-list, and wait for all data to come through before we are back in a \"live watching\" mode that is caught up with reality. This type of API consumption is problematic when you need to do work with reflectors/caches where you are generally storing complete snapshots in memory for a worker task. Controllers are effectively forced to treat every event as a potential change, and chase reconciler#idempotency as a work-around for not having guaranteed delivery.</p> <p>Let's focus on caches. To simplify these problems for users we have created certain guarantees in the abstractions of <code>kube::runtime</code>.</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#runtime-guarantees","title":"Runtime Guarantees","text":"<p>The watcher up until 0.92.0 has maintained a guarantee we have casually referred to as watcher atomicity:</p> <p>watcher atomicity &lt; 0.92.0</p> <p>You only see a <code>Restarted</code> on re-lists once every object has been received through an <code>api.list</code>. Watcher events will pause between a de-sync / restart and a <code>Restarted</code>. See watcher::Event@0.91.</p> <p>This property meant that stores could in-turn provide their own guarantee very easily:</p> <p>Store completeness</p> <p>Store always presents the full state once initialised. During a relist, previous state is presented. There is no down-time for a store during relists, and its <code>Cache</code> is replaced atomically in a single locked step.</p> <p>This property is needed for Controllers who rely on complete information and will kick in once the future from Store::wait_until_ready resolves.</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#store-consequences","title":"Store Consequences","text":"<p>If we do all the buffering on the <code>watcher</code> side, then achieving the store completeness guarantee is a rather trivial task to accomplish.</p> <p>Up until 0.91 this was handled in <code>Store::apply_watcher_event@0.91</code> as with a <code>*self.store.write() = new_objs</code> on the old <code>Restarted</code> event:</p> <pre><code>// 0.91 source:\n        match event {\n            watcher::Event::Applied(obj) =&gt; {\n                let key = obj.to_object_ref(self.dyntype.clone());\n                let obj = Arc::new(obj.clone());\n                self.store.write().insert(key, obj);\n            }\n            watcher::Event::Deleted(obj) =&gt; {\n                let key = obj.to_object_ref(self.dyntype.clone());\n                self.store.write().remove(&amp;key);\n            }\n            watcher::Event::Restarted(new_objs) =&gt; {\n                let new_objs = new_objs\n                    .iter()\n                    .map(|obj| (obj.to_object_ref(self.dyntype.clone()), Arc::new(obj.clone())))\n                    .collect::&lt;AHashMap&lt;_, _&gt;&gt;();\n                *self.store.write() = new_objs;\n            }\n        }\n</code></pre> <p>Thus, on a relist/restart:</p> <ol> <li>watcher pages were buffered internally</li> <li>entered <code>Restarted</code> arm, where each object got cloned while creating <code>new_objs</code></li> <li>store (containing the complete old data) swapped at the very end</li> </ol> <p>so you have a moment with 3x potential peak memory use (2x should have been the max).</p> <p>On top of that, the buffer in the <code>watcher</code> was not always released (quote from discord):</p> <p>The default system allocator never returns the memory to the OS after the burst, even if the objects are dropped. Since the initial list fetch happens sporadically you get a higher RSS usage together with the memory spike. Solving the burst will solve this problem, and reflectors and watchers can be started in parallel without worrying of OOM killers. The allocator does not return the memory to the OS since it treats it as a cache. This is mitigated by using jemalloc with some tuning, however, you still get the memory burst so our solution was to use jemalloc + start the watchers sequentially. As you can imagine it's not ideal.</p> <p>So in the end you might actually be holding on to between 2x and 3x the actual store size at all times.</p> <p>watcher guarantee was designed for the store guarantee</p> <p>If you were using <code>watcher</code> without <code>reflector</code>, you were the most affected by this excessive caching. You might not have needed watcher atomicity, as it was primarily designed to facilitate store completeness.</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#watcher-consequences","title":"Watcher Consequences","text":"<p>If you were just watching data on 0.91.0 (not using stores), the buffering is completely necessary if you just want to react to events about individual objects without considering the wider dataset.</p> <p>Your peak memory use for a single watcher (with all other things considered negligible) is going to scale with the size of the complete dataset because <code>watcher</code> was buffering ALL pages, whereas it really should only scale with the page size that you ask for objects returned in.</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#change-in-092","title":"Change in 0.92","text":"<p>The change in 0.92.0 is primarily to stop buffering events in the <code>watcher</code>, and present new watcher events that allows a store to achieve the Store completeness guarantee.</p> <p>As it stands the <code>Store::apply_watcher_event@0.92</code> now is slightly smarter and achieves the same guarantee:</p> <pre><code>// 0.92 source\n        match event {\n            watcher::Event::Apply(obj) =&gt; {\n                let key = obj.to_object_ref(self.dyntype.clone());\n                let obj = Arc::new(obj.clone());\n                self.store.write().insert(key, obj);\n            }\n            watcher::Event::Delete(obj) =&gt; {\n                let key = obj.to_object_ref(self.dyntype.clone());\n                self.store.write().remove(&amp;key);\n            }\n            watcher::Event::Init =&gt; {\n                self.buffer = AHashMap::new();\n            }\n            watcher::Event::InitApply(obj) =&gt; {\n                let key = obj.to_object_ref(self.dyntype.clone());\n                let obj = Arc::new(obj.clone());\n                self.buffer.insert(key, obj);\n            }\n            watcher::Event::InitDone =&gt; {\n                let mut store = self.store.write();\n                std::mem::swap(&amp;mut *store, &amp;mut self.buffer);\n                self.buffer = AHashMap::new();\n                /// ...\n            }\n        }\n</code></pre> <p>Thus, on a restart, objects are passed one-by-one up to the store, and buffered therein. When all objects are received, the buffers are swapped (meaning you use at most 2x the data). The blank buffer re-assignment also forces de-allocation* of the temporary <code>self.buffer</code>.</p> <p>Preparing for StreamingLists</p> <p>Note that the new partial <code>InitApply</code> event only pass up individual objects, not pages. This is to prepare for the 1.27 Alpha StreamingLists feature which also passed individual events. Once this becomes available for even our minimum kubernetes-version we can make this the default - reducing page buffers further - exposing the literal api results rather than pages (of default 500 objects). In the mean time, we send pages through item-by-item to avoid a breaking change in the future (and also to avoid exposing the confusing concept of flattened/unflattened streams).</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#results","title":"Results","text":"<p>The initial synthetic benchmarks saw 60% reductions when using stores, and 80% when not using stores (when there's nothing to cache), with further incremental improvements when using the <code>StreamingList</code> strategy.</p> <p>Ad-hoc Benchmarks</p> <p>The ad-hoc synthetic benchmarks are likely unrealistic for real world scenarios. The original 0.92.0 release had a bug affecting benchmarks, so many of the linked posts may be invalid / out-of-date. How much you can get out of this will depend on a range of factors from allocator choice to usage patterns.</p> <p>So far, we have seen controllers with a basically unchanged profile, some with small improvements in the 10-20% range, one 50% drop in a real-world controller from testing (EDIT: which is still sustained after the 0.92.1 bugfix with page size).</p> <p>In the current default <code>ListWatch</code> InitialListStrategy, the implicit default Config::page_size of <code>500</code> will undermine this optimization somewhat, because individual pages are still kept in the watcher while they are being sent out one-by-one. Setting the page size to <code>50</code> was necessary for me to get anything close to the benchmarks.</p> <p>Page Size Marginal Gains</p> <p>Lowering the page size below 50 did see further marginal gains (~5%ish from limited testing), but this will also increase API calls (/list next page). It will be interesting to see how well the streaming lists change will fare in the end (as it effectively functions as setting the page size to 1 as far as internal buffering is concerned).</p> <p>So for now; YMMV. Try setting the <code>page_size</code>, and chat about / share your results!</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#examples","title":"Examples","text":"<p>Two example results from my own deployment testing (by checking memory use after initialisation 5m in using standard kubelet metrics) showed uneven gains.</p> <p>Update after 0.92.1</p> <p>This post has been edited in light of 0.92.1 which casted the 0.92.0 release in overly favourable light. 0.92.0 dropped pages and this impacted the measurements.</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#optimized-metadata-controller","title":"Optimized Metadata Controller","text":"<p>A metadata controller watching 2000 objects (all in stores), doing 6000 reconciles an hour.</p> <p>45MB memory on 0.91.0, ~20MB on 0.92.1.</p> <p>This saw the biggest improvement, dropping ~50% of its memory usage. This is also a tiny controller with basically no other cached data though, it is doing all the biggest optimization tricks (<code>metadata_watcher</code>, page_size 50, pruning of managed fields) so the page buffering actually constituted the majority of the memory use (a perhaps uncommon situation).</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#ks-controller","title":"KS Controller","text":"<p>A controller for flux kustomizations storing and reconciling about 200 <code>ks</code> objects without any significant optimization techniques.</p> <p>~65MB memory on 0.91.0, ~65MB on 0.92.1.</p> <p>No improvements overall on this one despite setting page size down to 50.</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#thoughts-for-the-future","title":"Thoughts for the future","text":"<p>The peak 2x overhead here does hint at a potential future optimization; allowing users to opt-out of the store completeness guarantee.</p> <p>Store Tradeoffs</p> <p>It is possibly to build custom stores that avoids the buffering of objects on restarts by dropping the store completeness guarantee. This is not practical yet for <code>Controller</code> uses, due to requirements on <code>Store</code> types, but perhaps this could be made generic/opt-out in the future. It could be a potential flattener of the peak usage.</p> <p>As a step in the right direction, we would first like to get better visibility of our memory profile with some automated benchmarking. See kube#1505 for details.</p> <p>It would also be good to better understand the choices of allocators here and their implications for some of these designs.</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#breaking-change","title":"Breaking Change","text":"<p>Users not matching on <code>watcher::Event</code> or building custom stores should not ever need to interact with this and should get the memory improvements for free.</p> <p>If you are using a custom store please see the new watcher::Event and make the following changes in <code>match</code> arms:</p> <ul> <li><code>Applied</code> -&gt; <code>Apply</code></li> <li><code>Deleted</code> -&gt; <code>Delete</code></li> <li><code>Restarted</code> -&gt; change to <code>InitApply</code> with 2 new arms:<ul> <li>Create new arms for <code>Init</code> marking start (allocate a temporary buffer)</li> <li>buffer objects from <code>InitApply</code> (you get one object at a time, no need to loop)</li> <li>Swap store in <code>InitDone</code> and deallocate the old buffer</li> </ul> </li> </ul> <p>See the above <code>Store::apply_watcher_event</code> code for pointers.</p>"},{"location":"blog/2024/06/11/watcher-memory-improvements/#previous-improvements","title":"Previous Improvements","text":"<p>Memory optimization is a continuing saga and while the numbers herein are considerable, they build upon previous work:</p> <ol> <li>Metadata API support in 0.79.0</li> <li>Ability to pass minified streams into <code>Controller</code> in 0.81.0 documented in streams</li> <li><code>Controller::owns</code> relation moved to lighter metadata watches in 0.84.0</li> <li>Default pagination of watchers in 0.84.0 via #1249</li> <li>initial streaming list support in 0.86.0</li> <li>Remove buffering in watcher in 0.92.0 - today \ud83c\udf89</li> </ol> <p>Thanks to everyone who contribute to <code>kube</code>!</p>"},{"location":"controllers/admission/","title":"Admission","text":"<p>This chapter talks about controlling admission through imperative or declarative validation:</p> <ul> <li>admission controllers / admission controller frameworks</li> <li>CRD validation with CEL</li> <li>admission policies</li> </ul>"},{"location":"controllers/admission/#validation-using-cel-validation","title":"Validation Using CEL Validation","text":"<p>CRDs (can be extended with validation rules written in CEL, with canonical examples on kubernetes.io crd validation-rules.</p> <pre><code>  openAPIV3Schema:\n    type: object\n    properties:\n      spec:\n        type: object\n        x-kubernetes-validations:\n          - rule: \"self.minReplicas &lt;= self.replicas\"\n            message: \"replicas should be greater than or equal to minReplicas.\"\n          - rule: \"self.replicas &lt;= self.maxReplicas\"\n            message: \"replicas should be smaller than or equal to maxReplicas.\"\n        properties:\n          ...\n          minReplicas:\n            type: integer\n          replicas:\n            type: integer\n          maxReplicas:\n            type: integer\n        required:\n          - minReplicas\n          - replicas\n          - maxReplicas\n</code></pre> <p>If your controller object is a CRD you own, then this is the recommended way to include validation because it is much less error prone than writing an admission controller. The feature is GA on new clusters, but otherwise generally available as Beta unless your cluster is EOL:</p> <p>Feature: CustomResourceValidationExpressions</p> <p>This requires Kubernetes &gt;=1.25 (where the feature is Beta), or Kubernetes &gt;= 1.29 (where the feature is GA).</p> <p>To include validation rules in the schemas you must add <code>x-kubernetes-validations</code> entries by schemas#overriding-members for the necessary types manually (or use the <code>x-kube</code> validation attribute). The manual way (which does not depend on <code>KubeSchema</code>):</p> <pre><code>fn string_legality(_: &amp;mut schemars::gen::SchemaGenerator) -&gt; schemars::schema::Schema {\n    serde_json::from_value(serde_json::json!({\n        \"type\": \"string\",\n        \"x-kubernetes-validations\": [{\n            \"rule\": \"self != 'illegal'\",\n            \"message\": \"string cannot be illegal\"\n        }]\n    }))\n    .unwrap()\n}\n</code></pre> <p>this fn can be attached with a <code>#[schemars(schema_with = \"string_legality)]</code> field attribute on some <code>Option&lt;String&gt;</code> (in this example). See #1372 too see interactions with errors and a larger struct and other validations.</p>"},{"location":"controllers/admission/#x_kube-validation","title":"<code>x_kube</code> validation","text":"<p>To reduce manual schema overriding for CRDs, the alternate KubeSchema derive macro can be used instead of JsonSchema.</p> <p><code>JsonSchema</code> vs <code>KubeSchema</code></p> <p>This macro generates <code>schemars</code> <code>JsonSchema</code> derive macro for the provided structure. Keep in mind that using the <code>KubeSchema</code> derive macro replaces <code>JsonSchema</code> derive, and specifying both will cause a conflict.*</p> <p>This implementation allows users to declaratively extend each field with validation rules (along with other <code>x-kubernetes</code> schema properties).</p>"},{"location":"controllers/admission/#x_kubevalidation---attribute","title":"<code>x_kube(validation = \u2026)</code> attribute","text":"<p>This attribute can be used to set one or more <code>validation</code> rules on a field. Rules can be created via one of; an explicit Rule / a string expression / a (string, reason) string pair;</p> <pre><code>#[derive(KubeSchema)]\npub struct FooSpec {\n    #[x_kube(\n         validation = Rule::new(\"self != 'illegal'\").message(Message::Expression(\"'string cannot be illegal'\".into())).reason(Reason::FieldValueForbidden),\n         validation = Rule::new(\"self != 'not legal'\").reason(Reason::FieldValueInvalid),\n         validation = \"self == expected\",\n         validation = (\"self == expected\", \"with error message\"),\n    )]\n    cel_validated: String,\n}\n</code></pre> <p>The <code>x_kube(validation = ...)</code> macro uses the Rule structure underneath. This can be constructed using the builder pattern, allowing users to extend the validation with a <code>message</code> to distinguish specific validation error and alternative reasons via <code>reason</code>, as well as a <code>field_path</code> for detailed json path to the invalid field value.</p> <p>Alternatively, the rule can be constructed implicitly from a string or a tuple of two strings. The first string represents the validation rule, and the second string is the message assigned to the rule.</p> <p>To write CEL expressions consider using the CEL playground. There are more examples in the CRD Validation Rules announcement blog and under kubernetes.io crd validation-rules.</p>"},{"location":"controllers/admission/#x_kubemerge_strategy---attribute","title":"<code>x_kube(merge_strategy = \u2026)</code> attribute","text":"<p>This <code>x-kubernetes</code> extension flag controls the Kubernetes SSA merge strategy from our MergeStrategy enum and facilitates the structuring of lists and maps in a specific format.</p> <pre><code>#[derive(KubeSchema)]\npub struct FooSpec {\n    #[x_kube(merge_strategy = ListType::Map(\"key\"))]\n    merge: Vec&lt;FooItem&gt;,\n}\n\npub struct FooItem {\n    key: String,\n    value: String,\n}\n</code></pre> <p>This example generates a <code>x-kubernetes-list-type=map</code> and <code>x-kubernetes-list-map-keys=[\"key\"]</code> attributes with the field. This instructs the API server to treat the underlying list as a map and ensures that <code>key</code> field is used as a unique key for the internal map, preventing conflicts on submission of duplicate keys by different managers.</p>"},{"location":"controllers/admission/#validation-using-webhooks","title":"Validation Using Webhooks","text":"<p>AKA writing an admission controller.</p> <p>These controllers run as webservers rather than the traditional controller loop, and are given an AdmissionReview containing an AdmissionRequest that you must inspect and decide whether to deny or accept.</p> <p>An admission controller can be Validating, Mutating or both.</p> <p>See the kube::core::admission module for how to set this up, or the example mutating admission_controller using warp.</p> <p>Admission controller management is hard</p> <p>Creating an admission webhook requires a non-trivial amount of certificate management for the webhookconfiguration, and come with its fair share of footguns (see e.g. Benefits and Dangers of Admission Controllers KubeCon'23).</p> <p>Consider CEL validation / CEL policies before writing an admission controllers.</p> <p>Two examples of admission controllers in rust using kube:</p> <ul> <li>kuberwarden-controller</li> <li>linkerd-policy-controller</li> </ul>"},{"location":"controllers/admission/#validation-using-policies","title":"Validation Using Policies","text":"<p>External or native objects (that you do not wish to validate at the CRD level), can be validated externally using a <code>ValidatingAdmissionPolicy</code>.</p> <p>These AdmissionPolicies let you inline CEL validation rules in an object similar to the WebhookConfiguration object for admission controllers, and tell Kubernetes to reject/accept for you based on simpler CEL expressions.</p> <p>Feature: ValidatingAdmissionPolicy</p> <p>This feature is available in Beta in 1.28. The talk Declarative Everything at KubeCon'23 shows the current status of the feature and its plans for mutation.</p>"},{"location":"controllers/admission/#validation-using-frameworks","title":"Validation Using Frameworks","text":"<p>If your use-case is company-wide security policies, then rather than writing an admission controller, or waiting for <code>AdmissionPolicy</code> to handle your case, consider the currently available major tooling for admission policies:</p> <ul> <li>Kyverno - policy list</li> <li>Kubewarden - oci installable policies</li> <li>OPA Gatekeeper - regu policies</li> </ul> <p>If you are creating validation for a CRD on the other hand, then it's less ideal to tie the validation to a particular framework as this can limit adoption of your operator.</p>"},{"location":"controllers/application/","title":"The Application","text":"<p>The application is a Rust application that manages a Controller. It needs a reconciler that will be called with entries of your chosen object, and a few dependencies to deal with async streams, error handling, and upstream Kubernetes structs.</p> <p>This document shows how to create a minimal application, with the built-in <code>Pod</code> type as the main object, and a no-op reconciler.</p>"},{"location":"controllers/application/#requirements","title":"Requirements","text":"<p>You need a newish version of stable Rust, and access to a Kubernetes cluster.</p>"},{"location":"controllers/application/#project-setup","title":"Project Setup","text":"<p>We create a new rust project:</p> <pre><code>cargo new --bin ctrl\ncd ctrl\n</code></pre> <p>add then install our dependencies:</p> <pre><code>cargo add kube --features=runtime,client,derive\ncargo add k8s-openapi --features=latest\ncargo add thiserror\ncargo add tokio --features=macros,rt-multi-thread\ncargo add futures\n</code></pre> <p>This will populate some <code>[dependencies]</code> in your <code>Cargo.toml</code> file.</p>"},{"location":"controllers/application/#dependencies","title":"Dependencies","text":"<ul> <li>kube :: with the controller <code>runtime</code>, Kubernetes <code>client</code> and a <code>derive</code> macro for custom resources</li> <li>k8s-openapi :: structs for core Kubernetes resources at the <code>latest</code> supported kubernetes-version</li> <li>thiserror :: typed error handling</li> <li>futures :: async rust abstractions</li> <li>tokio :: supported runtime for async rust features</li> </ul> <p>Additional dependencies are useful, but we will go through these later as we add more features.</p> <p>Alternate async runtimes</p> <p><code>kube</code> depends on tokio for its <code>time</code>, <code>signal</code> and <code>sync</code> features. Trying to swap to an alternate runtime is neither recommended, nor practical.</p>"},{"location":"controllers/application/#setting-up-errors","title":"Setting up errors","text":"<p>A full <code>Error</code> enum is the most versatile approach:</p> <pre><code>#[derive(thiserror::Error, Debug)]\npub enum Error {}\n\npub type Result&lt;T, E = Error&gt; = std::result::Result&lt;T, E&gt;;\n</code></pre>"},{"location":"controllers/application/#define-the-object","title":"Define the object","text":"<p>Create or import the object that you want to control into your <code>main.rs</code>.</p> <p>For the purposes of this demo we will import Pod:</p> <pre><code>use k8s_openapi::api::core::v1::Pod;\n</code></pre>"},{"location":"controllers/application/#setting-up-the-controller","title":"Setting up the controller","text":"<p>This is where we will start defining our <code>main</code> and glue everything together:</p> <pre><code>#[tokio::main]\nasync fn main() -&gt; Result&lt;(), kube::Error&gt; {\n    let client = Client::try_default().await?;\n    let pods = Api::&lt;Pod&gt;::all(client);\n\n    Controller::new(pods.clone(), Default::default())\n        .run(reconcile, error_policy, Arc::new(()))\n        .for_each(|_| futures::future::ready(()))\n        .await;\n\n    Ok(())\n}\n</code></pre> <p>This creates a Client, a Pod Api object (for all namespaces), and a Controller for the full list of pods defined by a default watcher::Config.</p> <p>We are not using relations here, this only schedules reconciliations when a pod changes.</p>"},{"location":"controllers/application/#creating-the-reconciler","title":"Creating the reconciler","text":"<p>You need to define at least a basic <code>reconcile</code> fn</p> <pre><code>async fn reconcile(obj: Arc&lt;Pod&gt;, ctx: Arc&lt;()&gt;) -&gt; Result&lt;Action&gt; {\n    println!(\"reconcile request: {}\", obj.name_any());\n    Ok(Action::requeue(Duration::from_secs(3600)))\n}\n</code></pre> <p>and an error handler to decide what to do when <code>reconcile</code> returns an <code>Err</code>:</p> <pre><code>fn error_policy(_object: Arc&lt;Pod&gt;, _err: &amp;Error, _ctx: Arc&lt;()&gt;) -&gt; Action {\n    Action::requeue(Duration::from_secs(5))\n}\n</code></pre> <p>To make this reconciler useful, we can reuse the one created in the reconciler document, on a custom object.</p>"},{"location":"controllers/application/#checkpoint","title":"Checkpoint","text":"<p>If you copy-pasted everything above and fixed imports, you should have a <code>main.rs</code> with this:</p> <pre><code>use std::{sync::Arc, time::Duration};\nuse futures::StreamExt;\nuse k8s_openapi::api::core::v1::Pod;\nuse kube::{\n    Api, Client, ResourceExt,\n    runtime::controller::{Action, Controller}\n};\n\n#[derive(thiserror::Error, Debug)]\npub enum Error {}\npub type Result&lt;T, E = Error&gt; = std::result::Result&lt;T, E&gt;;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), kube::Error&gt; {\n    let client = Client::try_default().await?;\n    let pods = Api::&lt;Pod&gt;::all(client);\n\n    Controller::new(pods.clone(), Default::default())\n        .run(reconcile, error_policy, Arc::new(()))\n        .for_each(|_| futures::future::ready(()))\n        .await;\n\n    Ok(())\n}\n\nasync fn reconcile(obj: Arc&lt;Pod&gt;, ctx: Arc&lt;()&gt;) -&gt; Result&lt;Action&gt; {\n    println!(\"reconcile request: {}\", obj.name_any());\n    Ok(Action::requeue(Duration::from_secs(3600)))\n}\n\nfn error_policy(_object: Arc&lt;Pod&gt;, _err: &amp;Error, _ctx: Arc&lt;()&gt;) -&gt; Action {\n    Action::requeue(Duration::from_secs(5))\n}\n</code></pre>"},{"location":"controllers/application/#developing","title":"Developing","text":"<p>At this point, you are ready <code>cargo run</code> the app and see if it works against a Kubernetes cluster.</p>"},{"location":"controllers/application/#cluster-setup","title":"Cluster Setup","text":"<p>If you already have a cluster, skip this part.</p> <p>We will develop locally against a <code>k3d</code> cluster (which requires <code>docker</code> and <code>kubectl</code>).</p> <p>Install the latest k3d release, then run:</p> <pre><code>k3d cluster create kube --servers 1 --agents 1 --registry-create kube\n</code></pre> <p>If you can run <code>kubectl get nodes</code> after this, you are good to go. See k3d/quick-start for help.</p>"},{"location":"controllers/application/#local-development","title":"Local Development","text":"<p>You should now be able to <code>cargo run</code> and check that you can successfully connect to your cluster.</p> <p>You should see an output like the following:</p> <pre><code>reconcile request: helm-install-traefik-pxnnd\nreconcile request: helm-install-traefik-crd-8z56p\nreconcile request: traefik-97b44b794-wj5ql\nreconcile request: svclb-traefik-5gmsm\nreconcile request: coredns-7448499f4d-72rvq\nreconcile request: metrics-server-86cbb8457f-8fct5\nreconcile request: local-path-provisioner-5ff76fc89d-4x86w\nreconcile request: svclb-traefik-q8zkw\n</code></pre> <p>Implying you get reconcile requests for every pod in your cluster (cross reference with <code>kubectl get pods --all</code>).</p> <p>If you now edit a pod (via <code>kubectl edit pod traefik-xxx</code> and make a change), or create a new pod, you should immediately get a reconcile request.</p> <p>Congratulations. You have technically built a kube controller.</p> <p>Where to Go From Here</p> <p>You have created the application using a trivial reconciler and a built-in object. See the object and reconciler chapters to change it into something more useful. The documents under Concepts on the left navigation menu shows the core concepts that are instrumental to help create the right abstraction.</p>"},{"location":"controllers/application/#useful-dependencies","title":"Useful Dependencies","text":"<p>The following dependencies are already used transitively within kube that may be of use to you. Use of these will generally not inflate your total build times due to already being present in the tree:</p> <ul> <li>tracing</li> <li>futures</li> <li>k8s-openapi</li> <li>serde</li> <li>serde_json</li> <li>serde_yaml</li> <li>tower</li> <li>tower-http</li> <li>hyper</li> <li>thiserror</li> </ul> <p>These in turn also pull in their own dependencies (and tls features, depending on your tls stack), consult cargo-tree for help minimizing your dependency tree.</p>"},{"location":"controllers/availability/","title":"Availability","text":"<p>This chapter is about strategies for improving controller availability and tail latencies.</p>"},{"location":"controllers/availability/#motivation","title":"Motivation","text":"<p>Despite the common goals often set forth for application deployments, most <code>kube</code> controllers:</p> <ul> <li>can run in a single replica (default recommendation)</li> <li>can handle being killed, and be shifted to another node</li> <li>can handle minor downtime</li> </ul> <p>This is due to a couple of properties:</p> <ul> <li>Controllers are queue consumers that do not require 100% uptime to meet a 100% SLO</li> <li>Rust images are often very small and will reschedule quickly</li> <li>watch streams re-initialise quickly with the current state on boot</li> <li>reconciler#idempotency means multiple repeat reconciliations are not problematic</li> <li>parallel execution mode of reconciliations makes restarts fast</li> </ul> <p>These properties combined creates a low-overhead system that is normally quick to catch-up after being rescheduled, and offers a traditional Kubernetes eventual consistency guarantee.</p> <p>That said, this setup can struggle under strong consistency requirements. Ask yourself:</p> <ul> <li>How quickly do you expect your reconciler to respond to changes on average?</li> <li>Is a <code>30s</code> P95 downtime from reschedules acceptable?</li> </ul>"},{"location":"controllers/availability/#responsiveness","title":"Responsiveness","text":"<p>If you want to improve average responsiveness, then traditional scaling and optimization strategies can help:</p> <ul> <li>Configure controller concurrency to avoid waiting for a reconciler slot</li> <li>Optimize the reconciler, avoid duplicated work</li> <li>Satisfy CPU requirements to avoid cgroup throttling</li> <li>Ensure your relations are setup right to avoid waiting for the next requeue</li> </ul> <p>You can plot heatmaps of reconciliation times in grafana using standard observability#What Metrics.</p>"},{"location":"controllers/availability/#high-availability","title":"High Availability","text":"<p>Scaling a controller beyond one replica for HA is different than for a regular load-balanced traffic receiving application.</p> <p>A controller is effectively a consumer of Kubernetes watch events, and these are themselves unsynchronised event streams whose watchers are unaware of each other. Adding another pod - without some form of external locking - will result in duplicated work.</p> <p>To avoid this, most controllers lean into the eventual consistency model and run with a single replica, accepting higher tail latencies due to reschedules. However, if the performance demands are strong enough, these pod reschedules will dominate the tail of your latency metrics, and this can make a stronger case for HA.</p> <p>Scaling Replicas</p> <p>It not recommended to set <code>replicas: 2</code> for an application running a normal <code>Controller</code> without leaders/shards, as this will cause both controller pods to reconcile the same objects, creating duplicate work and potential race conditions.</p> <p>To safely operate with more than one pod, you must have leadership of your domain and wait for such leadership to be acquired before commencing. This is the concept of leader election.</p>"},{"location":"controllers/availability/#leader-election","title":"Leader Election","text":"<p>Leader election allows having control over resources managed in Kubernetes using Leases as distributed locking mechanisms.</p> <p>The common solution to downtime based-problems is to use the <code>leader-with-lease</code> pattern, by having another controller replica in \"standby mode\", ready to takeover immediately without stepping on the toes of the other controller pod. We can do this by creating a <code>Lease</code>, and gating on the validity of the lease before doing the real work in the reconciler.</p> <p>Unsynchronised Rollout Surges</p> <p>A 1 replica controller deployment without leader election might create short periods of duplicate work and racey writes during rollouts because of how rolling updates surge by default.</p> <p>The natural expiration of <code>leases</code> means that you are required to periodically update them while your main pod (the leader) is active. When your pod is about be replaced, you can initiate a step down (and expire the lease), ideally after receiving a <code>SIGTERM</code> after draining your active work queue. If your pod crashes, then a replacement pod must wait for the scheduled lease expiry.</p>"},{"location":"controllers/availability/#third-party-crates","title":"Third Party Crates","text":"<p>At the moment, leader election support is not supported by <code>kube</code> itself, and requires 3<sup>rd</sup> party crates (see kube#485). A brief list of popular crates:</p> <ul> <li><code>kube-leader-election</code> via hendrikmaus (examples / docs / disclaimer)</li> <li><code>kube-coordinate</code> via thedodd (docs)</li> <li><code>kubert</code> -&gt; <code>kubert::lease</code> via olix0r (example / linkerd use)</li> </ul>"},{"location":"controllers/availability/#elected-shards","title":"Elected Shards","text":"<p>Leader election can in-theory be used on top of explicit scaling#sharding to ensure you have at most one replica managing one shard by using one lease per shard. This could reduce the number of excess replicas standing-by in a sharded scenario.</p>"},{"location":"controllers/gc/","title":"Garbage Collection","text":"<p>This chapter covers the two main forms of Kubernetes garbage collection and when + how to use them with controllers.</p>"},{"location":"controllers/gc/#owner-references","title":"Owner References","text":"<p>When your object owns another resource living inside Kubernetes, you can put an owner reference on the child object so that when you delete the parent object, Kubernetes will automatically initiate cleanup of the dependents.</p> <p>This is explained in more detail in Kubernetes.io :: Owners and Dependents.</p> <p>OwnerReferences are for children</p> <p>You should use owner references on generated child objects that have a clear owner object within Kubernetes.</p> <p>To successfully use owner references you need to:</p> <ol> <li>insert the reference on any object you are creating from your reconciler</li> <li>mark the children as watchable through owner relations</li> </ol>"},{"location":"controllers/gc/#owner-reference-example","title":"Owner Reference Example","text":"<p>In the configmapgen_controller example, the controller creates a <code>ConfigMap</code> from a contrieved <code>ConfigMapGenerator</code> custom resource (cmg crd). The example's reconciler for the <code>ConfigMapGenerator</code> objects insert the <code>owner_reference</code> into the generated <code>ConfigMap</code>:</p> <pre><code>let oref = generator.controller_owner_ref(&amp;()).unwrap();\nlet cm = ConfigMap {\n    metadata: ObjectMeta {\n        name: generator.metadata.name.clone(),\n        owner_references: Some(vec![oref]),\n        ..ObjectMeta::default()\n    },\n    data: Some(contents),\n    ..Default::default()\n};\n</code></pre> <p>using <code>Resource::controller_owner_ref</code>. It then marks <code>ConfigMap</code> as a dependent type to watch for via relations:</p> <pre><code>Controller::new(cmgs, watcher::Config::default())\n    .owns(cms, watcher::Config::default())\n</code></pre>"},{"location":"controllers/gc/#finalizers","title":"Finalizers","text":"<p>A finalizer is a marker on a root object that indicates that a controller will perform cleanup if the object is ever deleted. Kubernetes will block the object from being deleted until the controller completes the cleanup. The controller is supposed to remove this marker in the finalizer list when cleanup is done, so that Kubernetes is free to proceed with deletion.</p> <p>This is explained in more detail in Kubernetes.io :: Finalizers.</p> <p>Finalizers mark the need for controller cleanup</p> <p>You should mark objects with a finalizer if it needs external cleanup to run in the event it is deleted.</p> <p>The main way to use finalizers with controllers is to define a unique finalizer name (many controllers can finalize an object) and make the finalizer helper manage it. The finalizer helper is designed to be used within a reconciler and split the work depending on the state we are in:</p> <ul> <li>Has a deletion occurred, and do we need to clean up? If so, we are in the <code>Event::Cleanup</code> arm</li> <li>Has no deletion been recorded? Then we are in the normal <code>Event::Apply</code> arm</li> </ul> <p>Finalizers can prevent objects from being deleted</p> <p>If your controller is down, deletes will be delayed until the controller is back.</p>"},{"location":"controllers/gc/#finalizer-example","title":"Finalizer Example","text":"<p>In the secret_syncer example, the controller manages an artificially external secret resource (in reality the example puts it in Kubernetes, but please ignore that) on changes to a <code>ConfigMap</code>.</p> <p>Because we cannot normally watch external resources through Kubernetes watches, we have not setup any relations for the secret. Instead, we use the finalizer helper in a reconciler (here as a lambda), and delegate to two more specific reconcilers:</p> <pre><code>|cm, _| {\n    let ns = cm.meta().namespace.as_deref().ok_or(Error::NoNamespace).unwrap();\n    let cms: Api&lt;ConfigMap&gt; = Api::namespaced(client.clone(), ns);\n    let secrets: Api&lt;Secret&gt; = Api::namespaced(client.clone(), ns);\n    async move {\n        finalizer(\n            &amp;cms,\n            \"configmap-secret-syncer.nullable.se/cleanup\",\n            cm,\n            |event| async {\n                match event {\n                    Event::Apply(cm) =&gt; apply(cm, &amp;secrets).await,\n                    Event::Cleanup(cm) =&gt; cleanup(cm, &amp;secrets).await,\n                }\n            },\n        )\n        .await\n    }\n}\n</code></pre> <p>in this example, the <code>cleanup</code> fn is deleting the secret (which you should imagine as not living inside Kubernetes), and the <code>apply</code> fn looks like how your <code>reconcile</code> fn normally would look like.</p> <p>If you run this example locally and apply the example configmap, you will notice you cannot <code>kubectl delete</code> it the object once it has been reconciled once without keeping the controller running; the <code>cleanup</code> is guaranteed to run.</p>"},{"location":"controllers/gc/#default-cleanup","title":"Default Cleanup","text":"<p>Not every controller needs extra cleanup in one of the two forms above.</p> <p>If you are satisfied with your object being removed if someone runs <code>kubectl delete</code>, then that's all the cleanup you need.</p> <p>You only need these extra forms of garbage collection when you are directly in charge of the lifecycle of other resources - inside or outside Kubernetes.</p>"},{"location":"controllers/gc/#summary","title":"Summary","text":"<p>In short, if you need to:</p> <ol> <li>Automatically garbage collect child objects? use <code>ownerReferences</code></li> <li>Programmatically garbage collect dependent resources? use <code>finalizers</code></li> </ol> <p>If you are generating resources both inside and outside Kubernetes, you might need both kinds of cleanup (or make a bigger <code>cleanup</code> finalizer routine).</p>"},{"location":"controllers/generics/","title":"Generics","text":"<p>This chapter contains tips and tricks for re-using generic reconcilers.</p>"},{"location":"controllers/generics/#generic-reconcilers","title":"Generic Reconcilers","text":"<p>It is possible to create generic reconcilers by abstracing away the underlying type by using either DynamicObject or PartialObjectMeta.</p> <p>This is a useful technique for controllers that need to do the same thing to a bunch of resources, like for instance adding consistent labels / annotations.</p>"},{"location":"controllers/generics/#partialobjectmeta","title":"PartialObjectMeta","text":"<p>This is the easiest and cheapest way to create a generic reconciler because it still retains type information, and you do not have to devolve into a fully dynamic api.</p> <p>You can create generic reconcilers:</p> <pre><code>pub async fn reconcile&lt;K&gt;(obj: Arc&lt;PartialObjectMeta&lt;K&gt;&gt;, ctx: Arc&lt;Context&gt;)\n-&gt; Result&lt;Action&gt;\nwhere\n    K: Resource&lt;Scope = NamespaceResourceScope, DynamicType = ()&gt;\n        + Clone\n        + DeserializeOwned\n        + Debug,\n{\n    let kind = K::kind(&amp;()).to_string();\n    let ns = obj.namespace().unwrap();\n    let object_name = obj.name_any();\n    let api: Api&lt;PartialObjectMeta&lt;K&gt;&gt; = Api::namespaced(ctx.client, &amp;ns);\n\n    // example work; apply some labels to the object\n    let patch: Patch&lt;serde_json::Value&gt; = get_standard_labels_for(&amp;obj)?;\n    let serverside = PatchParams::apply(\"labeller\");\n    api.patch(&amp;object_name, &amp;serverside, &amp;patch).await?;\n\n    Ok(Action::requeue(Duration::from_secs(5 * 60)))\n}\n</code></pre> <p>The generic constraints on the associated type of the Resource here means this is a namespaced resource (hence the unwrap). You could remove this bound (or change it for a cluster scoped only bound), but then you could not unwrap.</p> <p>The <code>DynamicType = ()</code> constraint is to indicate that this is one of the normal statically generated api types that we have api information for at the type level (i.e. they come from <code>k8s-openapi</code>).</p> <p>For information about the resource we rely on the generic Resource and ResourceExt traits which is implemented by PartialObjectMeta.</p> <p>Diverging Logic</p> <p>You will only get access to metadata of the object doing this. This can be mitigated by doing a <code>match</code> on <code>kind</code> and creating a more specific <code>Api&lt;K&gt;</code> inside a match arm.</p> <p>This reconciler can be hooked up to infallabily start a <code>Controller&lt;K&gt;</code> in a generic way:</p> <pre><code>async fn run_controller&lt;K&gt;(client: Client)\nwhere\n    K: Resource&lt;Scope = NamespaceResourceScope, DynamicType = ()&gt;\n        + Clone\n        + DeserializeOwned\n        + Debug\n        + Sync\n        + Send\n        + 'static,\n{\n    let kind = K::kind(&amp;()).to_string();\n    tracing::info!(\"Starting controller for {kind}\");\n\n    let api = Api::&lt;K&gt;::all(client.clone());\n    let (reader, writer) = reflector::store();\n\n    // controller main stream from metadata_watcher\n    let stream = metadata_watcher(api, watcher::Config::default())\n        .default_backoff()\n        .modify(|x| {\n            x.managed_fields_mut().clear(); // ResourceExt pruning\n        })\n        .reflect(writer)\n        .applied_objects();\n\n    Controller::for_stream(stream, reader)\n        .shutdown_on_signal()\n        .run(reconcile, error_policy, Arc::new(Context::new(client)))\n        .for_each(|_| futures::future::ready(()))\n        .await;\n\n    warn!(\"controller for {kind} shutdown\");\n}\n</code></pre> <p>This example assumes no relations between the main controller object, so that each controller can be started in isolation without worrying about inefficiencies in streams usage.</p> <p>It uses metadata_watcher to provide a consistent input stream of <code>PartialObjectMeta&lt;K&gt;</code> with pruning (optimization#pruning-fields) and Store management through WatchStreamExt.</p> <p>We can start and control the lifecycle of all the controllers with a tokio::join!:</p> <pre><code>pub async fn run_all_controllers(client: Client) {\n    let _ = tokio::join!(\n        run_controller::&lt;Deployment&gt;(client.clone()),\n        run_controller::&lt;DaemonSet&gt;(client.clone()),\n        run_controller::&lt;StatefulSet&gt;(client.clone()),\n        run_controller::&lt;CronJob&gt;(client.clone()),\n    );\n    info!(\"controllers all exited\");\n}\n</code></pre> <p>This returns when all controller fns return. This happens once shutdown_on_signal has safely propagated through all the controllers.</p>"},{"location":"controllers/internals/","title":"Internals","text":"<p>This is a brief overview of Controller internals.</p> <p>Suppose you have a <code>Controller</code> for a main object <code>K</code> which owns a child object <code>C</code>.</p> <p>I.e. if <code>child_c</code> is an <code>Api&lt;C&gt;</code> and <code>main_k</code> is an <code>Api&lt;K&gt;</code>, then the following code sets up this basic scenario:</p> <pre><code>Controller::new(main_k, watcher::Config::default())\n    .owns(child_c, watcher::Config::default())\n    .run(reconcile, error_policy, context)\n    .for_each(|_| futures::future::ready(()))\n    .await\n</code></pre> <p>This <code>Controller</code> builder sets up a series of streams and links between them:</p> <pre><code>graph TD\n    K{{Kubernetes}} --&gt;|Event K| W(watchers)\n    K --&gt;|Event C| W\n    W --&gt;|map C -&gt; K| I(queue)\n    W --&gt;|owned K| I\n    RU --&gt;|run| R(reconciler)\n    A(applier) --&gt;|poll| I\n    A --&gt;|schedule| S(scheduler)\n    A --&gt;|poll| S\n    A --&gt;|next| RU(runner)\n    R --&gt;|Update| X{{World}}\n    R -.-&gt;|result| A\n    subgraph \"Controller\"\n    W\n    I\n    S\n    A\n    RU\n    end\n    subgraph \"Application\"\n    Controller\n    R\n    end</code></pre> <p>I.e. basic flow.</p> <ul> <li><code>watcher</code>s poll the Kubernetes api for changes to configured objects (<code>K</code> and <code>C</code>)</li> <li>stream of events from each <code>watcher</code> needs to be turned into a stream of the same type</li> <li>streams of non K type run through mappers (<code>C</code> maps to <code>K</code> through relations)</li> <li>initial queue created as the union of these streams</li> </ul> <p>The <code>applier</code> then polls this stream and merges it further with previous reconciliation results (requeues for a later time). This forces the need for a <code>scheduler</code>.</p>"},{"location":"controllers/internals/#queue","title":"Queue","text":"<p>The queue is the stream of inputs.</p> <p>It takes <code>N</code> main inputs (root object, related objects, external triggers), and it is our <code>trigger_selector</code>.</p>"},{"location":"controllers/internals/#scheduler","title":"Scheduler","text":"<p>The <code>scheduler</code> wraps the object to be reconciled in a <code>future</code> that will resolve when its associated timer is up.</p> <p>All reconcile requests go through this, but only requeues are delayed significantly.</p>"},{"location":"controllers/internals/#applier","title":"Applier","text":"<p>The applier is the most complicated component on the diagram because it is in charge of juggling the various input streams, invoking the scheduler and runner, and also somehow produce a stream of reconcile results at the same time.</p> <p>The flow of doing this is quite complicated in rust so the way this is done internally is likely subject to change in the long run. Please refer to the source of the applier for more details.</p> <p>It is possible to set up the controller machinery yourself by creating the queue yourself from watchers and then calling the <code>applier</code> with the queue injected, but this is not recommended.</p>"},{"location":"controllers/intro/","title":"Introduction","text":"<p>This is a larger guide to showcase how to build controllers, and is a WIP with a progress issue.</p>"},{"location":"controllers/intro/#overview","title":"Overview","text":"<p>A controller is a long-running program that ensures the Kubernetes state of an object matches the state of the world.</p> <p>As users update the desired state, the controller sees the change and schedules a reconciliation, which will update the state of the world:</p> <pre><code>flowchart TD\n    A[User/CD] -- kubectl apply object.yaml --&gt; K[Kubernetes Api]\n    C[Controller] -- watch objects --&gt; K\n    C -- schedule object --&gt; R[Reconciler]\n    R -- result --&gt; C\n    R -- update state --&gt; K</code></pre> <p>Any unsuccessful reconciliations are retried or requeued, so a controller should eventually apply the desired state to the world.</p> <p>Writing a controller requires three pieces:</p> <ul> <li>an object dictating what the world should see</li> <li>an reconciler function that ensures the state of one object is applied to the world</li> <li>an application living in Kubernetes watching the object and related objects</li> </ul>"},{"location":"controllers/intro/#the-object","title":"The Object","text":"<p>The main object is the source of truth for what the world should be like, and it takes the form of a Kubernetes object like a:</p> <ul> <li>Pod</li> <li>Deployment</li> <li>..any native Kubernetes Resource</li> <li>a partially typed or dynamically typed Kubernetes Resource</li> <li>an object from api discovery</li> <li>a Custom Resource</li> </ul> <p>Kubernetes already has a core controller manager for the core native objects, so the most common use-case for controller writing is a Custom Resource, but other more fine-grained use-cases exist.</p> <p>Check if your use case fits</p> <p>Not all use-cases are well-served by a custom controller. Kubernetes.io has a checklist to consider before creating a CRD + a controller for it.</p> <p>See the object document for how to use the various types.</p>"},{"location":"controllers/intro/#the-reconciler","title":"The Reconciler","text":"<p>The reconciler is the part of the controller that ensures the world is up to date.</p> <p>It takes the form of an <code>async fn</code> taking the object along with some context, and performs the alignment between the state of the world and the <code>object</code>.</p> <p>In its simplest form, this is what a reconciler (that does nothing) looks like:</p> <pre><code>async fn reconcile(object: Arc&lt;MyObject&gt;, data: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt; {\n    // TODO: logic here\n    Ok(Action::requeue(Duration::from_secs(3600 / 2)))\n}\n</code></pre> <p>As a controller writer, your job is to complete the logic that align the world with what is inside the <code>object</code>. The core reconciler must at minimum contain mutating api calls to what your <code>object</code> is meant to manage, and in some situations, handle annotations management for ownership or garbage collection.</p> <p>Writing a good idempotent reconciler is the most difficult part of the whole affair, and its difficulty is the reason we generally provide diagnostics and observability.</p> <p>See the reconciler document for more information.</p>"},{"location":"controllers/intro/#the-application","title":"The Application","text":"<p>The controller application is the part that watches for changes, determines what root object needs reconciliations, and then schedules reconciliations for those changes. It is the glue that turns what you want into something running in Kubernetes.</p> <p>In this guide; the application is written in rust, using the kube crate as a dependency with the <code>runtime</code> feature, compiled into a container, and deployed in Kubernetes as a <code>Deployment</code>.</p> <p>The core features inside the application are:</p> <ul> <li>an encoding of the main object + relevant objects</li> <li>an infinite watch loop around relevant objects</li> <li>a system that maps object changes to the relevant main object</li> <li>an idempotent reconciler acting on a main object</li> </ul> <p>The system must be fault-tolerant, and thus must be able to recover from crashes, downtime, and resuming even having missed messages.</p> <p>Setting up a blank controller in rust satisfying these constraints is fairly simple, and can be done with minimal boilerplate (no generated files need to be inlined in your project).</p> <p>See the application document for the high-level details.</p>"},{"location":"controllers/intro/#controllers-and-operators","title":"Controllers and Operators","text":"<p>The terminology between controllers and operators are quite similar:</p> <ol> <li>Kubernetes uses the following controller terminology:</li> </ol> <p>In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.</p> <ol> <li>The term operator, on the other hand, was originally introduced by <code>CoreOS</code> as:</li> </ol> <p>An Operator is an application-specific controller that extends the Kubernetes API to create, configure and manage instances of complex stateful applications on behalf of a Kubernetes user. It builds upon the basic Kubernetes resource and controller concepts, but also includes domain or application-specific knowledge to automate common tasks better managed by computers.</p> <p>Which is further reworded now under their new agglomerate banner.</p> <p>The key difference between the two is that operators are generally a specific type of controller, sometimes more than one in a single application. To be classified as an operator, a controller would at the very least need to:</p> <ul> <li>manage custom resource definition(s)</li> <li>maintain single app focus</li> </ul> <p>The term operator is a flashier term that makes the common use-case for user-written CRD controllers more understandable. If you have a CRD, you likely want to write a controller for it (otherwise why go through the effort of making a custom resource?).</p>"},{"location":"controllers/intro/#guide-focus","title":"Guide Focus","text":"<p>Our goal is that with this guide, you will learn how to use and apply the various controller patterns, so that you can avoid scaffolding out a large / complex / underutilized structure.</p> <p>We will focus on all the patterns as to not betray the versatility of the Kubernetes API, because components found within complex controllers can generally be mixed and matched as you see fit.</p> <p>We will focus on how the various element composes so you can take advantage of any controller archetypes - operators included.</p>"},{"location":"controllers/manifests/","title":"Manifests","text":"<p>This chapter is about deployment manifests and common resources you likely want to include.</p>"},{"location":"controllers/manifests/#rbac","title":"RBAC","text":"<p>A Kubernetes <code>Role</code> / <code>ClusterRole</code> (with an associated binding) is necessary for your controller to function in-cluster. Below we list the common rules you need for the basics:</p> <pre><code>kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: kube-rs-controller\nrules:\n# You want access to your CRD if you have one\n# Replace documents with plural resource name, and kube.rs with your group\n- apiGroups: [\"kube.rs\"]\n  resources: [\"documents\", \"documents/status\", \"documents/finalizers\"]\n  verbs: [\"get\", \"list\", \"watch\", \"patch\", \"update\"]\n\n# If you want events\n- apiGroups: [\"events.k8s.io\"]\n  resources: [\"events\"]\n  verbs: [\"create\"]\n</code></pre> <p>See security#Access Constriction to ensure the setup is as strict as is needed.</p> <p>Two Event structs</p> <p>The runtime event Recorder uses the modern events.k8s.io.v1.Event not to be mistaken for the legacy core.v1.Event.</p> <p>We do not provide any hooks to generate RBAC from Rust source (it's not super helpful), so it is expected you put the various rules you need straight in your chart templates / jsonnet etc.</p> <p>See controller-rs/rbac for how to hook this up with <code>helm</code>.</p>"},{"location":"controllers/manifests/#network-policy","title":"Network Policy","text":"<p>To reduce unnecessary access from and to your controller, it is a good Security practice to use network policies.</p> <p>Below is a starter <code>netpol</code> here that allows DNS, talking to the Kubernetes apiserver, and basic observability such as pushing otel spans, and having metrics scraped by <code>prometheus</code>:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: kube-rs-controller\n  labels:\n    app: kube-rs-controller\n  namespace: controllers\nspec:\n  podSelector:\n    matchLabels:\n      app: kube-rs-controller\n  policyTypes:\n  - Ingress\n  - Egress\n\n  egress:\n  # Pushing tracing spans to an opentelemetry collector\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: opentelemetry-operator-system\n    ports:\n    # jaeger thrift\n    - port: 14268\n      protocol: TCP\n    # OTLP gRPC\n    - port: 4317\n      protocol: TCP\n    # OTLP HTTP\n    - port: 4318\n      protocol: TCP\n    # zipkin\n    - port: 9411\n      protocol: TCP\n\n  # Kubernetes apiserver\n  - to:\n    - ipBlock:\n        # range should be replaced by kubernetes endpoint addresses from:\n        # kubectl get endpoints kubernetes -oyaml\n        cidr: 10.20.0.2/32\n    ports:\n    - port: 443\n      protocol: TCP\n    - port: 6443\n      protocol: TCP\n\n  # DNS\n  - to:\n    - podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - port: 53\n      protocol: UDP\n\n  ingress:\n  # prometheus metric scraping support\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring\n      podSelector:\n        matchLabels:\n          app: prometheus\n    ports:\n    - port: http\n      protocol: TCP\n</code></pre> <p>Adjust your app labels, names, namespaces, and ingress port names to your own values. Consider using the Network Policy Editor for more interactive sanity.</p> <p>See controller-rs/networkpolicy for how to hook this up with <code>helm</code>.</p> <p>Some notes on the above:</p> <ul> <li>apiserver egress is complicated. A <code>namespaceSelector</code> on <code>default</code> sometimes work, but the safest is get the <code>endpoints</code>. See the controller-rs/netpol pr. Cilium's counterpart of <code>toEntities: [ kube-apiserver ]</code> looks friendlier.</li> <li>DNS egress should work for both <code>coredns</code> and <code>kube-dns</code> (via <code>k8s-app: kube-dns</code>)</li> <li><code>prometheus</code> port and app labels might depend on deployment setup, drop lines from the strict default, or tune values as you see fit</li> <li><code>opentelemetry-collector</code> values are the regular defaults from the collector helm chart - change as you see fit</li> <li>the policy editor needs a non-aliased integer port - while valid, it will reject <code>port: http</code> above</li> </ul>"},{"location":"controllers/object/","title":"The Object","text":"<p>A controller always needs a source of truth for what the world should look like, and this object always lives inside Kubernetes.</p> <p>Depending on how the object was created/imported or performance optimization reasons, you can pick one of the following object archetypes:</p> <ul> <li>typed Kubernetes native resource</li> <li>Derived Custom Resource for Kubernetes</li> <li>Imported Custom Resource already in Kubernetes</li> <li>untyped Kubernetes resource</li> <li>partially typed Kubernetes resource</li> </ul> <p>We will outline how they interact with controllers and the basics of how to set them up.</p>"},{"location":"controllers/object/#typed-resource","title":"Typed Resource","text":"<p>This is the most common, and simplest case. Your source of truth is an existing Kubernetes object found in the openapi spec.</p> <p>To use a typed Kubernetes resource as a source of truth in a Controller, import it from k8s-openapi, and create an Api from it, then pass it to the Controller.</p> <pre><code>use k8s_openapi::api::core::v1::Pod;\n\nlet pods = Api::&lt;Pod&gt;::all(client);\nController::new(pods, watcher::Config::default())\n</code></pre> <p>This is the simplest flow and works right out of the box because the openapi implementation ensures we have all the api information via the Resource traits.</p> <p>If you have a native Kubernetes type, you generally want to start with k8s-openapi. It will likely do exactly what you want without further issues. That said, if both your clusters and your chosen object are large, then you can consider optimizing further by changing to a partially typed resource for smaller memory profile.</p> <p>A separate k8s-pb repository for our future protobuf serialization structs also exists, and while it will slot into this category and should hotswappable with k8s-openapi, it is not yet usable here.</p>"},{"location":"controllers/object/#derived-resource","title":"Derived Resource","text":"<p>A similar way of doing this is to have your own struct, but inherit the typing (api parameters) from a known openapi type. This allows customising the type / impls for memory reasons / particular constraints, and is also a way of doing partial typing.</p> <pre><code>use k8s_openapi::api::core::v1::ConfigMap;\n// ConfigMap Variant that inherits from k8s-openapi\n#[derive(Resource, Serialize, Deserialize, Debug, Clone)]\n#[resource(inherit = ConfigMap)]\nstruct CaConfigMap {\n    metadata: ObjectMeta,\n    data: CaConfigMapData, // Custom Data struct implementation\n}\n\nlet cm = Api::&lt;CaConfigMap&gt;::all(client);\n</code></pre> <p>This can additionally be combined with DeserializeGuard for deserialize safety.</p> <p>See the errorbound_configmap, and cert_check example for more details.</p>"},{"location":"controllers/object/#custom-resources","title":"Custom Resources","text":""},{"location":"controllers/object/#derived-custom-resource","title":"Derived Custom Resource","text":"<p>The operator use case is heavily based on you writing your own struct, and a schema, and extending the Kubernetes api with it.</p> <p>This has historically required a lot of boilerplate for both the api information and the (now required) schema, but this is a lot simpler with kube thanks to the CustomResource derive proc_macro.</p> <pre><code>/// Our Document custom resource spec\n#[derive(CustomResource, Deserialize, Serialize, Clone, Debug, JsonSchema)]\n#[kube(kind = \"Document\", group = \"kube.rs\", version = \"v1\", namespaced)]\n#[kube(status = \"DocumentStatus\")]\npub struct DocumentSpec {\n    name: String,\n    author: String,\n}\n\n#[derive(Deserialize, Serialize, Clone, Debug, JsonSchema)]\npub struct DocumentStatus {\n    checksum: String,\n    last_updated: Option&lt;DateTime&lt;Utc&gt;&gt;,\n}\n</code></pre> <p>This will generate a <code>pub struct Document</code> in this scope which implements Resource. In other words, to use it with the controller is at this point analogous to a fully typed resource:</p> <pre><code>let docs = Api::&lt;Document&gt;::all(client);\nController::new(docs, watcher::Config::default())\n</code></pre> <p>Custom resources require schemas</p> <p>Kubernetes requires openapi schemas inside every CustomResourceDefinition (since Kubernetes 1.22). Below we use the standard mechanism of deriving <code>JsonSchema</code> using schemars. See schemas for details.</p>"},{"location":"controllers/object/#installation","title":"Installation","text":"<p>Before Kubernetes accepts api calls for a custom resource, we need to install it. This is the usual pattern for creating the yaml definition:</p> <pre><code># Cargo.toml\n[[bin]]\nname = \"crdgen\"\npath = \"src/crdgen.rs\"\n</code></pre> <pre><code>// crdgen.rs\nuse kube::CustomResourceExt;\nfn main() {\n    print!(\"{}\", serde_yaml::to_string(&amp;mylib::Document::crd()).unwrap())\n}\n</code></pre> <p>Here, a separate <code>crdgen</code> bin entry would install your custom resource using <code>cargo run --bin crdgen | kubectl apply -f -</code>.</p> <p>CRD Installation</p> <p>Be careful with installing CRDs inside a controller at startup. It is customary to provide a generated yaml file so consumers can install a CRD out of band to better support gitops and helm. See security#crd-access.</p>"},{"location":"controllers/object/#imported-custom-resource","title":"Imported Custom Resource","text":"<p>In the case that a <code>customresourcedefinition</code> already exists, but it was implemented in another language, then we can generate structs from the schema using kopium.</p> <p>Suppose you want to write some extra controller or replace the native controller for <code>PrometheusRule</code>:</p> <pre><code>curl -sSL https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml \\\n    | kopium -Af - &gt; prometheusrule.rs\n</code></pre> <p>this will read the crd from a file / cluster, and generate rust-optimized structs for it:</p> <pre><code>use kube::CustomResource;\nuse schemars::JsonSchema;\nuse serde::{Serialize, Deserialize};\nuse std::collections::BTreeMap;\nuse k8s_openapi::apimachinery::pkg::util::intstr::IntOrString;\n\n/// Specification of desired alerting rule definitions for Prometheus.\n#[derive(CustomResource, Serialize, Deserialize, Clone, Debug, JsonSchema)]\n#[kube(group = \"monitoring.coreos.com\", version = \"v1\", kind = \"PrometheusRule\", plural = \"prometheusrules\")]\n#[kube(namespaced)]\npub struct PrometheusRuleSpec {\n    /// Content of Prometheus rule file\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub groups: Option&lt;Vec&lt;PrometheusRuleGroups&gt;&gt;,\n}\n\n/// RuleGroup is a list of sequentially evaluated recording and alerting rules.\n#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]\npub struct PrometheusRuleGroups {\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub interval: Option&lt;String&gt;,\n    pub name: String,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub partial_response_strategy: Option&lt;String&gt;,\n    pub rules: Vec&lt;PrometheusRuleGroupsRules&gt;,\n}\n\n/// Rule describes an alerting or recording rule\n#[derive(Serialize, Deserialize, Clone, Debug, JsonSchema)]\npub struct PrometheusRuleGroupsRules {\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub alert: Option&lt;String&gt;,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub annotations: Option&lt;BTreeMap&lt;String, String&gt;&gt;,\n    pub expr: IntOrString,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub r#for: Option&lt;String&gt;,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub labels: Option&lt;BTreeMap&lt;String, String&gt;&gt;,\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub record: Option&lt;String&gt;,\n}\n</code></pre> <p>you can then import this file as a module and use it as follows:</p> <pre><code>use prometheusrule::PrometheusRule;\n\nlet prs: Api&lt;PrometheusRule&gt; = Api::default_namespaced(client);\nController::new(prs, watcher::Config::default())\n</code></pre> <p>Kopium is unstable</p> <p>Kopium is a relatively new project and it is neither feature complete nor bug free at the moment. While feedback has been very positive, and people have so far contributed fixes for several major customresources; expect some snags.</p> <p>These generated structs are sometimes published for easier consumption. The kube-custom-resources-rs crate contains a catalog of generated code from compatible schemas.</p>"},{"location":"controllers/object/#dynamic-typing","title":"Dynamic Typing","text":""},{"location":"controllers/object/#untyped-resources","title":"Untyped Resources","text":"<p>Untyped resources are using DynamicObject; an umbrella container for arbitrary Kubernetes resources.</p> <p>Hard to use with controllers</p> <p>This type is the most unergonomic variant available. You will have to operate on untyped json to grab data out of specifications and is best suited for general (non-controller) cases where you need to look at common metadata properties from ObjectMeta like <code>labels</code> and <code>annotations</code> across different object types.</p> <p>The DynamicObject consists of just the unavoidable properties like <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code>, whereas the entire spec is loaded onto an arbitrary serde_json::Value via flattening.</p> <p>The benefits you get are that:</p> <ul> <li>you avoid having to write out fields manually</li> <li>you can achieve tolerance against multiple versions of your object</li> <li>it is compatible with api discovery</li> </ul> <p>but you do have to find out where the object lives on the api (its ApiResource) manually:</p> <pre><code>use kube::{api::{Api, DynamicObject}, discovery};\n\n// Discover most stable version variant of `documents.kube.rs`\nlet apigroup = discovery::group(&amp;client, \"kube.rs\").await?;\nlet (ar, caps) = apigroup.recommended_kind(\"Document\").unwrap();\n\n// Use the discovered kind in an Api, and Controller with the ApiResource as its DynamicType\nlet api: Api&lt;DynamicObject&gt; = Api::all_with(client, &amp;ar);\nController::new_with(api, watcher::Config::default(), &amp;ar)\n</code></pre> <p>Other ways of doing discovery are also available. We are highlighting recommended_kind in particular here because it can be used to achieve version agnosticity.</p> <p>Multiple versions of an object</p> <p>Kubernetes supports specifying multiple versions of a specification, and using DynamicObject above can help solve that. There are other potential ways of achieving similar results, but it does require some work.</p>"},{"location":"controllers/object/#partially-typed-resource","title":"Partially-typed Resource","text":"<p>You can partially implement structs for an existing resource. This can be to manually control memory characteristic, deserialization parameters, or api parameters. It is done by using Object on an incomplete struct and supplying an existing ApiResource.</p> <p>This can be a quick way to control memory use, or to shield your app against the full api surface or versions of a quickly moving CRD.</p> <p>Memory Optimizations</p> <p>An easier way to control memory use of stores is via optimization#Pruning Fields.</p> <p>As an example: a handwritten implementation of Pod by overriding its spec and status and placing it inside Object, then stealing its type information from <code>k8s-openapi</code>:</p> <pre><code>use kube::api::{Api, ApiResource, NotUsed, Object};\n\n// Here we replace heavy type k8s_openapi::api::core::v1::PodSpec with\n#[derive(Clone, Deserialize, Debug)]\nstruct PodSpecSimple {\n    containers: Vec&lt;ContainerSimple&gt;,\n}\n#[derive(Clone, Deserialize, Debug)]\nstruct ContainerSimple {\n    #[allow(dead_code)]\n    image: String,\n}\n// Pod replacement\ntype PodSimple = Object&lt;PodSpecSimple, NotUsed&gt;;\n\n// steal api resource information from k8s-openapi\nlet ar = ApiResource::erase::&lt;k8s_openapi::api::core::v1::Pod&gt;(&amp;());\n\nController::new_with(api, watcher::Config::default(), &amp;ar)\n</code></pre> <p>We have to recursively re-implement every part of Pod that we care about, but we automatically drop every field except the ones we defined. In this case we do not gain version independence (due to re-using pinned type-information), but you could gain this by using api discovery.</p> <p>This is functionally similar way to deriving <code>CustomResource</code> on an incomplete struct, but using (possibly) dynamic api parameters.</p> <p>Dynamic Partial Typing vs Derived Typing</p> <p>Dynamic partial typing is an older, cumbersome method that struggles with core apis (that does not follow the spec/status) model. If you actually have a type, consider using the newer derived typed resources setup to avoid using the dynamic api.</p>"},{"location":"controllers/object/#dynamic-new_with-constructors","title":"Dynamic new_with constructors","text":"<p>Partial or dynamic typing always needs additional type information</p> <p>All usage of <code>DynamicObject</code> or <code>Object</code> requires the use of alternate constructors for multiple interfaces such as Api and Controller. These constructors have an additional <code>_with</code> suffix to carry an associated type for the Resource trait.</p>"},{"location":"controllers/object/#summary","title":"Summary","text":"<p>All the fully typed methods all have a consistent usage pattern once the types have been generated. The dynamic and partial objects have more niche use cases and require a little more work such as alternate constructors.</p> typing Source Implementation  full k8s-openapi <code>use k8s-openapi::X</code>  full Derive-CustomResource <code>#[derive(CustomResource)]</code>  full kopium <code>kopium crd &gt; gen.rs</code>  full Derive-Resource <code>#[derive(Resource)]</code> + partial handwrite  none kube::core::Object partial handwrite  none kube::core::DynamicObject fully dynamic <p>Where the last two requires the dynamic <code>_with</code> constructors.</p>"},{"location":"controllers/observability/","title":"Observability","text":"<p>This document showcases common techniques for instrumentation:</p> <ul> <li>logs (via tracing + tracing-subscriber + EnvFilter)</li> <li>traces (via tracing + tracing-subscriber + opentelemetry-otlp + opentelemetry + [opentelemetry_sdk])</li> <li>metrics (via tikv/prometheus exposed via actix-web)</li> </ul> <p>and follows the approach of controller-rs.</p> <p>Most of this logic happens in <code>main</code>, before any machinery starts, so it will liberally <code>.unwrap()</code>.</p>"},{"location":"controllers/observability/#adding-logs","title":"Adding Logs","text":"<p>We will use the tracing library for logging because it allows us reusing the same system for tracing later.</p> <pre><code>cargo add tracing\ncargo add tracing-subscriber --features=env-filter\n</code></pre> <p>We will configure this in <code>main</code> by creating a <code>json</code> log layer with an EnvFilter picking up on the common <code>RUST_LOG</code> environment variable:</p> <pre><code>let logger = tracing_subscriber::fmt::layer().compact();\nlet env_filter = EnvFilter::try_from_default_env()\n    .or_else(|_| EnvFilter::try_new(\"info\"))\n    .unwrap();\n</code></pre> <p>This can be set as the global collector using:</p> <pre><code>Registry::default().with(env_filter).with(logger).init();\n</code></pre> <p>We will change how the <code>collector</code> is built if using tracing, but for now, this is sufficient for adding logging.</p>"},{"location":"controllers/observability/#adding-traces","title":"Adding Traces","text":"<p>Following on from logging section, we add extra dependencies to let us push traces to an opentelemetry collector (sending over gRPC with tonic):</p> <pre><code>cargo add opentelemetry --features=trace\ncargo add opentelemetry_sdk --features=rt-tokio\ncargo add opentelemetry-otlp --no-default-features --features=trace,grpc-tonic\n</code></pre> <p>Telemetry Dependencies</p> <p>This simple use of <code>cargo add</code> above assumes the above dependencies may not always work well at latest versions. You might receive multiple versions of <code>opentelemetry</code> libs / <code>tonic</code> in <code>cargo tree</code> (which might not work), and due to different release cycles and pins, you might not be able to upgrade opentelemetry dependencies immediately. For working combinations see for instance the pins in controller-rs + examples in tracing-opentelemetry.</p> <p>Setting up the layer and configuring the <code>collector</code> follows fundamentally the same process:</p> <pre><code>let otel = tracing_opentelemetry::OpenTelemetryLayer::new(init_tracer());\n</code></pre> <p>Change our registry setup to use 3 layers:</p> <pre><code>-Registry::default().with(logger).with(env_filter).init();\n+Registry::default().with(env_filter).with(logger).with(otel).init();\n</code></pre> <p>However, tracing requires us to have a configurable location of where to send spans, the provders needs to be globally registered, and you likely want to set some resource attributes, so creating the actual <code>tracer</code> requires a bit more work:</p> <pre><code>use opentelemetry::trace::TracerProvider;\nuse opentelemetry_sdk::trace::{SdkTracer, SdkTracerProvider};\n\nfn init_tracer() -&gt; SdkTracer {\n    use opentelemetry_otlp::{SpanExporter, WithExportConfig};\n    let endpoint = std::env::var(\"OPENTELEMETRY_ENDPOINT_URL\").expect(\"Needs an otel collector\");\n    let exporter = SpanExporter::builder()\n        .with_tonic()\n        .with_endpoint(endpoint)\n        .build()\n        .unwrap();\n\n    let provider = SdkTracerProvider::builder()\n        .with_resource(resource())\n        .with_batch_exporter(exporter)\n        .build();\n\n    provider.tracer(\"tracing-otel-subscriber\")\n}\n</code></pre> <p>Note the gRPC address (e.g. <code>OPENTELEMETRY_ENDPOINT_URL=https://0.0.0.0:55680</code>) must point to an otlp port on otel collector / tempo / etc. This can point to <code>0.0.0.0:PORT</code> if you portforward to it when doing <code>cargo run</code> locally, but in the cluster it should be the cluster dns as e.g. <code>http://promstack-tempo.monitoring.svc:431</code>.</p> <p>For some starting resource attributes;</p> <pre><code>use opentelemetry_sdk::Resource;\nfn resource() -&gt; Resource {\n    use opentelemetry::KeyValue;\n    Resource::builder()\n        .with_service_name(env!(\"CARGO_PKG_NAME\"))\n        .with_attribute(KeyValue::new(\"service.version\", env!(\"CARGO_PKG_VERSION\")))\n        .build()\n}\n</code></pre> <p>which can be extended better using the opentelemetry_semantic_conventions.</p> <p>For a full setup example for this code see controller-rs/telemetry.rs.</p>"},{"location":"controllers/observability/#instrumenting","title":"Instrumenting","text":"<p>Once you have initialised your registry, you can start adding <code>#[instrument]</code> attributes onto functions you want. Let's do <code>reconcile</code>:</p> <pre><code>#[instrument(skip(ctx))]\nasync fn reconcile(foo: Arc&lt;Foo&gt;, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt;\n</code></pre> <p>Note that the <code>reconcile</code> span should generally be the root span in the context of a controller. A reconciliation starting is generally the root of the chain, and since the <code>reconcile</code> fn is invoked by the runtime, nothing significant sits above it.</p> <p>Higher levels spans</p> <p>Do not <code>#[instrument]</code> any function that creates a Controller as this would create an unintentionally wide (application lifecycle wide) span being a parent to all <code>reconcile</code> spans. Such a span will be problematic to manage.</p>"},{"location":"controllers/observability/#linking-logs-and-traces","title":"Linking Logs and Traces","text":"<p>To link logs and traces we take advantage that tracing data is being outputted to both logs and our tracing collector, and attach the <code>trace_id</code> onto our root span:</p> <pre><code>#[instrument(skip(ctx), fields(trace_id))]\nasync fn reconcile(foo: Arc&lt;Foo&gt;, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt; {\n    let trace_id = get_trace_id();\n    if trace_id != TraceId::INVALID {\n        Span::current().record(\"trace_id\", field::display(&amp;trace_id));\n    }\n    todo!(\"reconcile implementation\")\n}\n</code></pre> <p>This part is useful for Loki or other logging systems as a way to cross-link from logs to traces.</p> <p>Extracting the <code>trace_id</code> requires a helper function atm:</p> <pre><code>pub fn get_trace_id() -&gt; opentelemetry::trace::TraceId {\n    use opentelemetry::trace::TraceContextExt as _;\n    use tracing_opentelemetry::OpenTelemetrySpanExt as _;\n    tracing::Span::current()\n        .context()\n        .span()\n        .span_context()\n        .trace_id()\n}\n</code></pre> <p>and it is the only reason for needing to directly add opentelemetry as a dependency.</p>"},{"location":"controllers/observability/#adding-metrics","title":"Adding Metrics","text":"<p>This is the most verbose part of instrumentation because it introduces the need for a webserver, along with data modelling choices and library choices.</p> <p>There are multiple libraries that you can use here;</p> <ul> <li>tikv's prometheus library :: most battle tested library available, lacks newer features</li> <li>prometheus/client_rust :: official, newish. supports exemplars.</li> <li>measured :: very new, client-side cardinality control and memory optimisations</li> </ul> <p>While controller-rs uses client_rust to support exemplars, this tutorial will use <code>tikv/rust-prometheus</code> for now:</p> <pre><code>cargo add prometheus\n</code></pre>"},{"location":"controllers/observability/#registering","title":"Registering","text":"<p>We will start creating a basic <code>Metrics</code> struct to house two metrics, a histogram and a counter:</p> <pre><code>#[derive(Clone)]\npub struct Metrics {\n    pub reconciliations: IntCounter,\n    pub failures: IntCounterVec,\n    pub reconcile_duration: HistogramVec,\n}\n\nimpl Default for Metrics {\n    fn default() -&gt; Self {\n        let reconcile_duration = HistogramVec::new(\n            histogram_opts!(\n                \"doc_controller_reconcile_duration_seconds\",\n                \"The duration of reconcile to complete in seconds\"\n            )\n            .buckets(vec![0.01, 0.1, 0.25, 0.5, 1., 5., 15., 60.]),\n            &amp;[],\n        )\n        .unwrap();\n        let failures = IntCounterVec::new(\n            opts!(\n                \"doc_controller_reconciliation_errors_total\",\n                \"reconciliation errors\",\n            ),\n            &amp;[\"instance\", \"error\"],\n        )\n        .unwrap();\n        let reconciliations =\n            IntCounter::new(\"doc_controller_reconciliations_total\", \"reconciliations\").unwrap();\n        Metrics {\n            reconciliations,\n            failures,\n            reconcile_duration,\n        }\n    }\n}\n</code></pre> <p>and as these metrics are measurable entirely from within <code>reconcile</code> or <code>error_policy</code> we can attach the struct to the context passed to the reconciler##using-context.</p>"},{"location":"controllers/observability/#measuring","title":"Measuring","text":"<p>Measuring our metric values can then be done by explicitly taking a <code>Duration</code>  inside <code>reconcile</code>, but it is easier to wrap this in a struct that relies on <code>Drop</code> with a convenience constructor:</p> <pre><code>pub struct ReconcileMeasurer {\n    start: Instant,\n    metric: HistogramVec,\n}\n\nimpl Drop for ReconcileMeasurer {\n    fn drop(&amp;mut self) {\n        let duration = self.start.elapsed().as_millis() as f64 / 1000.0;\n        self.metric.with_label_values(&amp;[]).observe(duration);\n    }\n}\n\nimpl Metrics {\n    pub fn count_and_measure(&amp;self) -&gt; ReconcileMeasurer {\n        self.reconciliations.inc();\n        ReconcileMeasurer {\n            start: Instant::now(),\n            metric: self.reconcile_duration.clone(),\n        }\n    }\n}\n</code></pre> <p>and call this from <code>reconcile</code> with one line:</p> <pre><code>async fn reconcile(foo: Arc&lt;Foo&gt;, ctx: Arc&lt;Context&gt;) -&gt; Result&lt;Action, Error&gt; {\n    let _timer = ctx.metrics.count_and_measure(); // increments now\n\n    // main reconcile body here\n\n    Ok(...) // drop impl invoked, computes time taken\n}\n</code></pre> <p>and handle the <code>failures</code> metric inside your  <code>error_policy</code>:</p> <pre><code>fn error_policy(doc: Arc&lt;Document&gt;, error: &amp;Error, ctx: Arc&lt;Context&gt;) -&gt; Action {\n    warn!(\"reconcile failed: {:?}\", error);\n    ctx.metrics.reconcile_failure(&amp;doc, error);\n    Action::requeue(Duration::from_secs(5 * 60))\n}\n\nimpl Metrics {\n    pub fn reconcile_failure(&amp;self, doc: &amp;Document, e: &amp;Error) {\n        self.failures\n            .with_label_values(&amp;[doc.name_any().as_ref(), e.metric_label().as_ref()])\n            .inc()\n    }\n}\n</code></pre> <p>We could increment the failure metric directly, but we have also made a helper function stashed away that extracts the object name and a short error name as labels for the metric.</p> <p>This type of error extraction requires an impl on your <code>Error</code> type. We use <code>Debug</code> here:</p> <pre><code>impl Error {\n    pub fn metric_label(&amp;self) -&gt; String {\n        format!(\"{self:?}\").to_lowercase()\n    }\n}\n</code></pre> <p>Exemplars linking Logs and Traces</p> <p>In controller-rs (using prometheus_client) we attached our <code>trace_id</code> to the histogram metric - through <code>count_and_measure</code> - to be able to cross-browse from grafana metric panels into a trace-viewer. See this comment for more info.</p>"},{"location":"controllers/observability/#exposing","title":"Exposing","text":"<p>For prometheus to obtain our metrics, we require a web server. As per the webserver guide, we will assume actix-web.</p> <p>In our case, we will pass a <code>State</code> struct that contains the <code>Metrics</code> struct and attach it to the <code>HttpServer</code> in <code>main</code>:</p> <pre><code>HttpServer::new(move || {\n    App::new()\n        .app_data(Data::new(state.clone())) // new state\n        .service(metrics) // new endpoint\n    })\n</code></pre> <p>the <code>metrics</code> service is the important one here, and its implementation is able to extract the <code>Metrics</code> struct from actix's <code>web::Data</code>:</p> <pre><code>#[get(\"/metrics\")]\nasync fn metrics(c: web::Data&lt;State&gt;, _req: HttpRequest) -&gt; impl Responder {\n    let metrics = c.metrics(); // grab out of actix data\n    let encoder = TextEncoder::new();\n    let mut buffer = vec![];\n    encoder.encode(&amp;metrics, &amp;mut buffer).unwrap();\n    HttpResponse::Ok().body(buffer)\n}\n</code></pre>"},{"location":"controllers/observability/#what-metrics","title":"What Metrics","text":"<p>The included metrics <code>failures</code>, <code>reconciliations</code> and a <code>reconcile_duration</code> histogram will be sufficient to have prometheus compute a wide array of details:</p> <ul> <li>reconcile amounts in last hour - <code>sum(increase(reconciliations[1h]))</code></li> <li>hourly error rates - <code>sum(rate(failures[1h]) / sum(rate(reconciliations[1h]))</code></li> <li>success rates - same rate setup but <code>reconciliations / (reconciliations + failures)</code></li> <li>p90 reconcile duration - <code>histogram_quantile(0.9, sum(rate(reconciliations[1h])))</code></li> </ul> <p>and you could then create alerts on aberrant values (e.g. say 10% error rate, zero reconciliation rate, and maybe p90 durations &gt;30s).</p> <p>The above metric setup should comprise the core need of a standard controller (although you may have more things to care about than our simple example).</p> <p>kube-state-metrics</p> <p>It is possible to derive metrics from conditions and fields in your CRD schema using runtime flags to <code>kube-state-metrics</code> without instrumentation, but since this is an implicit dependency for operators, it should not be a default.</p> <p>You will also want resource utilization metrics, but this is typically handled upstream. E.g. cpu/memory utilization metrics are generally available via kubelet's metrics and other utilization metrics can be gathered from node_exporter.</p> <p>tokio-metrics</p> <p>New experimental runtime metrics are also availble for the tokio runtime via tokio-metrics.</p>"},{"location":"controllers/observability/#external-references","title":"External References","text":"<ul> <li>Metrics in axum using metrics crate</li> </ul>"},{"location":"controllers/optimization/","title":"Optimization","text":"<p>This document aims to help optimize various factors of resource consumption by controllers, and it will effectively be a guide on how to reduce the usage of your watcher streams to simplify things for downstream consumers.</p>"},{"location":"controllers/optimization/#watcher-optimization","title":"Watcher Optimization","text":"<p>One of the biggest contributor to activity in a Controller is the constant, long-polling watch of the min object and every related object by the use of multiple watcher streams created with some variant of:</p> <pre><code>let cfg = watcher::Config { RESTRICTIONS };\nlet stream = SOME_WATCHER(cfg).SOME_MODIFICATION();\n</code></pre> <p>By default, watcher streams are implicitly configured within the Controller, but using the controller streams interface outlined in the streams chapter, you can customize every aspect of these streams.</p> <p>The central premise herein is that controllers benefit from customizing the input streams and we will show how to do these configurations.</p> <p>Streams Interface Required</p> <p>The watcher optimizations here that directly call <code>watcher</code> / <code>metadata_watcher</code> require the unstable streams interface to interface with controllers.</p> <p>Better watcher configuration primarily aim to reduce IO or networked traffic. However, when used in combination with reflector caches, they also become memory optimizations.</p>"},{"location":"controllers/optimization/#reducing-number-of-watched-objects","title":"Reducing Number of Watched Objects","text":"<p>The default <code>watcher::Config</code> will watch every object in the Api scope you configured:</p> <ul> <li><code>Api::namespaced</code> or <code>Api::default_namespaced</code> -&gt; all objects in that namespace</li> <li><code>Api::all</code> -&gt; all cluster scoped objects (or all objects in all namespaces)</li> </ul> <p>This is easiest to do with label selectors of field selectors.</p> <p>Field selectors can be used to filter on certain common properties:</p> <pre><code>let cfg = watcher::Config::default().fields(&amp;format!(\"metadata.name={name}\"));\n</code></pre> <p>And allows explicitly excluding lists of namespaces:</p> <pre><code>let ignoring_system_namespaces = [\n    \"cert-manager\",\n    \"flux2\",\n    \"linkerd\",\n    \"linkerd-jaeger\",\n    \"linkerd-smi\",\n    \"linkerd-viz\",\n    \"gatekeeper-system\",\n    \"kube-node-lease\",\n    \"kube-public\",\n    \"kube-system\",\n]\n.into_iter()\n.map(|ns| format!(\"metadata.namespace!={ns}\"))\n.collect::&lt;Vec&lt;_&gt;&gt;()\n.join(\",\");\nlet cfg = watcher::Config::default().fields(&amp;ignoring_system_namespaces);\n</code></pre> <p>Field Selector Limitations</p> <p>Due to field-selector limitations, you cannot filter on arbitrary fields, nor can you do set operations, forcing explicit negative enumeration as one of the few helpful tricks.</p> <p>A more general solution involves explicitly labelling the objects you care about, and using the much more expressive label selectors instead:</p> <pre><code>let cfg = Config::default().labels(\"environment in (production, qa)\");\n</code></pre>"},{"location":"controllers/optimization/#backoff-on-errors","title":"Backoff on Errors","text":"<p>Implicitly created controller streams are always created with a DefaultBackoff <code>Backoff</code> implementation (from backoff) similar to the defaults set by client-go to avoid hammering the apiserver when errors occur.</p> <p>Use backoffs</p> <p>A watcher that does not call <code>.default_backoff()</code> or setup any other form of <code>.backoff(b)</code> will continuously loop on errors. This generally spams your logs/traces, and can starve your controller and your apiserver of resources. On newer Kubernetes versions, such behaviour can get you throttled.</p> <p>When going from implicit controller streams to manually created streams, remember to add <code>.default_backoff()</code> at some point in the stream chain:</p> <pre><code>let stream = watcher(api, cfg).default_backoff();\n</code></pre>"},{"location":"controllers/optimization/#watching-metadata-only","title":"Watching Metadata Only","text":"<p>If you do not need all the data on the resource you are watching, then you should consider using the streams#Metadata-Watcher to reduce the amount of data being handled by your watch stream.</p> <p>Metadata is generally responsible for a small amount of the total object size, so you can often expect a commensurate reduction in incoming bandwidth by only watching metadata. When these watches are sent through a reflector they will also significantly reduce the reflector memory footprint:</p> <p></p> <p>Note that this memory reduction can also be achieved through #pruning-fields - and you can potentially omit even more data that way by combining the approaches - but pruned data is still sent on the wire.</p> <p>There are three pathways to consider when changing controller inputs to metadata watchers:</p>"},{"location":"controllers/optimization/#1-changing-associated-streams","title":"1. Changing Associated Streams","text":"<p>When watching an associated stream (via relations) that is only used for mapping and not stored.</p> <p>If you are using normal watchers for these, then there is potentially a lot of unused data go on the wire. <code>Controller::owns_stream</code> should use <code>metadata_watcher</code> in general (ownerReference lookup only needs metadata), and <code>Controller::watches_stream</code> can also do this if its lookup is based on metadata properties. See streams#input-streams.</p>"},{"location":"controllers/optimization/#2-changing-primary-streams","title":"2. Changing Primary Streams","text":"<p>When only metadata properties are acted on (e.g. you have a controller that only acts on labels/annotations or similar).</p> <p>In this case you can replace the streams#main-stream for a significant memory reduction from the mandatory reflector. Do note the caveat that this changes the type signature of your reconciler slightly.</p>"},{"location":"controllers/optimization/#3-changing-predicated-primary-streams","title":"3. Changing Predicated Primary Streams","text":"<p>When running a controller with predicate_filters to limit the amount of reconciler hits, and simultaneously not using the cache for anything important.</p> <p>In this case you can actually also follow the streams#main-stream pattern and call <code>Api::get</code> on the object name when the <code>reconcile</code> function actually calls so you get an up-to-date object. Remember that if you are using predicates on the long watch, then you can quickly discard many changes without requiring all of the data being transmitted.</p> <p>Such a setup can involve running the metadata watcher with the cheaper/less consistent any_semantic because your reconciler only happens every so often, but when it does, you will do the work properly:</p> <pre><code>let deploys: Api&lt;Deployment&gt; = Api::all(client);\nlet cfg = watcher::Config::default().any_semantic();\nlet (reader, writer) = reflector::store();\nlet stream = reflector(writer, metadata_watcher(api, cfg))\n    .default_backoff()\n    .applied_objects()\n    .predicate_filter(predicates::generation);\n\nController::for_stream(stream, reader)...\n\nasync fn reconcile(partial: Arc&lt;PartialObjectMeta&lt;Deployment&gt;&gt;, ctx: Arc&lt;Context&gt;) -&gt; Result&lt;Action, Error&gt;\n{\n    // if we are here, then we have a changed generation - fetch it fully\n    let api: Api&lt;Deployment&gt; = Api::namespaced(partial.namespace(), ctx.client.clone());\n    let deploy = api.get(&amp;partial.name_any()).await?;\n\n    unimplemented!()\n}\n</code></pre> <p>This will also help avoid #Repeatedly Triggering Yourself.</p>"},{"location":"controllers/optimization/#reduce-the-list-spike","title":"Reduce the List Spike","text":"<p>Every permanent watch loop setup against kube-apiserver generally requires an initial listing of objects (usually in the form of a <code>list</code> api call), followed by a long <code>watch</code>starting at the given <code>resourceVersion</code> from the initial list. On big clusters this listing was until recently quite memory intensive (for both the apiserver and the controller).</p> <p>Pagination for this initial list was added for <code>0.84.0</code> - with a new default default page size matching client-go's <code>500</code>. This can be reduced further when working on large objects if needed:</p> <pre><code>let cfg = watcher::Config::default().page_size(50);\n</code></pre> <p>A more efficient alternative was added in <code>0.86.0</code> and avoids paging entirely using the streaming WatchList feature. This is supported through Config::initial_list_strategy, and can be sanity tested in the pod_watcher example on a cluster with the feature gate enabled.</p> <pre><code>let cfg = watcher::Config::default().streaming_lists();\n</code></pre> <p>Both these methods should reduce the peak memory footprint of both the apiserver and your controller at the times the controller needs to do a re-list.</p>"},{"location":"controllers/optimization/#reflector-optimization","title":"Reflector Optimization","text":"<p>Unless you have another large in-memory cache or other similar memory users in your controller, the primary contributor to your controller's memory use is going to be the mandatory reflector for the main object as well as any other optional reflectors for related objects.</p> <p>The memory usage of reflectors is directly proportional to how much stuff you put in it, and can be minimized by tweaking a number of properties:</p> <ol> <li>Amount of objects watched (<code>watcher::Config</code>)</li> <li>Asking for only metadata (<code>metadata_watcher</code>)</li> <li>Pruning unnecessary fields before storing (<code>WatchStreamExt</code>)</li> </ol> <p>We have already talked about the first two points (#Reducing Number of Watched Objects and #Watching Metadata Only) above as these have IO benefits for watchers on their own, but also cause memory usage reductions by forcing less stored objects/data.</p>"},{"location":"controllers/optimization/#pruning-fields","title":"Pruning Fields","text":"<p>By default, the memory stored for each object is equivalent to what you get from asking <code>kubectl</code> for all objects matching your <code>ListParams</code>, but additionally asking for <code>--show-managed-fields</code> which <code>kubectl</code> hides from you by default, but is always part of any underlying api based request.</p> <p>Most controllers do not need to know about the specifics of these, and they should usually be pruned pre-insertion:</p> <pre><code>let api: Api&lt;Pod&gt; = Api::default_namespaced(client);\nlet stream = watcher(pods, watcher::Config::default()).modify(|pod| {\n    // memory optimization for our store - we don't care about fields/annotations/status\n    pod.managed_fields_mut().clear();\n    pod.annotations_mut().clear();\n    pod.status = None;\n});\nlet (reader, writer) = reflector::store::&lt;Pod&gt;();\nlet rf = reflector(writer, stream).applied_objects();\n</code></pre> <p>In the above example we also clear out the status object and annotations entirely pre-storage.</p> <p>This can be effective, even when already using metadata_watcher, as managed-fields often accounts for close to half of the metadata yaml. Here is the result of clearing only managed fields from a roughly ~2000 object metadata reflector cache:</p> <p></p> <p>For reference, here is the full graph showing the effect of first switching from <code>watcher</code> to <code>metadata_watcher</code> (which effectively acts as a smarter way to prune the Spec/Status) on a 2000 object <code>Pod</code> reflector that adds managed field pruning on top:</p> <p></p> <p>Except for some of the .metadata properties  pruning can generally be done for all the fields you do not care about.</p> <p>Pruning ObjectMeta</p> <p>Do not prune everything from ObjectMeta as <code>kube::runtime</code> relies on being able to see <code>.metadata.name</code>, <code>.metadata.resourceVersion</code>, <code>.metadata.namespace</code> and in some cases <code>.metadata.ownerReferences</code>.</p> <p>Pruning will not reduce your network traffic</p> <p>All object data passed through a <code>watcher</code> is always ultimately received over the wire from <code>kube-apiserver</code>. Pruning is only local truncation.</p>"},{"location":"controllers/optimization/#reconciler-optimization","title":"Reconciler Optimization","text":"<p>The reconciler is generally the entry-point for your business logic, so we cannot give too much blanket advice on optimizing this, but we can give a few pointers.</p> <p>Note that instrumenting standard metrics (observability#what-metrics) on your reconciler, and sending traces of more complicated microservice interactions to a trace collector (observability#instrumenting) are good observability practices, and can go a long way in identifying performance issues.</p>"},{"location":"controllers/optimization/#repeatedly-triggering-yourself","title":"Repeatedly Triggering Yourself","text":"<p>AKA the problem that you will most easily run into; a reconciler that modifies the status (say) of its main object will cause a change in that object that is picked up by the Controller's watcher loop, and will be fed back into the reconciler.</p> <p>This is normally not a problem, because if your status patch that causes this change is idempotent, it will only happen once. The problem is when you start putting non-deterministic values inside the the <code>.status</code> resource (e.g. timestamps rather than hashes).</p> <p>In such cases, the controller will spin forever on such objects.</p> <p>Detecting spinlocks</p> <p>Spinlocks are usually noticeable quickly by just running the controller locally and watching logs for one object, or having a plot graph on your reconciler rate (observability#what-metrics) in <code>grafana</code>. You should expect about 1 to 5+ reconciles every <code>1/(your requeue time)</code> growing with the number of affected objects and self-interaction.</p> <p>The two ways to avoid reconciler re-triggering are:</p> <ol> <li>don't patch in non-deterministic values to your object (breaks idempotency)</li> <li>filter out changes to irrelevant parts of your object</li> </ol> <p>These approaches are both recommended because they both have independent merits;</p> <ol> <li>idempotency is good, and it avoids the spin problem the \"good practice way\"</li> <li>early filtering can allow for more precise reconcile bypasses with less code complexity, and allow opt-in non-determinism (such as timestamp fields)</li> </ol> <p>Watch events can be filtered out early using predicates with WatchStreamExt::predicate_filter, and passing on these pre-filtered streams to controllers:</p> <pre><code>let deploys: Api&lt;Deployment&gt; = Api::default_namespaced(client);\nlet changed_deploys = watcher(deploys, watcher::Config::default())\n    .applied_objects()\n    .predicate_filter(predicates::generation);\n</code></pre>"},{"location":"controllers/optimization/#debouncing-repetitions","title":"Debouncing Repetitions","text":"<p>After kube 0.86 (via #1265) it is possible to debounce to filter out reconcile calls that happen quick succession (only taking the latest). A debounce time can be set on the controller::Config, and will introduce a delay between the observed event and the eventual reconcile call, triggering only after no relevant events have been seen for the debounce period.</p> <pre><code>Controller::new(pods, watcher::Config::default())\n    .with_config(controller::Config::default().debounce(Duration::from_secs(5)))\n</code></pre> <p>A common example is to lessen the noise from rapid phase transitions of <code>Pod</code> watches (which receives several repeat status updates after initial application). If your controller only treats pods as a watched resource and does not need to react to every status update, then introducing a handful of seconds of debounce time can help reduce reconciler load:</p> <p></p> <p>The graph shows how a <code>5s</code> debounce acts on reconciliation rates for various workloads (via 4 different controllers) in a cluster with ~1500 pods (the last dashed line indicates a deployment of the controller that enabled debounce).</p> <p>Debounce effect is situational and will slow down responsiveness</p> <p>As can be seen from the generally completely unaffected other kinds in the above graph, the effect may not benefit all controllers, and a downside is that it will slow down the controller's average response time to changes. A few seconds can be helpful while waiting for other eventually-consistent components, but beyond that you should consider the tradeoff between the consistency loss and the cost of re-running.</p> <p>See the documentation for controller::Config for more information.</p>"},{"location":"controllers/optimization/#reconciler-concurrency","title":"Reconciler Concurrency","text":"<p>The controller will schedule different objects concurrently. For example, for the event queue ABA, we'd currently schedule object A and B to reconcile concurrently, and start the second reconciliation of A as soon as the first one finishes. Our guarantee here is that kube will never run two reconciliations for the same object concurrently. This means that default kube behaviour is infinite concurrency whereas <code>controller-runtime</code> has no default concurrency.</p> <p>On large datasets, spiky workloads can occur with this default. Big datasets being funnelled through the <code>watcher</code> and into the reconciler can cause large amount of work, and when using consistent requeue times, that same spike is likely to show up later:</p> <p></p> <p>To flatten such a workload curve, consider limiting your <code>concurrency</code> to control how much work can happen at the same time.</p> <pre><code>Controller::new(pods, watcher::Config::default())\n    .with_config(controller::Config::default().concurency(3))\n</code></pre> <p>Reconciler Deduplication</p> <p>Multiple repeat reconciliations for the same object are deduplicated if they are queued but haven't been started yet. For example, a queue for two objects that is meant to run <code>ABABAAAAA</code> will be deduped into a shorter queue <code>ABAB</code>, assuming that <code>A1</code> and <code>B1</code> are still executing when the subsequent entries come in.</p>"},{"location":"controllers/optimization/#future-optimizations","title":"Future Optimizations","text":"<p>Not everything is possible to optimize yet. Some tracking issues:</p> <ul> <li>Sharing watcher streams and caches between Controllers</li> </ul>"},{"location":"controllers/optimization/#summary","title":"Summary","text":"<p>As a short summary, here are the main listed optimization and the effect you should expect to see from utilising them.</p> Optimization Type Target Reduction metadata_watcher IO + Memory watcher selectors IO + Memory watcher page size Peak Memory reconciler concurrency Peak Usage pruning Memory Usage predicates Memory + Code Size <p>It is important to note that most of these are watcher tweaks. The target reductions above can all be granted by passing more precise streams to your controller, or by tweaking controller input parameters.</p>"},{"location":"controllers/reconciler/","title":"The Reconciler","text":"<p>The reconciler is the user-defined function in charge of reconciling the state of the world.</p> <pre><code>async fn reconcile(o: Arc&lt;K&gt;, ctx: Arc&lt;T&gt;) -&gt; Result&lt;Action, Error&gt;\n</code></pre> <p>It is always called with the object type that you instantiate the Controller with, independent of what auxiliary objects you may be watching:</p> <pre><code>graph TD\n    K{{Kubernetes}} --&gt;|changes| W(watchers)\n    W --&gt;|correlate| A(applier)\n    A --&gt;|run| R(reconciler)\n    R --&gt;|Update| X{{World}}\n    R -.-&gt;|result| A\n    subgraph \"Controller\"\n    W\n    A\n    end\n    subgraph \"Application\"\n    Controller\n    R\n    end</code></pre> <p>A Controller is a system that will:</p> <ol> <li>watch resources from the Kubernetes api (main + related objects)</li> <li><code>map</code> returned objects (via relations) into your main object</li> <li>schedule and run reconciliations</li> <li>observe the result of reconciliations to decide when to reschedule</li> </ol> <p>Details about this system is explored in internals and architecture.</p> <p>You must instantiate a Controller in your application, and define your <code>reconcile</code> + <code>error_policy</code> fns.</p>"},{"location":"controllers/reconciler/#the-world","title":"The World","text":"<p>The state of the world is your main Kubernetes object along with anything your reconciler touches.</p> <p>The World &gt;= Kubernetes</p> <p>While your main object must reside within Kubernetes, it is possible to manage/act on changes outside Kubernetes.</p> <p>You do not have to configure the world, as any side effect you perform implicitly becomes the world for your controller. It is, however, beneficial to specify any relations your object has with the world to ensure <code>reconcile</code> is correctly invoked:</p>"},{"location":"controllers/reconciler/#what-triggers-reconcile","title":"What triggers reconcile","text":"<p>The reconciler is invoked - for an instance of your <code>object</code> - if:</p> <ul> <li>that main object changed</li> <li>an owned object (with ownerReferences to the main object) changed</li> <li>a related object/api (pointing to the main object) changed</li> <li>the object had failed reconciliation before and was requeued earlier</li> <li>the object received an infrequent periodic reconcile request</li> </ul> <p>In other words; <code>reconcile</code> will be triggered periodically (infrequently), and immediately upon changes to the main object, or related objects.</p> <p>It is therefore beneficial to configure relations so the Controller will know what counts as a reconcile-worthy change.</p> <p>Typically, this is accomplished with a call to Controller::owns on any owned Api and ensuring ownerReferences are created in your reconciler. See relations for details.</p>"},{"location":"controllers/reconciler/#reasons-for-reconciliation","title":"Reasons for reconciliation","text":"<p>Notice that the ReconcileReason is not included in the signature of <code>reconcile</code>; you only get the object. The reason for this omission is to encourage fault-tolerance:</p> <p>Fault-tolerance against missed messages</p> <p>If your controller has downtime you can miss messages. Additionally, the Kubernetes watch api does not guarantee delivery. You will likely miss some messages, and when this happens, Kubernetes will coalesce the ones you have not received into one single message. </p> <p>As a result, the ReconcileReason is a debugging best-effort property (for telemetry). You should not attempt to write logic against the reason in your reconciler.</p> <p>Instead, you must write a defensive reconciler and handle missed messages, and partial runs:</p> <ul> <li>assume nothing about why reconciliation started</li> <li>assume the reconciler could have failed at any question mark</li> <li>check every property independently (you always start at the beginning)</li> </ul> <p>The type of defensive function writing described above is intended to grant a formal property called idempotency.</p>"},{"location":"controllers/reconciler/#idempotency","title":"Idempotency","text":"<p>A function is said to be idempotent if it can be applied multiple times without changing the result beyond the initial application.</p> <p>A reconciler must be idempotent</p> <p>If a reconciler is triggered twice for the same object, it must cause the same outcome. Care must be taken to ensure operations are not dependent on all-or-nothing approaches, and the flow of the reconciler must be able to recover from errors occurring in previous reconcile runs.</p> <p>Let us create a reconciler for a custom <code>PodManager</code> resource that will:</p> <ul> <li>create an associated <code>Pod</code> with ownerReferences</li> <li>note its <code>creation</code> time on the status object of <code>PodManager</code></li> </ul> <p>Both of these operations are idempotent by themselves, but you must be careful with how you combine them.</p>"},{"location":"controllers/reconciler/#combining-idempotent-operations","title":"Combining Idempotent Operations","text":"<p>The naive approach to the above problem would be to check if the work has been done, and if not, do it:</p> <pre><code>if pod_missing {\n    create_owned_pod()?;\n    set_timestamp_on_owner()?;\n}\n</code></pre> <p>Now, what happens if the timestamp creation fails after the pod got created? The second action will never get done!</p> <p>Reconciler interruptions</p> <p>If your reconciler errored half-way through a run; the only way you would know what failed, is if you check everything.</p> <p>Therefore the correct way to do these two actions is to do them independently:</p> <pre><code>if pod_missing {\n    create_owned_pod()?;\n}\nif is_timestamp_missing() {\n    set_timestamp_on_owner()?;\n}\n</code></pre>"},{"location":"controllers/reconciler/#in-depth-solution","title":"In-depth Solution","text":"<p>Let's suppose we have access to an <code>Api&lt;Pod&gt;</code> and an <code>Api&lt;PodManager&gt;</code> (via a context):</p> <pre><code>let api: Api&lt;PodManager&gt; = ctx.api.clone();\nlet pods: Api&lt;Pod&gt; = Api::default_namespaced(ctx.client.clone());\n</code></pre> <p>and let's define the <code>Pod</code> we want to create as:</p> <pre><code>fn create_owned_pod(source: &amp;PodManager) -&gt; Pod {\n    let oref = source.controller_owner_ref(&amp;()).unwrap();\n    Pod {\n        metadata: ObjectMeta {\n            name: source.metadata.name.clone(),\n            owner_references: Some(vec![oref]),\n            ..ObjectMeta::default()\n        },\n        spec: my_pod_spec(),\n        ..Default::default()\n    }\n}\n</code></pre> <p>one approach of achieving idempotency is to check every property carefully::</p> <pre><code>// TODO: find using ownerReferences instead - has to be done using jsonpath...\n// {range .items[?(.metadata.ownerReferences.uid=262bab1a-1c79-11ea-8e23-42010a800016)]}{.metadata.name}{end}\n// make a helper for this?\nlet podfilter = ListParams::default()\n    .labels(format!(\"owned-by/{}\", obj.name_any()));\n\n// if owned pod is not created, do the work to create it\nlet pod: Pod = match &amp;pods.list(&amp;podfilter).await?[..] {\n    [p, ..] =&gt; p, // return the first found pod\n    [] =&gt; {\n        let pod_data = create_owned_pod(&amp;obj);\n        pods.create(pod_data).await? // return the new pod from the apiserver\n    },\n};\n\nif obj.status.pod_created.is_none() {\n    // update status object with the creation_timestamp of the owned Pod\n    let status = json!({\n        \"status\": PodManagerStatus { pod_created: pod.meta().creation_timestamp }\n    });\n    api.patch_status(&amp;obj.name_any(), &amp;PatchParams::default(), &amp;Patch::Merge(&amp;status))\n        .await?;\n}\n</code></pre> <p>but we can actually simplify this significantly by taking advantage of idempotent Kubernetes apis:</p> <pre><code>let pod_data = create_owned_pod(&amp;obj);\nlet serverside = PatchParams::apply(\"mycontroller\");\nlet pod = pods.patch(pod.name_any(), serverside, Patch::Apply(pod_data)).await?\n\n// update status object with the creation_timestamp of the owned Pod\nlet status = json!({\n    \"status\": PodManagerStatus { pod_created: pod.meta().creation_timestamp }\n});\napi.patch_status(&amp;obj.name_any(), &amp;PatchParams::default(), &amp;Patch::Merge(&amp;status))\n    .await?;\n</code></pre> <p>Here we are taking advantage of Server-Side Apply and deterministic naming of the owned pod to call the equivalent of <code>kubectl apply</code> on the <code>pod_data</code>.</p> <p>The <code>patch_status</code> is already idempotent, and does not technically need the pre-check. However, we might wish to keep the check, as this will lead to less networked requests.</p>"},{"location":"controllers/reconciler/#using-context","title":"Using Context","text":"<p>To do anything useful inside the reconciler like persisting your changes, you typically need to inject some client in there.</p> <p>The way this is done is through the context parameter on Controller::run. It's whatever you want, packed in an <code>Arc</code>.</p> <pre><code>// Context for our reconciler\n#[derive(Clone)]\nstruct Data {\n    /// kubernetes client\n    client: Client,\n    /// In memory state\n    state: Arc&lt;RwLock&lt;State&gt;&gt;,\n}\n\nlet context = Arc::new(Data {\n    client: client.clone(),\n    state: state.clone(),\n});\nController::new(foos, watcher::Config::default())\n    .run(reconcile, error_policy, context)\n</code></pre> <p>then you can pull out your user defined struct (here <code>Data</code>) items inside <code>reconcile</code>:</p> <pre><code>async fn reconcile(object: Arc&lt;MyObject&gt;, ctx: Arc&lt;Data&gt;) -&gt; Result&lt;Action, Error&gt; {\n    ctx.state.write().await.last_event = Utc::now();\n    let reporter = ctx.state.read().await.reporter.clone();\n    let objs: Api&lt;MyObject&gt; = Api::all(ctx.client.clone());\n    // ...\n    Ok(Action::await_change())\n}\n</code></pre>"},{"location":"controllers/reconciler/#cleanup","title":"Cleanup","text":"<p>If you have dependencies, you should configure some form of gc.</p>"},{"location":"controllers/reconciler/#instrumentation","title":"Instrumentation","text":"<p>The root <code>reconcile</code> function should be instrumented with logs, traces and metrics.</p> <p>See the observability document for how to add good instrumentation to your <code>reconcile</code> fn.</p>"},{"location":"controllers/reconciler/#diagnostics","title":"Diagnostics","text":"<p>WIP. Separate document for posting diagnostic events to the events api + using the status object.</p>"},{"location":"controllers/relations/","title":"Related Objects","text":"<p>A Controller needs to specify related resources if changes to them are meant to trigger the reconciler.</p> <p>These relations are generally set up with Controller::owns, but we will go through the different variants below.</p>"},{"location":"controllers/relations/#owned-relation","title":"Owned Relation","text":"<p>The Controller::owns relation is the most straight-forward and most ubiquitous one. One object controls the lifecycle of a child object, and cleanup happens automatically via gc#Owner-References.</p> <pre><code>let cmgs = Api::&lt;ConfigMapGenerator&gt;::all(client.clone());\nlet cms = Api::&lt;ConfigMap&gt;::all(client.clone());\n\nController::new(cmgs, watcher::Config::default())\n    .owns(cms, watcher::Config::default())\n</code></pre> <p>This configmapgen example uses one custom resource <code>ConfigMapGenerator</code> whose controller is in charge of the lifecycle of the child <code>ConfigMap</code>.</p> <ul> <li>What happens if we delete a <code>ConfigMapGenerator</code> instance here? Well, there will be a <code>ConfigMap</code> with ownerReferences matching the <code>ConfigMapGenerator</code> so Kubernetes will automatically cleanup the associated <code>ConfigMap</code>.</li> <li>What happens if we modify the managed <code>ConfigMap</code>? The Controller sees a change and associates the change with the owning <code>ConfigMapGenerator</code>, ultimately triggering a reconciliation of the root <code>ConfigMapGenerator</code>.</li> </ul> <p>This relation relies on ownerReferences being created on the managed/owned objects for Kubernetes automatic cleanup, and the Controller relies on it for association with its owner.</p> <p>Streams Variant</p> <p>To configure or share the watcher for the owned resource, see streams#owned-stream.</p>"},{"location":"controllers/relations/#watched-relations","title":"Watched Relations","text":"<p>The Controller::watches relation is for related Kubernetes objects without ownerReferences, i.e. without a standard way for the controller to map the object to the root object. Thus, you need to define this mapper yourself:</p> <pre><code>let main = Api::&lt;MainObj&gt;::all(client);\nlet related = Api::&lt;RelatedObject&gt;::all(client);\n\nlet mapper = |obj: RelatedObject| {\n    obj.spec.object_ref.map(|oref| {\n        ReconcileRequest::from(oref)\n    })\n};\n\nController::new(main, watcher::Config::default())\n    .watches(related, watcher::Config::default(), mapper)\n</code></pre> <p>In this case, we are extracting an object reference from the spec of our object. Regardless of how you get the information, your mapper must return an iterator of ObjectRef for the root object(s) that must be reconciled as a result of the change.</p> <p>As a theoretical example; every HPA object bundles a scale ref to the workload, so you could use this to build a Controller for <code>Deployment</code> using HPA as a watched object.</p> <p>Streams Variant</p> <p>To configure or share the watcher for watched resource, see streams#watched-stream.</p>"},{"location":"controllers/relations/#external-relations","title":"External Relations","text":"<p>Free-form relations to external apis often serve to lift an external resource into your cluster via either a <code>ConfigMap</code> or a CRD (see the tradeoff table). This relation can go in both directions.</p>"},{"location":"controllers/relations/#external-watches","title":"External Watches","text":"<p>If you want changes on an external API to cause changes in the cluster, you will need to a way to stream changes from the external api.</p> <p>The change events must be provided as a <code>Stream&lt;Item = ObjectRef&gt;</code> and passed to Controller::reconcile_on. As an example:</p> <pre><code>struct ExternalObject {\n    name: String,\n}\nlet external_stream = watch_external_objects().map(|ext| {\n    ObjectRef::new(&amp;ext.name).within(&amp;ns)\n});\n\nController::new(Api::&lt;MyCr&gt;::namespaced(client, &amp;ns), Config::default())\n    .reconcile_on(external_stream)\n</code></pre> <p>In this case, we have some opaque <code>fn watch_external_objects()</code> which here returns <code>-&gt; impl Stream&lt;Item = ExternalObject&gt;</code>. It is meant to return changes from the external API. Whenever a new item is found on the stream, the controller will reconcile the matching cluster object.</p> <p>(The example assumes matching names between the external resource and cluster resource, and a fixed namespace for the cluster resources.)</p> <p>Streaming Interface</p> <p>If you do not have a streaming interface (like if you are doing periodic HTTP GETs), you can wrap your data in a <code>Stream</code> via either async_stream or by using channels (say tokio::sync::mpsc, using the Receiver side as a stream).</p>"},{"location":"controllers/relations/#external-writes","title":"External Writes","text":"<p>If you want to populate an external API from a cluster resource, you must update the external api from your reconciler (using the necessary client libraries for that API).</p> <p>To avoid build-up of generated objects on the external side, you will want to use gc#finalizers, to ensure the external resource gets safely cleaned up on <code>kubectl delete</code>.</p>"},{"location":"controllers/relations/#summary","title":"Summary","text":"<p>Depending on what type of child object and its relation with the main object, you will need the following setup and cleanup:</p> Child Controller relation Setup Cleanup Kubernetes object Owned Controller::owns ownerReferences Kubernetes object Related Controller::watches n/a External API Managed Controller::reconcile_on finalizers External API Related Controller::reconcile_on n/a"},{"location":"controllers/scaling/","title":"Scaling","text":"<p>This chapter is about strategies for scaling controllers and the tradeoffs these strategies make.</p>"},{"location":"controllers/scaling/#motivating-questions","title":"Motivating Questions","text":"<ul> <li>Why is the reconciler lagging? Are there too many resources being reconciled?</li> <li>What happens when your controller starts managing resource sets so large that it starts significantly impacting your CPU or memory use?</li> </ul> <p>Scaling an efficient Rust application that spends most of its time waiting for network changes might not seem like a complicated affair, and indeed, you can scale a controller in many ways and achieve good outcomes. But in terms of costs, not all solutions are created equal:</p> <p>Can you improve your algorithm, or should you throw more expensive machines at the problem?</p>"},{"location":"controllers/scaling/#scaling-strategies","title":"Scaling Strategies","text":"<p>We recommend trying the following scaling strategies in order:</p> <ol> <li>#Controller Optimizations (minimize expensive work to allow more work)</li> <li>#Vertical Scaling (more headroom for the single pod)</li> <li>#Sharding (horizontal scaling)</li> </ol> <p>In other words, try to improve your algorithm first, and once you've reached a reasonable limit of what you can achieve with that approach, allocate more resources to the problem.</p>"},{"location":"controllers/scaling/#controller-optimizations","title":"Controller Optimizations","text":"<p>Ensure you look at common controller optimization to get the most out of your resources:</p> <ul> <li>minimize network intensive operations</li> <li>avoid caching large manifests unnecessarily, and prune unneeded data</li> <li>cache/memoize expensive work</li> <li>checkpoint progress on <code>.status</code> objects to avoid repeating work</li> </ul> <p>When checkpointing, care should be taken to not accidentally break reconciler#idempotency.</p>"},{"location":"controllers/scaling/#vertical-scaling","title":"Vertical Scaling","text":"<ul> <li>increase CPU/memory limits</li> <li>configure controller concurrency (as a multiple of CPU limits)</li> </ul> <p>The controller::Config currently** defaults to unlimited concurrency and may need tuning for large workloads.</p> <p>It is possible to compute an optimal <code>concurrency</code> number based the CPU <code>resources</code> you assign to your container, but this would require specific measurement against your workload.</p> <p>Agressiveness meets fairness</p> <p>A highly parallel reconciler might be eventually throttled by apiserver flow-control rules, and this can clearly degrade your controller's performance. Measurements, calculations, and observability (particularly for error rates) are useful to identifying such scenarios.</p>"},{"location":"controllers/scaling/#sharding","title":"Sharding","text":"<p>If you are unable to meet latency/resource requirements using techniques above, you may need to consider partitioning/sharding your resources.</p> <p>Sharding is splitting your workload into mutually exclusive groups that you grant exclusive access to. In Kubernetes, shards are commonly seen as a side-effect of certain deployment strategies:</p> <ul> <li>sidecars :: pods are shards</li> <li>daemonsets :: nodes are shards</li> </ul> <p>Sidecars and Daemonsets</p> <p>Several big agents use daemonsets and sidecars in situations that require higher than average performance, and is commonly found in network components, service meshes, and sometimes observability collectors that benefit from co-location with a resource. This choice creates a very broad and responsive sharding strategy, but one that incurs a larger overhead using more containers than is technically necessary.</p> <p>Sharding can also be done in a more explicit way:</p> <ul> <li>1 controller deployment per namespace (naive sharding)</li> <li>1 controller deployment per labelled shard (precice, but requires labelling work)</li> </ul> <p>Explicitly labelled shards is less common, but is a powerful option. It is used by fluxcd via their sharding.fluxcd.io/key label to associate a resource with a shard. Flux's Stefan talks about scaling flux controllers at KubeCon 2024.</p> <p>Automatic Labelling</p> <p>A mutating admission policy can help automatically assign/label partitions cluster-wide based on constraints and rebalancing needs.</p> <p>In cases where HA is required, leases can be used gate access to particular shards. See availability#Leader Election</p>"},{"location":"controllers/schemas/","title":"Schemas","text":"<p>The schema is a required part of a CustomResource because the apiserver only accepts a CustomResourceDefinition with a valid schema.</p> <p>There are three main ways to get a schema injected into the CustomResourceDefinition.</p> <ul> <li>derived -&gt; #Deriving-JsonSchema (default)</li> <li>manual -&gt; #Implementing-JsonSchema</li> <li>disabled -&gt; #Disabling-Schemas</li> </ul>"},{"location":"controllers/schemas/#using-jsonschema","title":"Using JsonSchema","text":"<p>The JsonSchema proc macro from schemars is what gives a struct the ability to produce a schema. By default, a struct must <code>impl JsonSchema</code> to be able to derive <code>CustomResource</code>.</p> <p>In both <code>derive</code> mode (default) and <code>manual</code> mode, kube-derive forces an <code>impl JsonSchema</code> requirement. This impl is then used by kube-derive with our own conformance rewriter for structural schemas.</p> <p>When using <code>JsonSchema</code>, your generated CustomResourceDefinition (via CustomResourceExt) will contain a schema.</p>"},{"location":"controllers/schemas/#deriving-jsonschema","title":"Deriving JsonSchema","text":"<p>The default setting uses <code>#[derive(JsonSchema)]</code>, and kube-derive will propagate this derive to the generated Kubernetes struct.</p> <p>This requires <code>#[derive(CustomResource, JsonSchema)]</code> on the spec struct:</p> <pre><code>#[derive(CustomResource, Deserialize, Serialize, Clone, Debug, JsonSchema)]\n#[kube(kind = \"Document\", group = \"kube.rs\", version = \"v1\", namespaced)]\npub struct DocumentSpec {\n    pub title: String,\n    pub hide: bool,\n    pub content: String,\n}\n</code></pre> <p>This example (simplified variant from controller-rs) generates a CustomResourceDefinition whose yaml representation (including schema) can be serialized using <code>serde_yaml::to_string(&amp;Document::crd())?</code> and will output:</p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: documents.kube.rs\nspec:\n  group: kube.rs\n  names:\n    categories: []\n    kind: Document\n    plural: documents\n    shortNames: []\n    singular: document\n  scope: Namespaced\n  versions:\n  - additionalPrinterColumns: []\n    name: v1\n    schema:\n      openAPIV3Schema:\n        description: Auto-generated derived type for DocumentSpec via `CustomResource`\n        properties:\n          spec:\n            properties:\n              content:\n                type: string\n              hide:\n                type: boolean\n              title:\n                type: string\n            required:\n            - content\n            - hide\n            - title\n            type: object\n        required:\n        - spec\n        title: Document\n        type: object\n    served: true\n    storage: true\n    subresources: {}\n</code></pre> <p>See object#installation for a common pattern for generating this.</p> <p>Schema requirements are transitive</p> <p>If your spec struct tries to derive <code>JsonSchema</code>, then all its members must also derive <code>JsonSchema</code>.</p> <p>See examples/crd_derive_schema.</p>"},{"location":"controllers/schemas/#implementing-jsonschema","title":"Implementing JsonSchema","text":"<p>When using <code>#[kube(schema = \"manual\")]</code>, kube-derive will not insert the derive attr of <code>JsonSchema</code> on the generated struct, and you are expected to provide an <code>impl JsonSchema for GeneratedStruct</code> yourself.</p> <p>This allows filling the gaps if your struct members only has partial <code>JsonSchema</code> coverage.</p> <p>See examples/crd_derive_custom_schema.</p>"},{"location":"controllers/schemas/#overriding-members","title":"Overriding Members","text":"<p>When you are implementing or deriving <code>JsonSchema</code>, you can override specific parts of a <code>JsonSchema</code> schema using <code>#[schemars(schema_with)]</code>. Some specific examples:</p> <ul> <li>overriding merge strategy on a vec</li> <li>overriding x-kubernetes properties on a condition</li> </ul>"},{"location":"controllers/schemas/#disabling-schemas","title":"Disabling Schemas","text":"<p>When using <code>#[kube(schema = \"disabled)]</code>, you are telling kube-derive not to use schemars at all, and you are taking responsibility for creating the schema manually. This removes all the safety mechanisms, and requires manually patching the schema fields, and dealing with structural schema quirks yourself.</p> <p>Disabling schemas invalidates the generated CRD</p> <p>Setting this option means the CustomResourceDefinition provided by CustomResourceExt will require modification.</p> <p>Any manual schemas must be attached to the generated CustomResourceDefinition before use. An example of this can be found in examples/crd_derive_no_schema.</p> <p>The main reason for going down this approach is if you are porting a controller with a CRD from another language and you want 100% conformance to the existing schema out of the gate.</p> <p>This method allows eliding the <code>#[derive(JsonSchema)]</code> instruction, and possibly also <code>schemars</code> from the dependency tree if you are careful with features.</p>"},{"location":"controllers/schemas/#versioning","title":"Versioning","text":"<p>It is possible to progress between two structs deriving <code>CustomResource</code> in a versioned manner.</p> <p>You can define multiple structs within versioned modules ala https://github.com/kube-rs/kube/blob/main/examples/crd_derive_multi.rs and then use merge_crds to combine them.</p> <p>See CustomResource#versioning, and upstream docs on Versions in CustomResourceDefinitions for more info.</p>"},{"location":"controllers/schemas/#validation","title":"Validation","text":"<p>Kubernetes &gt;1.25 supports including validation rules in the openapi schema, and there are a couple of ways to include these. See admission#Validation Using CEL Validation for examples.</p>"},{"location":"controllers/schemas/#manual-rules","title":"Manual Rules","text":"<p>This can be done by following upstream docs, and manually #Implementing-JsonSchema or #Overriding-Members to inject validation rules into specific parts of the schema.</p> <p>This approach will let you use the 1.25 Common Expression Language feature. There are currently no recommended ways of doing client-side validation with this approach, but there are new cel parser/interpreter crates and a cel expression playground that might be useful here.</p>"},{"location":"controllers/schemas/#deriving-via-garde","title":"Deriving via Garde","text":"<p>Using garde is nice for the simple case because it allows doing both client-side validation, and server-side validation, with the caveat that it only works on both sides for basic validation rules as schemars can only pick up on some of them.</p> <p>See CustomResource#schema-validation.</p>"},{"location":"controllers/security/","title":"Security","text":"<p>Best practices for creating secure, least-privilege controllers with kube.</p>"},{"location":"controllers/security/#problem-statement","title":"Problem Statement","text":"<p>When we are deploying a <code>Pod</code> into a cluster with elevated controller credentials, we are creating an attractive escalation target for attackers. Because of the numerous attack paths on pods and clusters that exists, we should be extra vigilant and following the least-privilege principle.</p> <p>While we can reap some security benefits from the Rust language itself (e.g. memory safety, race condition protection), this alone is insufficient.</p>"},{"location":"controllers/security/#potential-consequences-of-a-breach","title":"Potential Consequences of a Breach","text":"<p>If an attacker can compromise your pod, or in some other ways piggy-back on a controller's access, the consequences could be severe.</p> <p>The incident scenarios usually vary based on what access attackers acquire:</p> <ul> <li>cluster wide secret access \u21d2 secret oracle for attackers / data exfiltration</li> <li>cluster wide write access to common objects \u21d2 denial of service attacks / exfiltration</li> <li>external access \u21d2 access exfiltration</li> <li>pod creation access \u21d2 bitcoin miner installation</li> <li>host/privileged access \u21d2 secret data exfiltration/app installation</li> </ul> <p>See Trampoline Pods: Node to Admin PrivEsc Built Into Popular K8s Platorms as an example of how these types of attacks can work.</p>"},{"location":"controllers/security/#access-constriction","title":"Access Constriction","text":"<p>Depending on the scope of what your controller is in charge of, you should review and constrict:</p> Access Scope Access to review Cluster Wide <code>ClusterRole</code> rules Namespaced <code>Role</code> rules External Token permissions / IAM roles"},{"location":"controllers/security/#rbac-access","title":"RBAC Access","text":"<p>Managing the RBAC rules requires a declaration somewhere (usually in your yaml/chart) of your controllers access intentions.</p> <p>Kubernetes manifests with such rules can be kept up-to-date via testing#end-to-end-tests in terms of sufficiency, but one should also document the intent of your controller so that excessive permissions are not just \"assumed to be needed\" down the road.</p> <p>RBAC Rules Sanity</p> <p>It is possible to generate rbac rules using audit2rbac (see controller-rs example). This approach has limitations: it needs a full e2e setup with an initial rbac config, and the output may need yaml conversion and refinement steps. However, you can use it to sanity check that your rbac rules are not scoped too broadly.</p> <p>See manifests#RBAC for a starter manifest.</p>"},{"location":"controllers/security/#crd-access","title":"CRD Access","text":"<p>Installing a CRD into a cluster requires write access to <code>customresourcedefinitions</code>. This can be requested for the controller, but because this is such a heavy access requirement that is only really needed at the install/upgrade time, it is often handled separately. This also means that a controller often assumes the CRD is installed when running (and panicking if not).</p> <p>If you do need CRD write access, consider scoping this to non-delete access, and only for the <code>resourceNames</code> you expect:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: NAME\nrules:\n- apiGroups:\n  - apiextensions.k8s.io\n  resourceNames:\n  - mycrd.kube.rs # &lt;-- key line\n  resources:\n  - customresourcedefinitions\n  verbs:\n  - create\n  - get\n  - list\n  - watch\n  - patch\n</code></pre>"},{"location":"controllers/security/#role-vs-clusterrole","title":"Role vs. ClusterRole","text":"<p>Use <code>Role</code> (access for a single namespace only) over <code>ClusterRole</code> unless absolutely necessary.</p> <p>Some common access downgrade paths:</p> <ul> <li>if a controller is only working on an enumerable list of namespaces, create a <code>Role</code> with the access <code>rules</code>, and a <code>RoleBinding</code> for each namespace</li> <li>if a controller is always generating its dependent resources in a single namespace, you could expect the crd to also be installed in that same namespace.</li> </ul>"},{"location":"controllers/security/#namespace-separation","title":"Namespace Separation","text":"<p>Deploy the controller to its own namespace to ensure leaked access tokens cannot be used on anything but the controller itself.</p> <p>The installation namespace can also easily be separated from the controlled namespace.</p>"},{"location":"controllers/security/#container-permissions","title":"Container Permissions","text":"<p>Follow the standard guidelines for securing your controller pods. The following properties are recommended security context flags to constrain access:</p> <ul> <li><code>runAsNonRoot: true</code> or <code>runAsUser</code></li> <li><code>allowPrivilegeEscalation: false</code></li> <li><code>readOnlyRootFilesystem: true</code></li> <li><code>capabilities.drop: [\"ALL\"]</code></li> </ul> <p>But they might not be compatible with your current container setup. See documentation of Kubernetes Security Context Object.</p> <p>For cluster operators, the Pod Security Standards are also beneficial.</p>"},{"location":"controllers/security/#base-images","title":"Base Images","text":"<p>Minimizing the attack surface and amount extraneous code in your base image is also beneficial. It's worth reconsidering and finding alternatives for:</p> <ul> <li> <code>ubuntu</code> or <code>debian</code> (out of date deps hitting security scanners)</li> <li> <code>busybox</code> or <code>alpine</code> for your shell/debug access (escalation attack surface)</li> <li> <code>scratch</code> (basically a blank default root user)</li> </ul> <p>Instead, consider these security optimized base images:</p> <ul> <li> distroless base images (e.g. <code>:cc</code> for glibc / <code>:static</code> for musl)</li> <li> chainguard base images (e.g. gcc-glibc / static for musl)</li> </ul> <p>For shell debugging, consider <code>kubectl debug</code> using ephemeral containers instead.</p>"},{"location":"controllers/security/#network-permissions","title":"Network Permissions","text":"<p>Limiting who your controller can talk to / be called by will limit how useful of a target the controller will be in the case of a breach.</p> <p>It is good practice to setup a default-deny network policy for both <code>ingress</code> and <code>egress</code> and selectively apply as needed.</p> <p>See manifests#Network Policy for a starter manifest.</p>"},{"location":"controllers/security/#supply-chain-security","title":"Supply Chain Security","text":"<p>If malicious code gets injected into your controller through dependencies, you can still get breached even when following all the above. Thankfully, you will also most likely hear about it quickly from your security scanners, so make sure to use one.</p> <p>We recommend the following selection of tools that play well with the Rust ecosystem:</p> <ul> <li>dependabot or renovate for automatic dependency updates (upgrading)</li> <li><code>cargo audit</code> against rustsec</li> <li><code>cargo deny</code></li> <li><code>cargo auditable</code> embedding an SBOM for trivy / <code>cargo audit</code> / syft</li> </ul>"},{"location":"controllers/security/#references","title":"References","text":"<ul> <li>CNCF Operator WhitePaper</li> <li>Red Hat Blog: Kubernetes Operators: good security practices</li> <li>CNL: Creating a \u201cPaved Road\u201d for Security in K8s Operators</li> <li>Kubernetes Philly, November 2021 - Distroless Docker Images</li> <li>Wolfi OS and Building Declarative Containers (Chainguard)</li> <li>No more reasons to use distroless containers; kubectl debug</li> </ul>"},{"location":"controllers/streams/","title":"Streams","text":"<p>This chapter is about watcher streams and their use in controllers:</p> <p>We will first cover:</p> <ul> <li>watcher / metadata_watcher stream entrypoints</li> <li>working with watcher streams using WatchStreamExt</li> </ul> <p>and then the <code>unstable-runtime</code> controller streams interface</p> <ul> <li>Controller::for_stream - analogue for <code>Controller::new</code></li> <li>Controller::watches_stream - analogue for <code>Controller::watches</code></li> <li>Controller::owns_stream - analogue for <code>Controller::owns</code></li> </ul> <p>Together these sets of apis enable controller optimization, as well as stream sharing between co-hosted controllers (WIP#1080)</p>"},{"location":"controllers/streams/#stream-entrypoints","title":"Stream Entrypoints","text":"<p>All watcher streams are started by either watcher or metadata_watcher.</p>"},{"location":"controllers/streams/#watcher","title":"Watcher","text":"<p>A watcher is a high level primitive combining <code>Api::watch</code> and <code>Api::list</code> to provide an infinite watch <code>Stream</code> while handling all the error cases.</p> <p>The watcher <code>Stream</code> can be passed through cache writers (via reflector), passed on to controllers (or created implicitly by the <code>Controller</code>), or even observed directly:</p> <pre><code>let api = Api::&lt;Pod&gt;::default_namespaced(client);\nwatcher(api, watcher::Config::default())\n    .applied_objects()\n    .default_backoff()\n    .try_for_each(|p| async move {\n        info!(\"saw {}\", p.name_any());\n        Ok(())\n    }).await?;\n</code></pre> <p>The above example will run continuously until the end of the program. Note that as <code>watcher</code> produces an async rust stream, it must be polled to actually call the underlying api and do the work.</p>"},{"location":"controllers/streams/#metadata-watcher","title":"Metadata Watcher","text":"<p>A metadata_watcher is a watcher analogue that using the metadata api that only returns TypeMeta (<code>.api_version</code> + <code>.kind</code>) + ObjectMeta (<code>.metadata</code>).</p> <p>This can generally be used as a drop-in replacement for watcher provided you do not need data in <code>.spec</code> or <code>.status</code>.</p> <p>This means less IO, and less memory usage (especially if you are using it with a reflector). See the optimization chapter for details.</p> <p>You can generally replace <code>watcher</code> with <code>metadata_watcher</code> in the examples above as:</p> <pre><code># General change:\n-let stream =          watcher(api, cfg).applied_objects();\n+let stream = metadata_watcher(api, cfg).applied_objects();\n\n# Same change inside a reflector:\n-let stream = reflector(writer,          watcher(api, cfg)).applied_objects();\n+let stream = reflector(writer, metadata_watcher(api, cfg)).applied_objects();\n</code></pre> <p>But note this changes the stream signature slightly; returning a wrapped PartialObjectMeta.</p>"},{"location":"controllers/streams/#watcher-streams","title":"Watcher Streams","text":""},{"location":"controllers/streams/#terminology","title":"Terminology","text":"<ul> <li>watcher stream :: a stream that is started by one of the watcher #stream-entrypoints</li> <li>decoded stream :: a stream that's been through EventDecode via one of <code>WatchStreamExt::touched_objects</code>, <code>WatchStreamExt::applied_objects</code></li> <li>event stream :: a raw watcher stream producing watcher::Event objects</li> </ul> <p>The significant difference between them is that the user and the Controller generally wants to interact with a decoded stream, but a reflector needs an event stream to be able to safely replace its contents.</p>"},{"location":"controllers/streams/#watchstreamext","title":"WatchStreamExt","text":"<p>The WatchStreamExt trait is a <code>Stream</code> extension trait (ala StreamExt) with Kubernetes specific helper methods that can be chained onto a watcher stream;</p> <pre><code>watcher(api, watcher::Config::default())\n    .default_backoff()\n    .modify(|x| { x.managed_fields_mut().clear(); })\n    .applied_objects()\n    .predicate_filter(predicates::generation)\n</code></pre> <p>These methods can require one of:</p> <ul> <li>event stream (where the input stream <code>Item = Result&lt;Event&lt;K&gt;, ...&gt;</code></li> <li>decoded stream (where <code>Item = Result&lt;K, ...&gt;</code>, the last ones in the chain)</li> </ul> <p>It is impossible to apply them in an incompatible configuration.</p>"},{"location":"controllers/streams/#stream-mutation","title":"Stream Mutation","text":"<p>It is possible to modify or filter the input streams before passing them on. This can usually either done to limit data in memory by pruning, or to filter events to a downstream controller so that it either triggers less frequently.</p>"},{"location":"controllers/streams/#predicates","title":"Predicates","text":"<p>Using predicates, we can filter out events from a stream where the last value of a particular property is unchanged. This is done internally by storing hashes of the given property(ies), and can be chained onto an decoded stream:</p> <pre><code>let api: Api&lt;Deployment&gt; = Api::all(client);\nlet stream = watcher(api, cfg)\n    .applied_objects()\n    .predicate_filter(predicates::generation);\n</code></pre> <p>in this case, deployments with the last previously seen <code>.metadata.generation</code> hash will be filtered out from the stream.</p> <p>A generation predicate effectively filters out changes that only affect the <code>.status</code> object (for resources that support .generation), and is one useful way to avoding reconcile changes to your own CR re-triggering your reconciler.</p> <p>We can additionally wrap a reflector around the raw watcher stream before doing the filter. This ensures we still have the most up-to-date value received in the cache:</p> <pre><code>let stream = reflector(writer, watcher(api, cfg))\n    .applied_objects()\n    .predicate_filter(predicates::generation);\n</code></pre>"},{"location":"controllers/streams/#event-modification","title":"Event Modification","text":"<p>You can modify raw objects in flight before they are passed on to a reflector or controller. This can help minimise reflector memory consumption by optimization#pruning-fields.</p> <pre><code>let stream = watcher(pods, cfg).modify(|pod| {\n    pod.managed_fields_mut().clear();\n    pod.status = None;\n});\nlet (reader, writer) = reflector::store::&lt;Pod&gt;();\nlet rf = reflector(writer, stream).applied_objects();\n</code></pre> <p>Ordering</p> <p>It is possible to do the modification after the <code>reflector</code> call, but this would result in the modification not being persisted in the store and merely passed on in the stream.</p>"},{"location":"controllers/streams/#controller-streams","title":"Controller Streams","text":"<p>By default, watcher streams are implicitly configured within the Controller, but using the controller streams interface setup introduced in kube 0.81 you can explicitly setup all the watcher stream for more precise targeting:</p> <pre><code>Controller::for_stream(main_stream, reader)\n    .owns_stream(owned_custom_stream, cfg)\n    .watches_stream(watched_custom_stream, cfg)\n</code></pre> <p>where the various stream variables would be created from either watcher, or metadata_watcher with a decoder applied.</p> <p>The controller streams interface is unstable</p> <p>Currently plugging streams into Controller requires the <code>kube/unstable-runtime</code> feature. This interface is planned to be stabilized in a future release.</p>"},{"location":"controllers/streams/#output-stream","title":"Output Stream","text":"<p>To start a controller, you typically invoke <code>Controller::run</code>, and this actually produces a stream of object references that are yielded after being passed through the reconciler.</p> <p>This is not important from a data-perspective (as you will see everything from <code>reconcile</code>), but it is the stream that back-propagates through the stream of streams that the <code>Controller</code> ultimately manages. The key point:</p> <p>Polling</p> <p>You must continuously poll the output stream to cause the controller to work.</p> <p>You can do this by looping through the output stream:</p> <pre><code>Controller::new(api, Config::default())\n    .run(reconcile, error_policy, context)\n    .filter_map(|x| async move { std::result::Result::ok(x) })\n    .for_each(|_| futures::future::ready(()))\n    .await;\n</code></pre>"},{"location":"controllers/streams/#input-streams","title":"Input Streams","text":"<p>To configure one of the input streams manually you need to:</p> <ol> <li>create a watcher stream with backoff</li> <li>decode the stream</li> <li>call the stream-equivalent <code>Controller</code> interface</li> </ol> <p>Note that the <code>Controller</code> will poll all the passed (or implicitly created) watcher streams as a whole when you poll the output stream from the controller.</p>"},{"location":"controllers/streams/#main-stream","title":"Main Stream","text":"<p>The controller runtime requires a reflector for the main api, so you must also create a reflector pair yourself in this case:</p> <pre><code> let cfg = watcher::Config::default();\n let api = Api::&lt;MyCustomResource&gt;::all(client.clone());\n+let (reader, writer) = reflector::store();\n+let stream = reflector(writer, watcher(api, cfg))\n+    .default_backoff()\n+    .applied_objects();\n\n-Controller::new(api, cfg)\n+Controller::for_stream(stream, reader)\n</code></pre> <p>leaving additionally the <code>reader</code> in your hands should you need it (obviating the non-stream requirement to call to <code>Controller::store</code>). The <code>Controller</code> will wait for the <code>Store</code> (reader) to be populated until starting reconciles.</p> <p>Metadata Watchers on Main Controller Stream</p> <p>Using metadata_watcher on the main stream (using <code>Controller::for_stream</code>) changes the <code>reconcile</code> / <code>error_policy</code> type signature from returning objects of form <code>Arc&lt;K&gt;</code> to <code>Arc&lt;PartialObjectMeta&lt;K&gt;&gt;</code>:</p> <pre><code>-async fn reconcile(_: Arc&lt;Deployment&gt;, _: Arc&lt;()&gt;) ...\n+async fn reconcile(_: Arc&lt;PartialObjectMeta&lt;Deployment&gt;&gt;, _: Arc&lt;()&gt;) ...\n-fn error_policy(_: Arc&lt;Deployment&gt;, _: &amp;kube::Error, _: Arc&lt;()&gt;) ...\n+fn error_policy(_: Arc&lt;PartialObjectMeta&lt;Deployment&gt;&gt;, _: &amp;kube::Error, _: Arc&lt;()&gt;) ...\n</code></pre> <p>This means the object you get in your reconciler is just a partial object with only <code>.metadata</code>. You can call <code>api.get()</code> inside <code>reconcile</code> to get a full object if needed.</p>"},{"location":"controllers/streams/#owned-stream","title":"Owned Stream","text":"<p>As per relations, this requires your owned objects to have owner refrences back to your main object (<code>cr</code>):</p> <pre><code> let cfg_owned = watcher::Config::default();\n let cfg_cr = watcher::Config::default();\n let cr: Api&lt;MyCustomResource&gt; = Api::all(client.clone());\n let owned_api: Api&lt;Deployment&gt; = Api::default_namespaced(client);\n+let deploys = metadata_watcher(owned_api, cfg_owned).default_backoff().applied_objects();\n\n Controller::new(cr, cfg_cr)\n-    .owns(owned_api, cfg_owned)\n+    .owns_stream(deploys)\n</code></pre> <p>Metadata Watcher Default</p> <p>#Metadata-Watcher is used in Controller::owns for its stream, as the reverse mapping (say from <code>Deployment</code> to <code>MyCustomResource</code>) is always done entirely with metadata properties. As such, it is also the recommended default for Controller::owns_stream.</p>"},{"location":"controllers/streams/#watched-stream","title":"Watched Stream","text":"<p>As per relations, this requires a custom mapper mapping back to your main object (<code>cr</code>):</p> <pre><code> fn mapper(_: DaemonSet) -&gt; Option&lt;ObjectRef&lt;MyCustomResource&gt;&gt; { todo!() }\n\n let cfg_ds = watcher::Config::default();\n let cfg_cr = watcher::Config::default();\n let cr_api: Api&lt;MyCustomResource&gt; = Api::all(client.clone());\n let ds_api: Api&lt;DaemonSet&gt; = Api::all(client);\n+let daemons = watcher(ds_api, cfg_ds).default_backoff().touched_objects();\n\n Controller::new(cr_api, cfg_cr)\n-    .watches(ds_api, cfg_ds, mapper)\n+    .watches_stream(daemons, mapper)\n</code></pre> <p>This often combines cleanly with #Metadata-Watcher when the <code>mapper</code> only relies on metadata properties.</p>"},{"location":"controllers/streams/#multi-stream-example","title":"Multi Stream Example","text":"<p>A more advanced example using:</p> <ul> <li>main stream through a watcher + reflector with predicates</li> <li>owned stream through a metadata_watcher</li> </ul> <pre><code>let cfg_owned = watcher::Config::default();\nlet cfg_cr = watcher::Config::default();\n\nlet api_owned = Api::&lt;PartialObjectMeta&lt;Deployment&gt;&gt;::all(client.clone());\nlet api_cr = Api::&lt;MyCustomResource&gt;::all(client.clone());\n\nlet (reader, writer) = reflector::store();\nlet cr_stream = reflector(writer, watcher(api_cr, cfg_cr))\n    .default_backoff()\n    .applied_objects()\n    .predicate_filter(predicates::generation);\n\nlet owned_stream = metadata_watcher(api_owned, cfg_owned)\n    .default_backoff()\n    .touched_objects();\n\nController::for_stream(cr_stream, reader)\n    .owns_stream(owned_stream)\n    .run(reconcile, error_policy, Arc::new(()))\n    .for_each(|_| std::future::ready(()))\n    .await;\n</code></pre>"},{"location":"controllers/testing/","title":"Testing","text":"<p>This chapter covers controller testing and example Rust Kubernetes test patterns.</p>"},{"location":"controllers/testing/#terminology","title":"Terminology","text":"<p>We will loosely re-use the kube test categories and outline four types of tests:</p> <ul> <li>End to End tests (requires Kubernetes through an in-cluster Client)</li> <li>Integration tests (requires Kubernetes)</li> <li>Mocked unit tests (requires mocking Kubernetes dependencies)</li> <li>Unit tests (no Kubernetes calls)</li> </ul> <p>These types should roughly match what you see in a standard test pyramid where testing power and maintenance costs both increase as you go up the list.</p> <p>Classification Subjectivity</p> <p>This classification and terminology re-use herein is partially subjective. Variant approaches are discussed.</p>"},{"location":"controllers/testing/#unit-tests","title":"Unit Tests","text":"<p>The basic unit <code>#[test]</code>. Typically composed of individual test function in a tests only module inlined in files containing what you want to test.</p> <p>We will defer to various official guides on good unit test writing in rust:</p> <ul> <li>Rust By Example - Unit testing</li> <li>Rust Book - How to Write Tests</li> </ul>"},{"location":"controllers/testing/#benefits","title":"Benefits","text":"<p>Very simple to setup, with generally no extra interfaces needed.</p> <p>Works extremely well for algorithms, state machinery, and business logic that has been separated out from network behavior (e.g. the sans-IO approach). Splitting out business logic from IO will reduce the need for more expensive tests below and should be favored where possible.</p>"},{"location":"controllers/testing/#downsides","title":"Downsides","text":"<p>While it is definitely possible to go overboard with unit tests and test too deeply (without protecting any real invariants), this is not what we will focus on here. When unit tests are appropriate, they are great.</p> <p>In the controller context, the main unit test downside is that we cannot cover the IO component without something standing in for Kubernetes - such as an apiserver mock or an actual cluster - making it, by definition, not a plain unit test anymore. </p> <p>The controller is fundamentally tied up in the reconciler, so there is always going to be a sizable chunk of code that you cannot do with plain unit tests.</p>"},{"location":"controllers/testing/#kubernetes-io-strategies","title":"Kubernetes IO Strategies","text":"<p>For the reconciler (and similar Kubernetes calling logic you may have), there are 3 major strategies to test this code.</p> <p>You have one basic choice:</p> <ol> <li>stay in unit-test land by mocking out your network dependencies (worse test code)</li> <li>move up the test pyramid and do full-scale integration testing (worse test reliability)</li> </ol> <p>and then you can also choose to do e2e testing either as an additional bonus, or as a substitute for integration testing. Larger projects may wish to do everything.</p> <p>Idempotency reducing the need for tests</p> <p>The more you learn to lean on using Server-Side Apply, the less if/else gates will end up with in your reconciler, and thus the less testing you will need.</p>"},{"location":"controllers/testing/#unit-tests-with-mocks","title":"Unit Tests with Mocks","text":"<p>It is possible to to test your reconciler and IO logic and retain the speed and isolation of unit tests by using mocks. This is common practice to avoid having to bring in your all your dependencies and is typically done through crates such as wiremock, mockito, tower-test, or mockall.</p> <p>Out of these, tower-test integrates well with our Client out of the box, and is the one we will focus on here.</p>"},{"location":"controllers/testing/#example","title":"Example","text":"<p>To create a mocked Client with <code>tower-test</code> it is sufficient to instantiate one on a mock service:</p> <pre><code>let (mocksvc, handle) = tower_test::mock::pair::&lt;Request&lt;Body&gt;, Response&lt;Body&gt;&gt;();\nlet client = Client::new(mocksvc, \"default\");\n</code></pre> <p>This is using the generic:</p> <ul> <li><code>http::Request</code> + <code>http::Response</code> objects</li> <li><code>kube::client::Body</code> as request/response content</li> </ul> <p>This <code>Client</code> can then be passed into to reconciler in the usual way through a context object (reconciler##using-context), allowing you to test <code>reconcile</code> directly.</p> <p>You do need to write a bit of code to make the test <code>handle</code> do the right thing though, and this does require a bit of boilerplate because there is nothing equivalent to <code>envtest</code> in Rust (so far). Effectively, we need to mock bits of the kube-apiserver and it can look something like this:</p> <pre><code>// We wrap tower_test::mock::Handle\ntype ApiServerHandle = tower_test::mock::Handle&lt;Request&lt;Body&gt;, Response&lt;Body&gt;&gt;;\npub struct ApiServerVerifier(ApiServerHandle);\n\n/// Scenarios we want to test for\npub enum Scenario {\n    /// We expect exactly one `patch_status` call to the `Document` resource\n    StatusPatch(Document),\n    /// We expect nothing to be sent to Kubernetes\n    RadioSilence,\n}\nimpl ApiServerVerifier {\n    pub fn run(self, scenario: Scenario) -&gt; tokio::task::JoinHandle&lt;()&gt; {\n        tokio::spawn(async move { // moving self =&gt; one scenario per test\n            match scenario {\n                Scenario::StatusPatch(doc) =&gt; self.handle_status_patch(doc).await,\n                Scenario::RadioSilence =&gt; Ok(self),\n            }\n            .expect(\"scenario completed without errors\");\n        })\n    }\n\n    /// Respond to PATCH /status with passed doc + status from request body\n    async fn handle_status_patch(mut self, doc: Document) -&gt; Result&lt;Self&gt; {\n        let (request, send) = self.0.next_request().await.expect(\"service not called\");\n        assert_eq!(request.method(), http::Method::PATCH);\n        let exp_url = format!(\"/apis/kube.rs/v1/namespaces/testns/documents/{}/status?&amp;force=true&amp;fieldManager=cntrlr\", doc.name_any());\n        assert_eq!(request.uri().to_string(), exp_url);\n        // extract the `status` object from the `patch_status` call via the body of the request\n        let req_body = request.into_body().collect().await.unwrap().to_bytes();\n        let json: serde_json::Value = serde_json::from_slice(&amp;req_body).expect(\"patch_status object is json\");\n        let status_json = json.get(\"status\").expect(\"status object\").clone();\n        let status: DocumentStatus = serde_json::from_value(status_json).expect(\"contains valid status\");\n        // attach it to the supplied document to pretend we are an apiserver\n        let response = serde_json::to_vec(&amp;doc.with_status(status)).unwrap();\n        send.send_response(Response::builder().body(Body::from(response)).unwrap());\n        Ok(self)\n    }\n}\n</code></pre> <p>Here we have made an apiserver mock wrapper that will run certain scenarios. Each scenario calls a number of handler functions that assert on certain basic expectations about the nature of the message we receive. Here we only have made one for <code>handle_status_patch</code>, but more are used in controller-rs/fixtures.rs.</p> <p>An actual tests that uses the above wrapper can end up being quite readable:</p> <pre><code>#[tokio::test]\nasync fn doc_reconcile_causes_status_patch() {\n    let (testctx, fakeserver) = Context::test();\n    let doc = Document::test();\n    let mocksrv = fakeserver.run(Scenario::StatusPatch(doc.clone()));\n    reconcile(Arc::new(doc), testctx).await.expect(\"reconciler\");\n    timeout_after_1s(mocksrv).await;\n}\n</code></pre> <p>Effectively, this is an exercise in running two futures together (one in a task), and one in the main test fn, then joining at the end.</p> <p>In this test we are effectively verifying that:</p> <ol> <li>reconcile ran successfully in the given scenario</li> <li>apiserver handler saw all expected messages</li> <li>apiserver handler saw no unexpected messages.</li> </ol> <p>This is satisfied because:</p> <ol> <li>reconcile is unwrapped while handler is running through the scenario</li> <li>Each scenario blocked on sequential api calls to happen (we await each message), so mockserver's joinhandle will not resolve until every expected message in the given scenario has happened (hence the timeout) </li> <li>If the mock server is receiving more Kubernetes calls than expected the reconciler will error with a <code>KubeError(Service(Closed(())))</code> caught by the reconcilers <code>expect</code></li> </ol> <p>Context and Document constructors omitted</p> <p>Test functions to create the rest of the reconciler context and a test document used by a reconciler are not shown, see controller-rs/fixtures.rs for a relatively small <code>Context</code>. Note that the more things you pass in to your reconciler the larger your <code>Context</code> will be, and the more stuff you will want to mock. </p>"},{"location":"controllers/testing/#benefits_1","title":"Benefits","text":"<p>Using mocks are comparable to using integration tests in power and versatility. It lets us move up the pyramid in terms of testing power, but without needing an actual network boundary and a real cluster. As a result, we maintain test reliability.</p>"},{"location":"controllers/testing/#downsides_1","title":"Downsides","text":"<p>Compared to using a real cluster, the amount of code we need to write - to compensate for a missing apiserver - is currently quite significant. This verbosity means a higher initial cost of writing these tests, and also more complexity to keep in your head and maintain. We hope that some of this complexity can be reduced in the future with more Kubernetes focused test helpers.</p>"},{"location":"controllers/testing/#external-examples","title":"External Examples","text":"<ul> <li>nais/hahaha exec tests using an automock trait</li> </ul>"},{"location":"controllers/testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests run against a real Kubernetes cluster, and lets you verify that the IO components of your controller is doing the right thing in a real environment. The big selling point is that they require little code to write and are easy to understand.</p>"},{"location":"controllers/testing/#example_1","title":"Example","text":"<p>Let us try to verify the same status patching scenario from above using an integration test.</p> <p>First, we need a working Client. Using <code>Client::try_default()</code> inside an async-aware <code>#[test]</code> we end up using using the <code>current-context</code> set in your local kubeconfig.</p> <pre><code>    // Integration test without mocks\n    use kube::api::{Api, Client, ListParams, Patch, PatchParams};\n    #[tokio::test]\n    #[ignore = \"uses k8s current-context\"]\n    async fn integration_reconcile_should_set_status() {\n        let client = Client::try_default().await.unwrap();\n        let ctx = State::default().to_context(client.clone());\n\n        // create a test doc and run it through ~= kubectl apply --server-side\n        let doc = Document::test().finalized().needs_hide();\n        let docs: Api&lt;Document&gt; = Api::namespaced(client.clone(), \"default\");\n        let ssapply = PatchParams::apply(\"ctrltest\");\n        let patch = Patch::Apply(doc.clone());\n        docs.patch(\"test\", &amp;ssapply, &amp;patch).await.unwrap();\n\n        // reconcile it (as if it was applied to the cluster like this)\n        reconcile(Arc::new(doc), ctx).await.unwrap();\n\n        // verify side-effects happened\n        let output = docs.get_status(\"test\").await.unwrap();\n        assert!(output.status.is_some());\n    }\n</code></pre> <p>this sets up a <code>Client</code>, a <code>Context</code> (to be passed to the reconciler), then applies an actual document into the cluster, and at the same time giving it to the reconciler.</p> <p>Feeding the apply result (usually seen by watching the api) is what the Controller internals does, so we skip testing this part. As a result, we get a much simpler test call around only <code>reconcile</code> that we can verify by querying the api after it has completed.</p> <p>The tests at the bottom of controller-rs/controller.rs go a little deeper, testing a larger scenario.</p> <p>We need a cluster for these tests though, so on CI we will spin up a k3d instance for each PR. Here is a GitHub Actions based setup:</p> <pre><code>  integration:\n    runs-on: ubuntu-latest\n    strategy:\n      # Prevent GitHub from cancelling all in-progress jobs when a matrix job fails.\n      fail-fast: false\n      matrix:\n        # Run these tests against older clusters as well\n        k8s: [v1.25, latest]\n    steps:\n      - uses: actions/checkout@v3\n      - uses: dtolnay/rust-toolchain@stable\n      - uses: Swatinem/rust-cache@v2\n      - uses: nolar/setup-k3d-k3s@v1\n        with:\n          version: ${{matrix.k8s}}\n          k3d-name: kube\n          # Used to avoid rate limits when fetching the releases from k3s repo.\n          # Anonymous access is limited to 60 requests / hour / worker\n          # github-token: ${{ secrets.GITHUB_TOKEN }}\n          k3d-args: \"--no-lb --no-rollback --k3s-arg --disable=traefik,servicelb,metrics-server@server:*\"\n\n      # Real CI work starts here\n      - name: Build workspace\n        run: cargo build\n\n      # Run the integration tests\n      - name: install crd\n        run: cargo run --bin crdgen | kubectl apply -f -\n      - name: Run all default features integration library tests\n        run: cargo test --lib --all -- --ignored\n</code></pre> <p>This creates a minimal k3d cluster (against both the latest k3d version and our last supported k8s version), and then runs <code>cargo test -- --ignored</code> to specifically only run the <code>#[ignore]</code> marked integration tests.</p> <p><code>#[ignore]</code> annotations on integration tests</p> <p>We advocate for using the <code>#[ignore]</code> attribute as a visible opt-in for developers. The <code>Client::try_default</code> will work against whatever arbitrary cluster a developer has set to their <code>current-context</code>, so makes it harder (than merely typing <code>cargo test</code>) to accidentally modify random clusters.</p>"},{"location":"controllers/testing/#benefits_2","title":"Benefits","text":"<p>As you can see, this is a lot simpler than the mocking version on the Rust side; no request/response handling and task spawning.</p> <p>At the same time, these tests are more powerful than mocks; we can test the major flow path of a controller in a cluster against different Kubernetes versions with very little code.</p>"},{"location":"controllers/testing/#downsides_2","title":"Downsides","text":""},{"location":"controllers/testing/#low-reliability","title":"Low Reliability","text":"<p>While this will vary between CI providers, cluster setup problems are common.</p> <p>As you now depend on both cluster specific actions to set up a cluster (e.g. setup-k3d-k3s), and the underlying cluster interface (e.g. k3d), you have to deal with compatibility issues between these. Spurious cluster creation failures on GHA are common (particularly on <code>latest</code>).</p> <p>You also have to wait for resources to be ready. Usually, this involves waiting for a Condition, but Kubernetes does not have conditions for everything*, so you can still run into race conditions.</p> <p>It is possible to reduce the reliability problems a bit by using dedicated clusters, but that brings us onto the second pain point;</p>"},{"location":"controllers/testing/#no-isolation","title":"No Isolation","text":"<p>Tests from one file can cause interactions and race conditions with other tests, and re-using a cluster across test runs makes this problem worse as tests now need to be idempotent.</p> <p>It is possible to achieve full test isolation for integration tests, but it often brings impractical costs (such as setting up a new cluster per test, or writing all tests to be idempotent and using disjoint resources).</p> <p>Thus, you can only (realistically) write so many of these tests because you have to keep in your head which tests is doing what to your environment and they may be competing for the same resource names.</p>"},{"location":"controllers/testing/#black-box-variant","title":"Black Box Variant","text":"<p>The setup above is not a black-box integration test, because we pull out internals to create state, and call <code>reconcile</code> almost like a unit test.</p> <p>Rust conventions on integration tests</p> <p>Rust defines integration tests as acting only on public interfaces and residing in a separate <code>tests</code> directory. </p> <p>We effectively have a white-box integration test instead.</p> <p>It is possible to export our <code>reconcile</code> plus associated <code>Context</code> types as a new public interface from a new controller library, and then black-box test that (per the Rust definition) from a separate <code>tests</code> directory.</p> <p>In the most basic cases, this is effectively a definitional hack as we;</p> <ul> <li>introduce an arbitrary public boundary that's only used by tests and controller main</li> <li>declare this boundary as public, and test that (in the exact same way)</li> <li>re-plumb controller main to use this interface rather than the old private internals</li> </ul> <p>But this does also separate the code that we consider important enough to test from the rest, and that boundary has been made explicit via the (technically unnecessary) library (at the cost of having more files and boundaries).</p> <p>Doing so will make make your interfaces more explicit, and this can be valuable for more advanced controllers using multiple reconcilers.</p>"},{"location":"controllers/testing/#functional-variant","title":"Functional Variant","text":"<p>Rather than moving interfaces around to fit definitions of black-box tests, we can can also remove all our assumptions about code layout in the first place, and create more functional tests.</p> <p>In functional tests, we instead run the controller directly, and test against it, via something like:</p> <ol> <li>explicitly <code>cargo run &amp;</code> the controller binary</li> <li><code>kubectl apply</code> a test document</li> <li>verify your conditions outside (e.g. <code>kubectl wait</code> or a separate test suite)</li> </ol> <p>CoreDB's operator follows this approach, and it is definitely an important thing to test. In this guide, you can see functional testing done as part of End to End Tests.</p>"},{"location":"controllers/testing/#end-to-end-tests","title":"End to End Tests","text":"<p>End to End tests install your release unit (image + yaml) into a cluster, then runs verification against the cluster and the application.</p> <p>The most common use-case of this type of test is smoke testing, but we can also test a multitude of integration scenarios using this approach.</p>"},{"location":"controllers/testing/#example_2","title":"Example","text":"<p>We will do e2e testing to get a basic verification of our:</p> <ul> <li>packaging system (does the image work and install with the yaml pipeline?)</li> <li>controller happy path (does it reconcile on cluster mutation?)</li> </ul> <p>This thus focuses entirely on the extreme high-level details, leaving lower-level specifics to integration tests, mocked unit tests, or even linting tools (for yaml verification).</p> <p>As a result, we do not require any additional Rust code, as here we will treat the controller as a black box, and do all verification with <code>kubectl</code>.</p> <p>E2E Container Building</p> <p>An e2e test will require access to the built container image, so spending time on CI caching can be helpful.</p> <p>An example setup for GitHub Actions:</p> <pre><code>  e2e:\n    runs-on: ubuntu-latest\n    needs: [docker]\n    steps:\n      - uses: actions/checkout@v3\n      - uses: nolar/setup-k3d-k3s@v1\n        with:\n          version: v1.27\n          k3d-name: kube\n          k3d-args: \"--no-lb --no-rollback --k3s-arg --disable=traefik,servicelb,metrics-server@server:*\"\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n      - name: Download docker image artifact from docker job\n        uses: actions/download-artifact@v3\n        with:\n          name: controller-image\n          path: /tmp\n      - name: Load docker image from tarball\n        run: docker load --input /tmp/image.tar\n      # install crd + controller (via chart)\n      - run: kubectl apply -f yaml/crd.yaml\n      - run: helm template charts/doc-controller | kubectl apply -f -\n      - run: kubectl wait --for=condition=available deploy/doc-controller --timeout=20s\n      # add a test intance\n      - run: kubectl apply -f yaml/instance-samuel.yaml\n      - run: kubectl wait --for=condition=somecondition doc/samuel --timeout 2\n      # verify basic happy path outcomes have happened\n      - run: kubectl get event --field-selector \"involvedObject.kind=Document,involvedObject.name=samuel\" | grep \"HideRequested\"\n      - run: kubectl get doc -oyaml | grep -A1 finalizers | grep documents.kube.rs\n</code></pre> <p>Here we are loading a built container via docker buildx from a different test job (named <code>docker</code> here) stashed as a build artifact. Building images will be covered elsewhere, but you can see the CI configuration for controller-rs as a reference.</p> <p>Once the image and the cluster (same k3d setup) is available, we can install the CRD, a test document, and the deployment yaml using whatever yaml pipeline we want/have to deal with (here helm).</p> <p>After installations and resources are ready (checked by kubectl wait or a simpler <code>sleep</code> if you do not have enough conditions), we can verify that basic changes have occurred in the cluster.</p> <p>CRD installation</p> <p>We separated the CRD installation and the deployment installation because CRD write access is generally a much stronger security requirement that is often controlled separately in a corporate environment.</p>"},{"location":"controllers/testing/#benefits_3","title":"Benefits","text":"<p>These tests are useful because they cover the interface between the yaml and the application along with most unknown-unknowns. E.g. do you read a new evar in the app now? Did you typo something in RBAC, or provide insufficient access?</p> <p>By having a single e2e test we can avoid most of those awkward post-release hot-fixes.</p> <p>Different approaches</p> <p>It is possible to detect some of these failure modes in other ways. Schema testing via kubeconform, or client-side admission policy verification via conftest for OPA, or kwctl for kubewarden, or polaris CLI for polaris to name a few. You should consider these, but note that they are not foolproof. Policy tests generally verify security constraints, and schema tests are limited by schema completeness and openapi.</p>"},{"location":"controllers/testing/#downsides_3","title":"Downsides","text":"<p>In addition to requiring slightly more complicated CI, the main new downside of using e2e tests over integration tests is error handling complexity; all possible failures modes can occur - often with bad error messages. On top of this, all the previous integration test downsides still apply.</p> <p>As a result, we only want a small number of e2e tests as the signal to noise ratio is going to be low, and errors may not be obvious from failures.</p>"},{"location":"controllers/testing/#summary","title":"Summary","text":"<p>Each test category comes with its own unique set of benefits and challenges:</p> Test Type Isolation Maintenance Cost Main Test Case End-to-End  No Reliability + Isolation + Debug Real IO + Full Smoke Integration  No Reliability + Isolation Real IO + Smoke Unit w/mocks  Yes Complexity + Readability Substitute IO Unit  Yes Unrealistic Scenarios Non-IO <p>The high cost of end-to-end and integration tests is almost entirely due to reliability issues with clusters on CI that ends up being a constant cost. The lack of test isolation in these real environments also make them more attractive as a form of sanity verification/smoke.</p> <p>Focusing on the lower end of the test pyramid (by separating the IO code from your business logic, or by mocking liberally), and proving a few specialized tests at the top end, is likely to to have the biggest benefit-to-pain ratio. As an exercise in redundancy, controller-rs does everything, and can be inspected as a reference.</p>"},{"location":"controllers/webserver/","title":"Web Server","text":"<p>This is a WIP document.</p>"},{"location":"controllers/webserver/#actix-web","title":"Actix-web","text":"<p>Now that we have a more stable release chain of actix-web (version 4 is out), it is easier to write guides, and will use this heavily battle tested web-framework.</p> <pre><code>cargo add actix-web\n</code></pre> <p>Heavy Weight Framework</p> <p>The <code>actix-web</code> crate is fairly heavy-weight for just exposing metrics. For a simpler web framework that we have partial support for, consider axum and our version-rs application using it.</p>"},{"location":"controllers/webserver/#usage","title":"Usage","text":"<p>This document is unfinished so we refer to controller-rs which is a full-featured example of using actix-web with kube.</p>"},{"location":"crates/kube-client/","title":"kube-client","text":"<p><code>kube-client</code> is the client crate with config and client abstractions. It is re-exported from kube under the <code>kube::client</code> and <code>kube::config</code> modules.</p> <p>No need to depend on this directly</p> <p>The standard way to get only client + core features is to depend on <code>kube</code> with its default features. If you need a runtime or the proc-macro later, you don't then need to change your import paths.</p> <p>This crate has the most extensive documentation on</p> <ul> <li>docs.rs/kube/client</li> <li>docs.rs/kube/config</li> </ul>"},{"location":"crates/kube-core/","title":"kube-core","text":"<p><code>kube-core</code> is the core crate with the lowest level abstractions. It is re-exported from kube under <code>kube::core</code>.</p> <p>No need to depend on this directly</p> <p>The standard way to get only core features is to depend on <code>kube</code> with <code>default-features = false</code> as this will bring in the minimal <code>kube</code>. If you need a client later, you don't then need to change your import paths.</p> <p>This crate has the most extensive documentation on docs.rs/kube/core.</p>"},{"location":"crates/kube-core/#contains","title":"Contains","text":"<p>Core traits and types necessary for interacting with the Kubernetes API. This crate is the rust counterpart to kubernetes/apimachinery.</p>"},{"location":"crates/kube-derive/","title":"kube-derive","text":"<p><code>kube-derive</code> is a procedural macro crate with helpers for managing Custom Resource Definitions. The macros are re-exported from kube.</p> <p>No need to depend on this directly</p> <p>The standard way to get derive features is to depend on <code>kube</code> with the additional <code>derive</code> feature, and access <code>kube::CustomResource</code>. This way you avoid multiple entries in <code>Cargo.toml</code> which can cause version mismatches.</p> <p>The macros exported are heavily documented on:</p> <ul> <li>docs.rs/kube/CustomResource</li> </ul>"},{"location":"crates/kube-runtime/","title":"kube-runtime","text":"<p><code>kube-runtime</code> is the runtime crate with the highest level abstractions. It is re-exported from kube under <code>kube::runtime</code>.</p> <p>No need to depend on this directly</p> <p>The standard way to get runtime + client features is to depend on <code>kube</code> with the additional <code>runtime</code> feature, and access <code>kube::runtime</code>. This way you avoid multiple entries in <code>Cargo.toml</code> which can cause version mismatches.</p> <p>This crate has the most extensive documentation on docs.rs/kube/runtime.</p>"},{"location":"crates/kube/","title":"kube","text":"<p><code>kube</code> is the facade crate that re-exports all the other crates.</p> <p>This crate has the most extensive documentation on docs.rs/kube.</p> <pre><code>graph TD\n    kube-client --&gt; kube-core\n    kube-derive --&gt; kube-core\n    kube-runtime --&gt; kube-client\n    kube --&gt; kube-client\n    kube --&gt; kube-core\n    kube --&gt; kube-runtime\n    kube --&gt; kube-derive</code></pre>"},{"location":"crates/kube/#re-exports","title":"Re-exports","text":"<ul> <li><code>kube</code> re-exports kube-runtime under <code>kube::runtime</code></li> <li><code>kube</code> re-exports kube-core under <code>kube::core</code></li> <li><code>kube</code> re-exports kube-derive's proc macros onto <code>kube</code></li> <li><code>kube</code> re-exports kube-client (flattened) under <code>kube</code> (<code>kube::client</code> and <code>kube::config</code>)</li> </ul>"},{"location":"blog/archive/2024/","title":"2024","text":""}]}